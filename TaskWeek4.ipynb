{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RifqiHafizuddin/Computational-Intelligence-week3/blob/main/TaskWeek4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c522d716",
      "metadata": {
        "id": "c522d716"
      },
      "source": [
        "Name: Rifqi Hafizuddin\n",
        "\n",
        "NPM: 2106638204"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270c2008",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "270c2008",
        "outputId": "b8da1876-0f93-4122-d3c0-4dee9f5ff7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l==1.0.3\n",
            "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Collecting jupyter==1.0.0 (from d2l==1.0.3)\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting numpy==1.23.5 (from d2l==1.0.3)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting matplotlib==3.7.2 (from d2l==1.0.3)\n",
            "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting matplotlib-inline==0.1.6 (from d2l==1.0.3)\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting requests==2.31.0 (from d2l==1.0.3)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pandas==2.0.3 (from d2l==1.0.3)\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scipy==1.10.1 (from d2l==1.0.3)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.5)\n",
            "Collecting qtconsole (from jupyter==1.0.0->d2l==1.0.3)\n",
            "  Downloading qtconsole-5.6.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (10.4.0)\n",
            "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib==3.7.2->d2l==1.0.3)\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline==0.1.6->d2l==1.0.3) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->d2l==1.0.3) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l==1.0.3) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.6.9)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.0.13)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (2.18.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.1.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter==1.0.0->d2l==1.0.3)\n",
            "  Downloading QtPy-2.4.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.23.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l==1.0.3) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l==1.0.3) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.2.2)\n",
            "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qtconsole-5.6.0-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: requests, qtpy, pyparsing, numpy, matplotlib-inline, jedi, scipy, pandas, matplotlib, qtconsole, jupyter, d2l\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.4\n",
            "    Uninstalling pyparsing-3.1.4:\n",
            "      Successfully uninstalled pyparsing-3.1.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: matplotlib-inline\n",
            "    Found existing installation: matplotlib-inline 0.1.7\n",
            "    Uninstalling matplotlib-inline-0.1.7:\n",
            "      Successfully uninstalled matplotlib-inline-0.1.7\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.16 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.18.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 2.0.3 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.23.5 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 2.0.3 which is incompatible.\n",
            "xarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed d2l-1.0.3 jedi-0.19.1 jupyter-1.0.0 matplotlib-3.7.2 matplotlib-inline-0.1.6 numpy-1.23.5 pandas-2.0.3 pyparsing-3.0.9 qtconsole-5.6.0 qtpy-2.4.1 requests-2.31.0 scipy-1.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "matplotlib_inline",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "384cbae8592f4d558ceda6c3ddde97d7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install d2l==1.0.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3ed4012",
      "metadata": {
        "id": "c3ed4012"
      },
      "source": [
        "# Deep Convolutional Neural Networks (AlexNet)\n",
        ":label:`sec_alexnet`\n",
        "\n",
        "\n",
        "Although CNNs were well known\n",
        "in the computer vision and machine learning communities\n",
        "following the introduction of LeNet :cite:`LeCun.Jackel.Bottou.ea.1995`,\n",
        "they did not immediately dominate the field.\n",
        "Although LeNet achieved good results on early small datasets,\n",
        "the performance and feasibility of training CNNs\n",
        "on larger, more realistic datasets had yet to be established.\n",
        "In fact, for much of the intervening time between the early 1990s\n",
        "and the watershed results of 2012 :cite:`Krizhevsky.Sutskever.Hinton.2012`,\n",
        "neural networks were often surpassed by other machine learning methods,\n",
        "such as kernel methods :cite:`Scholkopf.Smola.2002`, ensemble methods :cite:`Freund.Schapire.ea.1996`,\n",
        "and structured estimation :cite:`Taskar.Guestrin.Koller.2004`.\n",
        "\n",
        "For computer vision, this comparison is perhaps not entirely accurate.\n",
        "That is, although the inputs to convolutional networks\n",
        "consist of raw or lightly-processed (e.g., by centering) pixel values, practitioners would never feed raw pixels into traditional models.\n",
        "Instead, typical computer vision pipelines\n",
        "consisted of manually engineering feature extraction pipelines, such as SIFT :cite:`Lowe.2004`, SURF :cite:`Bay.Tuytelaars.Van-Gool.2006`, and bags of visual words :cite:`Sivic.Zisserman.2003`.\n",
        "Rather than *learning* the features, the features were *crafted*.\n",
        "Most of the progress came from having more clever ideas for feature extraction on the one hand and deep insight into geometry :cite:`Hartley.Zisserman.2000` on the other. The learning algorithm was often considered an afterthought.\n",
        "\n",
        "Although some neural network accelerators were available in the 1990s,\n",
        "they were not yet sufficiently powerful to make\n",
        "deep multichannel, multilayer CNNs\n",
        "with a large number of parameters. For instance, NVIDIA's GeForce 256 from 1999\n",
        "was able to process at most 480 million floating-point operations, such as additions and multiplications, per second (MFLOPS), without any meaningful\n",
        "programming framework for operations beyond games. Today's accelerators are able to perform in excess of 1000 TFLOPs per device.\n",
        "Moreover, datasets were still relatively small: OCR on 60,000 low-resolution $28 \\times 28$ pixel images was considered a highly challenging task.\n",
        "Added to these obstacles, key tricks for training neural networks\n",
        "including parameter initialization heuristics :cite:`Glorot.Bengio.2010`,\n",
        "clever variants of stochastic gradient descent :cite:`Kingma.Ba.2014`,\n",
        "non-squashing activation functions :cite:`Nair.Hinton.2010`,\n",
        "and effective regularization techniques :cite:`Srivastava.Hinton.Krizhevsky.ea.2014` were still missing.\n",
        "\n",
        "Thus, rather than training *end-to-end* (pixel to classification) systems,\n",
        "classical pipelines looked more like this:\n",
        "\n",
        "1. Obtain an interesting dataset. In the early days, these datasets required expensive sensors. For instance, the [Apple QuickTake 100](https://en.wikipedia.org/wiki/Apple_QuickTake) of 1994 sported a whopping 0.3 megapixel (VGA) resolution, capable of storing up to 8 images, all for the price of \\$1000.\n",
        "1. Preprocess the dataset with hand-crafted features based on some knowledge of optics, geometry, other analytic tools, and occasionally on the serendipitous discoveries by lucky graduate students.\n",
        "1. Feed the data through a standard set of feature extractors such as the SIFT (scale-invariant feature transform) :cite:`Lowe.2004`, the SURF (speeded up robust features) :cite:`Bay.Tuytelaars.Van-Gool.2006`, or any number of other hand-tuned pipelines. OpenCV still provides SIFT extractors to this day!\n",
        "1. Dump the resulting representations into your favorite classifier, likely a linear model or kernel method, to train a classifier.\n",
        "\n",
        "If you spoke to machine learning researchers,\n",
        "they would reply that machine learning was both important and beautiful.\n",
        "Elegant theories proved the properties of various classifiers :cite:`boucheron2005theory` and convex\n",
        "optimization :cite:`Boyd.Vandenberghe.2004` had become the mainstay for obtaining them.\n",
        "The field of machine learning was thriving, rigorous, and eminently useful. However,\n",
        "if you spoke to a computer vision researcher,\n",
        "you would hear a very different story.\n",
        "The dirty truth of image recognition, they would tell you,\n",
        "is that features, geometry :cite:`Hartley.Zisserman.2000,hartley2009global`, and engineering,\n",
        "rather than novel learning algorithms, drove progress.\n",
        "Computer vision researchers justifiably believed\n",
        "that a slightly bigger or cleaner dataset\n",
        "or a slightly improved feature-extraction pipeline\n",
        "mattered far more to the final accuracy than any learning algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07938be6",
      "metadata": {
        "id": "07938be6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3425d857",
      "metadata": {
        "id": "3425d857"
      },
      "source": [
        "## Representation Learning\n",
        "\n",
        "Another way to cast the state of affairs is that\n",
        "the most important part of the pipeline was the representation.\n",
        "And up until 2012 the representation was calculated mostly mechanically.\n",
        "In fact, engineering a new set of feature functions, improving results, and writing up the method\n",
        "all featured prominently in papers.\n",
        "SIFT :cite:`Lowe.2004`,\n",
        "SURF :cite:`Bay.Tuytelaars.Van-Gool.2006`,\n",
        "HOG (histograms of oriented gradient) :cite:`Dalal.Triggs.2005`,\n",
        "bags of visual words :cite:`Sivic.Zisserman.2003`,\n",
        "and similar feature extractors ruled the roost.\n",
        "\n",
        "Another group of researchers,\n",
        "including Yann LeCun, Geoff Hinton, Yoshua Bengio,\n",
        "Andrew Ng, Shun-ichi Amari, and Juergen Schmidhuber,\n",
        "had different plans.\n",
        "They believed that features themselves ought to be learned.\n",
        "Moreover, they believed that to be reasonably complex,\n",
        "the features ought to be hierarchically composed\n",
        "with multiple jointly learned layers, each with learnable parameters.\n",
        "In the case of an image, the lowest layers might come\n",
        "to detect edges, colors, and textures, by analogy with how the visual system in animals\n",
        "processes its input. In particular, the automatic design of visual features such as those obtained\n",
        "by sparse coding :cite:`olshausen1996emergence` remained an open challenge until the advent of modern CNNs.\n",
        "It was not until :citet:`Dean.Corrado.Monga.ea.2012,le2013building` that the idea of generating features\n",
        "from image data automatically gained significant traction.\n",
        "\n",
        "The first modern CNN :cite:`Krizhevsky.Sutskever.Hinton.2012`, named\n",
        "*AlexNet* after one of its inventors, Alex Krizhevsky, is largely an evolutionary improvement\n",
        "over LeNet. It achieved excellent performance in the 2012 ImageNet challenge.\n",
        "\n",
        "![Image filters learned by the first layer of AlexNet. Reproduction courtesy of :citet:`Krizhevsky.Sutskever.Hinton.2012`.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/filters.png?raw=1)\n",
        ":width:`400px`\n",
        ":label:`fig_filters`\n",
        "\n",
        "Interestingly, in the lowest layers of the network,\n",
        "the model learned feature extractors that resembled some traditional filters.\n",
        ":numref:`fig_filters`\n",
        "shows lower-level image descriptors.\n",
        "Higher layers in the network might build upon these representations\n",
        "to represent larger structures, like eyes, noses, blades of grass, and so on.\n",
        "Even higher layers might represent whole objects\n",
        "like people, airplanes, dogs, or frisbees.\n",
        "Ultimately, the final hidden state learns a compact representation\n",
        "of the image that summarizes its contents\n",
        "such that data belonging to different categories can be easily separated.\n",
        "\n",
        "AlexNet (2012) and its precursor LeNet (1995) share many architectural elements. This begs the question: why did it take so long?\n",
        "A key difference was that, over the previous two decades, the amount of data and the computing power available had increased significantly. As such AlexNet was much larger: it was trained on much more data, and on much faster GPUs compared to the CPUs available in 1995.\n",
        "\n",
        "### Missing Ingredient: Data\n",
        "\n",
        "Deep models with many layers require large amounts of data\n",
        "in order to enter the regime\n",
        "where they significantly outperform traditional methods\n",
        "based on convex optimizations (e.g., linear and kernel methods).\n",
        "However, given the limited storage capacity of computers,\n",
        "the relative expense of (imaging) sensors,\n",
        "and the comparatively tighter research budgets in the 1990s,\n",
        "most research relied on tiny datasets.\n",
        "Numerous papers relied on the UCI collection of datasets,\n",
        "many of which contained only hundreds or (a few) thousands of images\n",
        "captured in low resolution and often with an artificially clean background.\n",
        "\n",
        "In 2009, the ImageNet dataset was released :cite:`Deng.Dong.Socher.ea.2009`,\n",
        "challenging researchers to learn models from 1 million examples,\n",
        "1000 each from 1000 distinct categories of objects. The categories themselves\n",
        "were based on the most popular noun nodes in WordNet :cite:`Miller.1995`.\n",
        "The ImageNet team used Google Image Search to prefilter large candidate sets\n",
        "for each category and employed\n",
        "the Amazon Mechanical Turk crowdsourcing pipeline\n",
        "to confirm for each image whether it belonged to the associated category.\n",
        "This scale was unprecedented, exceeding others by over an order of magnitude\n",
        "(e.g., CIFAR-100 has 60,000 images). Another aspect was that the images were at\n",
        "relatively high resolution of $224 \\times 224$ pixels, unlike the 80 million-sized\n",
        "TinyImages dataset :cite:`Torralba.Fergus.Freeman.2008`, consisting of $32 \\times 32$ pixel thumbnails.\n",
        "This allowed for the formation of higher-level features.\n",
        "The associated competition, dubbed the ImageNet Large Scale Visual Recognition\n",
        "Challenge :cite:`russakovsky2015imagenet`,\n",
        "pushed computer vision and machine learning research forward,\n",
        "challenging researchers to identify which models performed best\n",
        "at a greater scale than academics had previously considered. The largest vision datasets, such as LAION-5B\n",
        ":cite:`schuhmann2022laion` contain billions of images with additional metadata.\n",
        "\n",
        "### Missing Ingredient: Hardware\n",
        "\n",
        "Deep learning models are voracious consumers of compute cycles.\n",
        "Training can take hundreds of epochs, and each iteration\n",
        "requires passing data through many layers of computationally expensive\n",
        "linear algebra operations.\n",
        "This is one of the main reasons why in the 1990s and early 2000s,\n",
        "simple algorithms based on the more-efficiently optimized\n",
        "convex objectives were preferred.\n",
        "\n",
        "*Graphical processing units* (GPUs) proved to be a game changer\n",
        "in making deep learning feasible.\n",
        "These chips had earlier been developed for accelerating\n",
        "graphics processing to benefit computer games.\n",
        "In particular, they were optimized for high throughput $4 \\times 4$\n",
        "matrix--vector products, which are needed for many computer graphics tasks.\n",
        "Fortunately, the math is strikingly similar\n",
        "to that required for calculating convolutional layers.\n",
        "Around that time, NVIDIA and ATI had begun optimizing GPUs\n",
        "for general computing operations :cite:`Fernando.2004`,\n",
        "going as far as to market them as *general-purpose GPUs* (GPGPUs).\n",
        "\n",
        "To provide some intuition, consider the cores of a modern microprocessor\n",
        "(CPU).\n",
        "Each of the cores is fairly powerful running at a high clock frequency\n",
        "and sporting large caches (up to several megabytes of L3).\n",
        "Each core is well-suited to executing a wide range of instructions,\n",
        "with branch predictors, a deep pipeline, specialized execution units,\n",
        "speculative execution,\n",
        "and many other bells and whistles\n",
        "that enable it to run a large variety of programs with sophisticated control flow.\n",
        "This apparent strength, however, is also its Achilles heel:\n",
        "general-purpose cores are very expensive to build. They excel at general-purpose\n",
        "code with lots of control flow.\n",
        "This requires lots of chip area, not just for the\n",
        "actual ALU (arithmetic logical unit) where computation happens, but also for\n",
        "all the aforementioned bells and whistles, plus\n",
        "memory interfaces, caching logic between cores,\n",
        "high-speed interconnects, and so on. CPUs are\n",
        "comparatively bad at any single task when compared with dedicated hardware.\n",
        "Modern laptops have 4--8 cores,\n",
        "and even high-end servers rarely exceed 64 cores per socket,\n",
        "simply because it is not cost-effective.\n",
        "\n",
        "By comparison, GPUs can consist of thousands of small processing elements (NIVIDA's latest Ampere chips have up to 6912 CUDA cores), often grouped into larger groups (NVIDIA calls them warps).\n",
        "The details differ somewhat between NVIDIA, AMD, ARM and other chip vendors. While each core is relatively weak,\n",
        "running at about 1GHz clock frequency,\n",
        "it is the total number of such cores that makes GPUs orders of magnitude faster than CPUs.\n",
        "For instance, NVIDIA's recent Ampere A100 GPU offers over 300 TFLOPs per chip for specialized 16-bit precision (BFLOAT16) matrix-matrix multiplications, and up to 20 TFLOPs for more general-purpose floating point operations (FP32).\n",
        "At the same time, floating point performance of CPUs rarely exceeds 1 TFLOPs. For instance, Amazon's Graviton 3  reaches 2 TFLOPs peak performance for 16-bit precision operations, a number similar to the GPU performance of Apple's M1 processor.\n",
        "\n",
        "There are many reasons why GPUs are much faster than CPUs in terms of FLOPs.\n",
        "First, power consumption tends to grow *quadratically* with clock frequency.\n",
        "Hence, for the power budget of a CPU core that runs four times faster (a typical number),\n",
        "you can use 16 GPU cores at $\\frac{1}{4}$ the speed,\n",
        "which yields $16 \\times \\frac{1}{4} = 4$ times the performance.\n",
        "Second, GPU cores are much simpler\n",
        "(in fact, for a long time they were not even *able*\n",
        "to execute general-purpose code),\n",
        "which makes them more energy efficient. For instance, (i) they tend not to support speculative evaluation, (ii) it typically is not possible to program each processing element individually, and (iii) the caches per core tend to be much smaller.\n",
        "Last, many operations in deep learning require high memory bandwidth.\n",
        "Again, GPUs shine here with buses that are at least 10 times as wide as many CPUs.\n",
        "\n",
        "Back to 2012. A major breakthrough came\n",
        "when Alex Krizhevsky and Ilya Sutskever\n",
        "implemented a deep CNN\n",
        "that could run on GPUs.\n",
        "They realized that the computational bottlenecks in CNNs,\n",
        "convolutions and matrix multiplications,\n",
        "are all operations that could be parallelized in hardware.\n",
        "Using two NVIDIA GTX 580s with 3GB of memory, either of which was capable of 1.5 TFLOPs (still a challenge for most CPUs a decade later),\n",
        "they implemented fast convolutions.\n",
        "The [cuda-convnet](https://code.google.com/archive/p/cuda-convnet/) code\n",
        "was good enough that for several years\n",
        "it was the industry standard and powered\n",
        "the first couple of years of the deep learning boom.\n",
        "\n",
        "## AlexNet\n",
        "\n",
        "AlexNet, which employed an 8-layer CNN,\n",
        "won the ImageNet Large Scale Visual Recognition Challenge 2012\n",
        "by a large margin :cite:`Russakovsky.Deng.Huang.ea.2013`.\n",
        "This network showed, for the first time,\n",
        "that the features obtained by learning can transcend manually-designed features, breaking the previous paradigm in computer vision.\n",
        "\n",
        "The architectures of AlexNet and LeNet are strikingly similar,\n",
        "as :numref:`fig_alexnet` illustrates.\n",
        "Note that we provide a slightly streamlined version of AlexNet\n",
        "removing some of the design quirks that were needed in 2012\n",
        "to make the model fit on two small GPUs.\n",
        "\n",
        "![From LeNet (left) to AlexNet (right).](http://d2l.ai/_images/alexnet.svg)\n",
        ":label:`fig_alexnet`\n",
        "\n",
        "There are also significant differences between AlexNet and LeNet.\n",
        "First, AlexNet is much deeper than the comparatively small LeNet-5.\n",
        "AlexNet consists of eight layers: five convolutional layers,\n",
        "two fully connected hidden layers, and one fully connected output layer.\n",
        "Second, AlexNet used the ReLU instead of the sigmoid\n",
        "as its activation function. Let's delve into the details below.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "In AlexNet's first layer, the convolution window shape is $11\\times11$.\n",
        "Since the images in ImageNet are eight times taller and wider\n",
        "than the MNIST images,\n",
        "objects in ImageNet data tend to occupy more pixels with more visual detail.\n",
        "Consequently, a larger convolution window is needed to capture the object.\n",
        "The convolution window shape in the second layer\n",
        "is reduced to $5\\times5$, followed by $3\\times3$.\n",
        "In addition, after the first, second, and fifth convolutional layers,\n",
        "the network adds max-pooling layers\n",
        "with a window shape of $3\\times3$ and a stride of 2.\n",
        "Moreover, AlexNet has ten times more convolution channels than LeNet.\n",
        "\n",
        "After the final convolutional layer, there are two huge fully connected layers\n",
        "with 4096 outputs.\n",
        "These layers require nearly 1GB model parameters.\n",
        "Because of the limited memory in early GPUs,\n",
        "the original AlexNet used a dual data stream design,\n",
        "so that each of their two GPUs could be responsible\n",
        "for storing and computing only its half of the model.\n",
        "Fortunately, GPU memory is comparatively abundant now,\n",
        "so we rarely need to break up models across GPUs these days\n",
        "(our version of the AlexNet model deviates\n",
        "from the original paper in this aspect).\n",
        "\n",
        "### Activation Functions\n",
        "\n",
        "Furthermore, AlexNet changed the sigmoid activation function to a simpler ReLU activation function. On the one hand, the computation of the ReLU activation function is simpler. For example, it does not have the exponentiation operation found in the sigmoid activation function.\n",
        " On the other hand, the ReLU activation function makes model training easier when using different parameter initialization methods. This is because, when the output of the sigmoid activation function is very close to 0 or 1, the gradient of these regions is almost 0, so that backpropagation cannot continue to update some of the model parameters. By contrast, the gradient of the ReLU activation function in the positive interval is always 1 (:numref:`subsec_activation-functions`). Therefore, if the model parameters are not properly initialized, the sigmoid function may obtain a gradient of almost 0 in the positive interval, meaning that the model cannot be effectively trained.\n",
        "\n",
        "### Capacity Control and Preprocessing\n",
        "\n",
        "AlexNet controls the model complexity of the fully connected layer\n",
        "by dropout (:numref:`sec_dropout`),\n",
        "while LeNet only uses weight decay.\n",
        "To augment the data even further, the training loop of AlexNet\n",
        "added a great deal of image augmentation,\n",
        "such as flipping, clipping, and color changes.\n",
        "This makes the model more robust and the larger sample size effectively reduces overfitting.\n",
        "See :citet:`Buslaev.Iglovikov.Khvedchenya.ea.2020` for an in-depth review of such preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29feac8e",
      "metadata": {
        "id": "29feac8e"
      },
      "outputs": [],
      "source": [
        "class AlexNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59e725f1",
      "metadata": {
        "id": "59e725f1"
      },
      "source": [
        "We [**construct a single-channel data example**] with both height and width of 224 (**to observe the output shape of each layer**). It matches the AlexNet architecture in :numref:`fig_alexnet`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5c2c0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d5c2c0a",
        "outputId": "f2c20112-d810-41d4-ebb4-4fccce380dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
            "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
            "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
            "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
            "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
            "Flatten output shape:\t torch.Size([1, 6400])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "AlexNet().layer_summary((1, 1, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193ba4c7",
      "metadata": {
        "id": "193ba4c7"
      },
      "source": [
        "## Training\n",
        "\n",
        "Although AlexNet was trained on ImageNet in :citet:`Krizhevsky.Sutskever.Hinton.2012`,\n",
        "we use Fashion-MNIST here\n",
        "since training an ImageNet model to convergence could take hours or days\n",
        "even on a modern GPU.\n",
        "One of the problems with applying AlexNet directly on [**Fashion-MNIST**]\n",
        "is that its (**images have lower resolution**) ($28 \\times 28$ pixels)\n",
        "(**than ImageNet images.**)\n",
        "To make things work, (**we upsample them to $224 \\times 224$**).\n",
        "This is generally not a smart practice, as it simply increases the computational\n",
        "complexity without adding information. Nonetheless, we do it here to be faithful to the AlexNet architecture.\n",
        "We perform this resizing with the `resize` argument in the `d2l.FashionMNIST` constructor.\n",
        "\n",
        "Now, we can [**start training AlexNet.**]\n",
        "Compared to LeNet in :numref:`sec_lenet`,\n",
        "the main change here is the use of a smaller learning rate\n",
        "and much slower training due to the deeper and wider network,\n",
        "the higher image resolution, and the more costly convolutions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd6a3e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "acd6a3e0",
        "outputId": "6c702ff7-4b13-4095-80a0-075dce492d7f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T08:08:27.982627</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 238.965625 183.35625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"mcbaa8af1db\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mcbaa8af1db\" x=\"30.103125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#mcbaa8af1db\" x=\"69.163125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#mcbaa8af1db\" x=\"108.223125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mcbaa8af1db\" x=\"147.283125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#mcbaa8af1db\" x=\"186.343125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mcbaa8af1db\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m4c074ed1ae\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m4c074ed1ae\" x=\"30.103125\" y=\"121.976374\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 125.775592) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m4c074ed1ae\" x=\"30.103125\" y=\"91.88145\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 95.680669) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m4c074ed1ae\" x=\"30.103125\" y=\"61.786526\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.5 -->\n      <g transform=\"translate(7.2 65.585745) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m4c074ed1ae\" x=\"30.103125\" y=\"31.691603\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 35.490822) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path d=\"M 34.954394 13.5 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 49.633125 13.621774 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_15\"/>\n   <g id=\"line2d_16\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 49.633125 13.621774 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 49.633125 131.125088 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 49.633125 13.621774 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 49.633125 131.125088 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 49.633125 13.621774 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 49.633125 131.125088 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 49.633125 131.125088 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \nL 190.861259 122.402483 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \nL 190.861259 122.402483 \nL 200.605438 123.844053 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \nL 190.861259 122.402483 \nL 200.605438 123.844053 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \nL 205.873125 124.589795 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \nL 190.861259 122.402483 \nL 200.605438 123.844053 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \nL 205.873125 124.589795 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \nL 205.873125 101.97063 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \nL 190.861259 122.402483 \nL 200.605438 123.844053 \nL 210.349618 124.305174 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \nL 205.873125 124.589795 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \nL 205.873125 101.97063 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \nL 190.861259 122.402483 \nL 200.605438 123.844053 \nL 210.349618 124.305174 \nL 220.093797 125.281445 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \nL 205.873125 124.589795 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \nL 205.873125 101.97063 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \nL 190.861259 122.402483 \nL 200.605438 123.844053 \nL 210.349618 124.305174 \nL 220.093797 125.281445 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \nL 205.873125 124.589795 \nL 225.403125 125.332058 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \nL 205.873125 101.97063 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.558521 \nL 54.442752 13.695691 \nL 64.186931 14.30653 \nL 73.93111 60.982784 \nL 83.675289 91.729958 \nL 93.419468 98.879832 \nL 103.163647 103.466676 \nL 112.907826 107.341738 \nL 122.652006 110.56473 \nL 132.396185 113.128274 \nL 142.140364 116.004955 \nL 151.884543 117.496286 \nL 161.628722 119.188566 \nL 171.372901 120.427255 \nL 181.11708 121.806164 \nL 190.861259 122.402483 \nL 200.605438 123.844053 \nL 210.349618 124.305174 \nL 220.093797 125.281445 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 49.633125 13.621774 \nL 69.163125 16.361027 \nL 88.693125 94.686812 \nL 108.223125 105.017357 \nL 127.753125 112.068737 \nL 147.283125 117.359373 \nL 166.813125 120.634502 \nL 186.343125 122.717937 \nL 205.873125 124.589795 \nL 225.403125 125.332058 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 49.633125 131.125088 \nL 69.163125 139.5 \nL 88.693125 114.577641 \nL 108.223125 110.434828 \nL 127.753125 106.500345 \nL 147.283125 104.952742 \nL 166.813125 103.589661 \nL 186.343125 102.542053 \nL 205.873125 101.97063 \nL 225.403125 101.696824 \n\" clip-path=\"url(#pab27cbc266)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 145.8 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 145.8 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 138.8125 60.06875 \nL 218.403125 60.06875 \nQ 220.403125 60.06875 220.403125 58.06875 \nL 220.403125 14.2 \nQ 220.403125 12.2 218.403125 12.2 \nL 138.8125 12.2 \nQ 136.8125 12.2 136.8125 14.2 \nL 136.8125 58.06875 \nQ 136.8125 60.06875 138.8125 60.06875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_127\">\n     <path d=\"M 140.8125 20.298438 \nL 150.8125 20.298438 \nL 160.8125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- train_loss -->\n     <g transform=\"translate(168.8125 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_128\">\n     <path d=\"M 140.8125 35.254688 \nL 150.8125 35.254688 \nL 160.8125 35.254688 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- val_loss -->\n     <g transform=\"translate(168.8125 38.754688) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_129\">\n     <path d=\"M 140.8125 50.210938 \nL 150.8125 50.210938 \nL 160.8125 50.210938 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- val_acc -->\n     <g transform=\"translate(168.8125 53.710938) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pab27cbc266\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = AlexNet(lr=0.01)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c33357",
      "metadata": {
        "id": "f9c33357"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "AlexNet's structure bears a striking resemblance to LeNet, with a number of critical improvements, both for accuracy (dropout) and for ease of training (ReLU). What is equally striking is the amount of progress that has been made in terms of deep learning tooling. What was several months of work in 2012 can now be accomplished in a dozen lines of code using any modern framework.\n",
        "\n",
        "Reviewing the architecture, we see that AlexNet has an Achilles heel when it comes to efficiency: the last two hidden layers require matrices of size $6400 \\times 4096$ and $4096 \\times 4096$, respectively. This corresponds to 164 MB of memory and 81 MFLOPs of computation, both of which are a nontrivial outlay, especially on smaller devices, such as mobile phones. This is one of the reasons why AlexNet has been surpassed by much more effective architectures that we will cover in the following sections. Nonetheless, it is a key step from shallow to deep networks that are used nowadays. Note that even though the number of parameters exceeds by far the amount of training data in our experiments (the last two layers have more than 40 million parameters, trained on a datasets of 60 thousand images), there is hardly any overfitting: training and validation loss are virtually identical throughout training. This is due to the improved regularization, such as dropout, inherent in modern deep network designs.\n",
        "\n",
        "Although it seems that there are only a few more lines in AlexNet's implementation than in LeNet's, it took the academic community many years to embrace this conceptual change and take advantage of its excellent experimental results. This was also due to the lack of efficient computational tools. At the time neither DistBelief :cite:`Dean.Corrado.Monga.ea.2012` nor Caffe :cite:`Jia.Shelhamer.Donahue.ea.2014` existed, and Theano :cite:`Bergstra.Breuleux.Bastien.ea.2010` still lacked many distinguishing features. It was the availability of TensorFlow :cite:`Abadi.Barham.Chen.ea.2016` that dramatically changed the situation.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Following up on the discussion above, analyze the computational properties of AlexNet.\n",
        "    1. Compute the memory footprint for convolutions and fully connected layers, respectively. Which one dominates?\n",
        "    1. Calculate the computational cost for the convolutions and the fully connected layers.\n",
        "    1. How does the memory (read and write bandwidth, latency, size) affect computation? Is there any difference in its effects for training and inference?\n",
        "1. You are a chip designer and need to trade off computation and memory bandwidth. For example, a faster chip requires more power and possibly a larger chip area. More memory bandwidth requires more pins and control logic, thus also more area. How do you optimize?\n",
        "1. Why do engineers no longer report performance benchmarks on AlexNet?\n",
        "1. Try increasing the number of epochs when training AlexNet. Compared with LeNet, how do the results differ? Why?\n",
        "1. AlexNet may be too complex for the Fashion-MNIST dataset, in particular due to the low resolution of the initial images.\n",
        "    1. Try simplifying the model to make the training faster, while ensuring that the accuracy does not drop significantly.\n",
        "    1. Design a better model that works directly on $28 \\times 28$ images.\n",
        "1. Modify the batch size, and observe the changes in throughput (images/s), accuracy, and GPU memory.\n",
        "1. Apply dropout and ReLU to LeNet-5. Does it improve? Can you improve things further by preprocessing to take advantage of the invariances inherent in the images?\n",
        "1. Can you make AlexNet overfit? Which feature do you need to remove or change to break training?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Fomula of the number of parameters of convolutions is\n",
        "∑(ci * c0 * kh * kw+c0) = 3747200\n",
        "\n",
        "\n",
        "Fomula of the number of parameters of fully connected is\n",
        "∑(xi * x0+x0) = 43040778\n",
        "\n",
        "So, The **fully connected layers dominates**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ryzbP0qPqei-"
      },
      "id": "ryzbP0qPqei-"
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2\n",
        "x = torch.randn(1,3, 224, 224)\n",
        "params = {'conv':0, 'lr':0}\n",
        "for idx, module in enumerate(model.net):\n",
        "    c_i = x.shape[1]\n",
        "    x = module(x)\n",
        "    if type(module) == nn.Conv2d:\n",
        "        k = [p.shape for p in module.parameters()]\n",
        "        c_o,h_o,w_o = x.shape[1], x.shape[2], x.shape[3]\n",
        "        params['conv'] += c_i*c_o*h_o*w_o*k[0][-1]*k[0][-2]\n",
        "    if type(module) == nn.Linear:\n",
        "        params['lr'] += sum(p.numel() for p in module.parameters())\n",
        "params\n"
      ],
      "metadata": {
        "id": "Uv9fSpehgMLX"
      },
      "id": "Uv9fSpehgMLX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'conv': 962858112, 'lr': 43040778}\n"
      ],
      "metadata": {
        "id": "m4RR1GpjgOLN"
      },
      "id": "m4RR1GpjgOLN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 How does the memory (read and write bandwidth, latency, size) affect computation? Is there any difference in its effects for training and inference?\n",
        "\n",
        "Memory characteristics like read and write bandwidth, latency, and size significantly impact the performance of both training and inference in neural networks. Here's a breakdown:\n",
        "\n",
        "- **Read and Write Bandwidth:** High bandwidth allows faster data transfer between memory and processing units, improving overall computation speed. It reduces data bottlenecks and enhances both training and inference performance.\n",
        "  \n",
        "- **Latency:** Lower latency means quicker data access, leading to faster computations. In training, this reduces time spent waiting during frequent weight updates, while in inference, low latency ensures quicker response times, especially for real-time applications.\n",
        "\n",
        "- **Memory Size:** Larger memory enables more data to be cached, reducing data movement and improving performance. During training, it helps store gradients and activations, while in inference, it supports storing intermediate results.\n",
        "\n",
        "### Differences Between Training and Inference:\n",
        "- Training often demands higher memory bandwidth due to frequent updates during backpropagation, particularly in batch processing.\n",
        "- Inference requires low latency for quick responses, with a focus on minimizing response time in real-time applications.\n",
        "\n",
        "Efficient memory hierarchy (cache, GPU memory, etc.) is crucial for both training and inference, impacting how data is accessed and utilized. In short, optimizing memory characteristics boosts computation efficiency in both phases."
      ],
      "metadata": {
        "id": "3o9jxFETgVvO"
      },
      "id": "3o9jxFETgVvO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. As a chip designer, optimizing the trade-off between computation and memory bandwidth involves balancing performance, power, and area (PPA). To do this effectively, here are some strategies:\n",
        "\n",
        "### 1. **Compute vs. Memory Bound Workloads**:\n",
        "   - **Analyze Workload Characteristics**: Determine if the workload is compute-bound (limited by processing speed) or memory-bound (limited by data access speed). If it's compute-bound, prioritize improving computation efficiency (e.g., using faster processing cores). If it's memory-bound, focus on optimizing memory bandwidth and latency.\n",
        "   - **Local Caching**: Utilize on-chip cache to reduce the need for frequent memory access, lowering the demand for memory bandwidth. A well-designed cache hierarchy (e.g., L1, L2, and L3 caches) can significantly enhance data reuse and reduce off-chip bandwidth requirements.\n",
        "\n",
        "### 2. **Optimize Power and Area**:\n",
        "   - **Specialized Accelerators**: Use domain-specific accelerators (e.g., Tensor Cores, AI accelerators) for tasks like matrix multiplications that dominate neural networks. These accelerators improve compute efficiency without drastically increasing power and area.\n",
        "   - **Memory Access Compression**: Implement techniques like memory compression or quantization to reduce data movement, effectively decreasing memory bandwidth requirements without sacrificing significant computational power.\n",
        "   - **Lower Precision Computations**: In many AI applications, lower precision (e.g., 16-bit or 8-bit floating point) is sufficient. This reduces both memory bandwidth and computational load, improving power efficiency and enabling smaller chip areas.\n",
        "\n",
        "### 3. **Balance Power and Performance**:\n",
        "   - **Power Gating and Dynamic Voltage Scaling**: Use techniques like dynamic voltage and frequency scaling (DVFS) to balance power consumption and performance based on the workload. This allows you to reduce power when full computational resources aren’t needed.\n",
        "   - **Parallelism and Pipelining**: Increase parallelism (e.g., multi-core designs) and pipeline stages to enhance performance, while optimizing power consumption. Carefully designed parallelism can improve throughput without significantly increasing power or area.\n",
        "\n",
        "### 4. **Pin and Control Logic Optimization**:\n",
        "   - **SerDes (Serializer/Deserializer) for Fewer Pins**: Use high-speed serial interfaces (e.g., SerDes) to reduce the number of pins required for memory bandwidth, minimizing the area consumed by control logic.\n",
        "   - **Bus Width vs. Frequency**: Increasing the memory bus width or clock frequency can improve bandwidth, but each approach comes with trade-offs in terms of power and area. A balance between bus width and frequency can achieve desired performance without excessive pin count or control overhead.\n",
        "\n",
        "### 5. **Memory Access Patterns**:\n",
        "   - **Data Locality Optimization**: Organize data in memory to exploit spatial and temporal locality, reducing the need for frequent memory access and improving bandwidth utilization. This includes optimizing algorithms to minimize cache misses and leverage data reuse.\n",
        "   - **Memory Prefetching**: Intelligent prefetching of data into cache can hide memory latency and reduce the demand for immediate memory access, improving both performance and memory bandwidth efficiency.\n",
        "\n",
        "### 6. **Co-Design Approach**:\n",
        "   - **Hardware-Software Co-Design**: Optimize both hardware and software together. For example, tailoring software algorithms to better utilize on-chip memory (e.g., tiled computations or reducing redundant memory accesses) can lessen the strain on memory bandwidth and improve overall system efficiency.\n",
        "   - **Heterogeneous Computing**: Leverage heterogeneous architectures (combining CPUs, GPUs, and specialized accelerators) to distribute workloads more efficiently, optimizing both computational resources and memory bandwidth usage.\n",
        "\n",
        "In summary, optimization involves finding the right balance between compute efficiency, memory bandwidth, power, and chip area. Techniques such as caching, compression, parallelism, and hardware-software co-design are essential to achieve an optimal trade-off. By carefully analyzing workload requirements and prioritizing key factors, you can design a chip that meets performance goals while managing power and area constraints effectively."
      ],
      "metadata": {
        "id": "25KjquXwglsV"
      },
      "id": "25KjquXwglsV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Engineers no longer report performance benchmarks on AlexNet for several reasons, as deep learning models and technology have significantly evolved since AlexNet’s introduction in 2012:\n",
        "\n",
        "### 1. **Outdated Architecture**:\n",
        "   - **AlexNet is Considered Obsolete**: Since its breakthrough in 2012, AlexNet has been surpassed by more advanced neural network architectures like VGG, ResNet, Inception, and EfficientNet, which offer significantly better performance, efficiency, and accuracy. These newer models utilize deeper layers, more efficient designs, and improved techniques for handling large-scale data, making AlexNet less relevant for modern applications.\n",
        "   \n",
        "### 2. **Improved Accuracy and Efficiency**:\n",
        "   - **Higher-Performing Models Available**: Current state-of-the-art models achieve much higher accuracy on benchmarks like ImageNet compared to AlexNet. AlexNet’s top-5 error rate on ImageNet is around 15%, while modern architectures like EfficientNet, ResNet, and Vision Transformers achieve significantly lower error rates, often below 5%. Engineers now focus on more competitive and efficient models.\n",
        "   \n",
        "   - **AlexNet is Inefficient for Modern Hardware**: AlexNet’s architecture, designed for the hardware available in 2012, is less efficient on today’s accelerators, such as GPUs and TPUs. It lacks optimizations like batch normalization, depthwise separable convolutions, or residual connections, which newer models use to improve efficiency and scalability.\n",
        "\n",
        "### 3. **Changes in Benchmarking Standards**:\n",
        "   - **AlexNet is No Longer Challenging**: For most modern hardware, AlexNet has become a trivial benchmark. Its relatively shallow architecture and limited complexity do not fully utilize the computational power of today’s hardware accelerators. Engineers prefer using deeper, more complex models that push the limits of hardware performance and reflect real-world use cases better.\n",
        "   \n",
        "   - **New Benchmarks Reflect Real-World Applications**: The focus in benchmarking has shifted to models that are more representative of current tasks in computer vision, natural language processing, and other domains. Models like ResNet, EfficientNet, and Transformers (for vision and language) are now the standard benchmarks, as they are more reflective of the needs of modern AI systems.\n",
        "\n",
        "### 4. **Shift in Research Focus**:\n",
        "   - **Progress Beyond Image Classification**: Research focus has moved beyond basic image classification tasks, which AlexNet was primarily designed for. Today, engineers and researchers are more interested in complex tasks like object detection, segmentation, generative models, and multimodal models (e.g., combining vision and language). Benchmarking on models that address these tasks is more relevant to current research and industrial needs.\n",
        "   \n",
        "   - **Efficiency in Terms of Parameters and FLOPs**: In addition to accuracy, there is now a greater focus on model efficiency, including the number of parameters, floating-point operations (FLOPs), and memory consumption. Modern models are designed to balance accuracy with computational efficiency, which AlexNet does not do as effectively.\n",
        "\n",
        "### 5. **Availability of Better Tools**:\n",
        "   - **Better Benchmarking Frameworks**: Benchmarking tools have also evolved, allowing researchers to evaluate newer, more efficient models using modern metrics, datasets, and hardware configurations. Engineers are now focused on more comprehensive benchmarking suites that evaluate models across multiple tasks, scales, and hardware configurations.\n",
        "\n",
        "### 6. **Community and Industrial Trends**:\n",
        "   - **Shift in Industry Standards**: In the tech industry, there’s a preference for state-of-the-art models that reflect practical use cases and the performance of modern AI systems. Since AlexNet no longer offers competitive performance, companies and research institutions have shifted to using more relevant models in their benchmarks and comparisons.\n",
        "   \n",
        "   - **Benchmarks Aligned with Specialized Applications**: With the rise of specialized AI applications, benchmarks are now more focused on models suited for specific tasks (e.g., autonomous driving, medical imaging) rather than general-purpose architectures like AlexNet.\n",
        "\n",
        "In summary, AlexNet is no longer used in performance benchmarks because it has been surpassed by more accurate, efficient, and relevant models. Engineers now focus on more modern architectures that better reflect the demands of current hardware and applications."
      ],
      "metadata": {
        "id": "RWEXUmpfhRni"
      },
      "id": "RWEXUmpfhRni"
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "model = Alexnet(lr=0.01)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224),num_workers=0)\n",
        "trainer = d2l.Trainer(max_epochs=100)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "TpyS1elKhaBf"
      },
      "id": "TpyS1elKhaBf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "class SmallAlexnet(d2l.Classifier):\n",
        "    def __init__(self,lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1),nn.ReLU(),\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1),nn.ReLU(),\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1),nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LazyConv2d(512, kernel_size=3, padding=1),nn.ReLU(),\n",
        "            nn.LazyConv2d(512, kernel_size=3, padding=1),nn.ReLU(),\n",
        "            nn.LazyConv2d(256, kernel_size=3, padding=1),nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
        "            nn.LazyLinear(1024), nn.ReLU(),\n",
        "            nn.LazyLinear(num_classes)\n",
        "            )"
      ],
      "metadata": {
        "id": "dkBWN3XAhhWW"
      },
      "id": "dkBWN3XAhhWW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "class LeNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(nn.LazyConv2d(6, kernel_size=5, padding=2),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                 nn.LazyConv2d(16, kernel_size=5),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                 nn.Flatten(),\n",
        "                                 nn.LazyLinear(120),\n",
        "                                 nn.ReLU(), nn.Dropout(0.5),\n",
        "                                 nn.LazyLinear(84),\n",
        "                                 nn.ReLU(), nn.Dropout(0.5),\n",
        "                                 nn.LazyLinear(num_classes))\n",
        "model = LeNet(lr=0.01)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(28, 28))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "print(f'acc: {model.accuracy(y_hat,y).item():.2f}')\n"
      ],
      "metadata": {
        "id": "FCemu2PdhoqY"
      },
      "id": "FCemu2PdhoqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Yes, **AlexNet can overfit** if the model is exposed to certain conditions, such as training on a small dataset or with specific architectural changes that limit its regularization capabilities. Here are ways to make AlexNet overfit and some features you could change or remove to break its training:\n",
        "\n",
        "### How to Make AlexNet Overfit:\n",
        "1. **Train on a Small Dataset**:\n",
        "   - If AlexNet is trained on a very small dataset, it will likely memorize the training examples instead of learning generalized features. Overfitting occurs when the model performs very well on the training data but poorly on unseen test data.\n",
        "   \n",
        "2. **Remove Data Augmentation**:\n",
        "   - **AlexNet relies on data augmentation** (like random crops, flips, and image distortions) to increase the diversity of training data and avoid overfitting. By removing or reducing augmentation, the model is more likely to overfit because it will be exposed to a smaller variety of examples, increasing the chance of memorizing the training data.\n",
        "   \n",
        "3. **Use a Very Large Model Capacity**:\n",
        "   - Increasing the number of neurons or layers in AlexNet (by significantly expanding layer sizes) without sufficient regularization will increase the model’s capacity to memorize the training data, leading to overfitting.\n",
        "   \n",
        "4. **Reduce or Remove Regularization Techniques**:\n",
        "   - **Dropout**: AlexNet uses dropout (typically 0.5) in fully connected layers as a form of regularization. Dropout helps prevent overfitting by randomly disabling a portion of the neurons during training, forcing the model to learn more robust representations. By removing dropout, the model has a higher chance of overfitting.\n",
        "   - **L2 Regularization**: AlexNet might also include weight decay (L2 regularization) to penalize large weights, which helps generalization. Removing this regularization can allow the model weights to grow excessively, making overfitting more likely.\n",
        "\n",
        "5. **Train for Too Many Epochs**:\n",
        "   - Training AlexNet for too long (more epochs) on a fixed training set without early stopping can cause overfitting. The model will continue to improve on the training data while losing generalization on the test set.\n",
        "\n",
        "6. **Lower or Remove Batch Normalization (if applied)**:\n",
        "   - Batch normalization (if introduced as a later modification in AlexNet) helps by normalizing activations within layers, stabilizing the training process, and reducing overfitting. Removing it or lowering its effect can lead to overfitting, especially in deeper networks.\n",
        "\n",
        "### Features to Remove or Change to Break Training:\n",
        "1. **Remove the Convolutional Layers**:\n",
        "   - The convolutional layers in AlexNet extract hierarchical features (edges, textures, shapes) from images. If these layers are removed or significantly altered, the model will lose its ability to capture meaningful representations, breaking the training process.\n",
        "\n",
        "2. **Change the Kernel Sizes**:\n",
        "   - Changing the kernel sizes in the convolutional layers (e.g., making them much larger or smaller than appropriate) can disrupt the feature extraction process, leading to poor learning or inability to capture patterns.\n",
        "\n",
        "3. **Reduce the Number of Filters/Neurons**:\n",
        "   - Significantly reducing the number of filters in the convolutional layers or neurons in the fully connected layers can reduce the model’s capacity, making it too simple to learn the complexity of the data, resulting in underfitting or broken training.\n",
        "\n",
        "4. **Increase Learning Rate Too Much**:\n",
        "   - Increasing the learning rate too drastically can cause the model’s weights to fluctuate wildly during training, preventing convergence. This will result in poor training outcomes, or in extreme cases, the model won't train at all.\n",
        "\n",
        "5. **Remove or Change Activation Functions**:\n",
        "   - AlexNet relies on activation functions (e.g., ReLU) to introduce non-linearity into the network. Removing or using inappropriate activation functions (e.g., linear activations throughout) will break the model’s ability to learn complex patterns, essentially crippling its training capacity.\n",
        "\n",
        "### Summary:\n",
        "To **make AlexNet overfit**, you can:\n",
        "- Train on a small dataset.\n",
        "- Remove data augmentation or regularization techniques like dropout.\n",
        "- Increase the model’s capacity without proper constraints.\n",
        "- Train for too many epochs.\n",
        "\n",
        "To **break training** in AlexNet, you can:\n",
        "- Remove key architectural features like convolutional layers.\n",
        "- Change kernel sizes or drastically reduce the number of filters/neurons.\n",
        "- Remove or misuse activation functions.\n",
        "\n",
        "In essence, AlexNet's performance can be manipulated by adjusting factors like dataset size, regularization, and training conditions."
      ],
      "metadata": {
        "id": "o3SPn0ryh0YQ"
      },
      "id": "o3SPn0ryh0YQ"
    },
    {
      "cell_type": "markdown",
      "id": "3e825ff7",
      "metadata": {
        "id": "3e825ff7"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/76)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4728265",
      "metadata": {
        "id": "f4728265"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b56088",
      "metadata": {
        "id": "36b56088"
      },
      "source": [
        "# Networks Using Blocks (VGG)\n",
        ":label:`sec_vgg`\n",
        "\n",
        "While AlexNet offered empirical evidence that deep CNNs\n",
        "can achieve good results, it did not provide a general template\n",
        "to guide subsequent researchers in designing new networks.\n",
        "In the following sections, we will introduce several heuristic concepts\n",
        "commonly used to design deep networks.\n",
        "\n",
        "Progress in this field mirrors that of VLSI (very large scale integration)\n",
        "in chip design\n",
        "where engineers moved from placing transistors\n",
        "to logical elements to logic blocks :cite:`Mead.1980`.\n",
        "Similarly, the design of neural network architectures\n",
        "has grown progressively more abstract,\n",
        "with researchers moving from thinking in terms of\n",
        "individual neurons to whole layers,\n",
        "and now to blocks, repeating patterns of layers. A decade later, this has now\n",
        "progressed to researchers using entire trained models to repurpose them for different,\n",
        "albeit related, tasks. Such large pretrained models are typically called\n",
        "*foundation models* :cite:`bommasani2021opportunities`.\n",
        "\n",
        "Back to network design. The idea of using blocks first emerged from the\n",
        "Visual Geometry Group (VGG) at Oxford University,\n",
        "in their eponymously-named *VGG* network :cite:`Simonyan.Zisserman.2014`.\n",
        "It is easy to implement these repeated structures in code\n",
        "with any modern deep learning framework by using loops and subroutines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89467a5c",
      "metadata": {
        "id": "89467a5c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c884009",
      "metadata": {
        "id": "7c884009"
      },
      "source": [
        "## (**VGG Blocks**)\n",
        ":label:`subsec_vgg-blocks`\n",
        "\n",
        "The basic building block of CNNs\n",
        "is a sequence of the following:\n",
        "(i) a convolutional layer\n",
        "with padding to maintain the resolution,\n",
        "(ii) a nonlinearity such as a ReLU,\n",
        "(iii) a pooling layer such\n",
        "as max-pooling to reduce the resolution. One of the problems with\n",
        "this approach is that the spatial resolution decreases quite rapidly. In particular,\n",
        "this imposes a hard limit of $\\log_2 d$ convolutional layers on the network before all\n",
        "dimensions ($d$) are used up. For instance, in the case of ImageNet, it would be impossible to have\n",
        "more than 8 convolutional layers in this way.\n",
        "\n",
        "The key idea of :citet:`Simonyan.Zisserman.2014` was to use *multiple* convolutions in between downsampling\n",
        "via max-pooling in the form of a block. They were primarily interested in whether deep or\n",
        "wide networks perform better. For instance, the successive application of two $3 \\times 3$ convolutions\n",
        "touches the same pixels as a single $5 \\times 5$ convolution does. At the same time, the latter uses approximately\n",
        "as many parameters ($25 \\cdot c^2$) as three $3 \\times 3$ convolutions do ($3 \\cdot 9 \\cdot c^2$).\n",
        "In a rather detailed analysis they showed that deep and narrow networks significantly outperform their shallow counterparts. This set deep learning on a quest for ever deeper networks with over 100 layers for typical applications.\n",
        "Stacking $3 \\times 3$ convolutions\n",
        "has become a gold standard in later deep networks (a design decision only to be revisited recently by\n",
        ":citet:`liu2022convnet`). Consequently, fast implementations for small convolutions have become a staple on GPUs :cite:`lavin2016fast`.\n",
        "\n",
        "Back to VGG: a VGG block consists of a *sequence* of convolutions with $3\\times3$ kernels with padding of 1\n",
        "(keeping height and width) followed by a $2 \\times 2$ max-pooling layer with stride of 2\n",
        "(halving height and width after each block).\n",
        "In the code below, we define a function called `vgg_block`\n",
        "to implement one VGG block.\n",
        "\n",
        "The function below takes two arguments,\n",
        "corresponding to the number of convolutional layers `num_convs`\n",
        "and the number of output channels `num_channels`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a35971a",
      "metadata": {
        "id": "7a35971a"
      },
      "outputs": [],
      "source": [
        "def vgg_block(num_convs, out_channels):\n",
        "    layers = []\n",
        "    for _ in range(num_convs):\n",
        "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c267659",
      "metadata": {
        "id": "6c267659"
      },
      "source": [
        "## [**VGG Network**]\n",
        ":label:`subsec_vgg-network`\n",
        "\n",
        "Like AlexNet and LeNet,\n",
        "the VGG Network can be partitioned into two parts:\n",
        "the first consisting mostly of convolutional and pooling layers\n",
        "and the second consisting of fully connected layers that are identical to those in AlexNet.\n",
        "The key difference is\n",
        "that the convolutional layers are grouped in nonlinear transformations that\n",
        "leave the dimensonality unchanged, followed by a resolution-reduction step, as\n",
        "depicted in :numref:`fig_vgg`.\n",
        "\n",
        "![From AlexNet to VGG. The key difference is that VGG consists of blocks of layers, whereas AlexNet's layers are all designed individually.](http://d2l.ai/_images/vgg.svg)\n",
        ":width:`400px`\n",
        ":label:`fig_vgg`\n",
        "\n",
        "The convolutional part of the network connects several VGG blocks from :numref:`fig_vgg` (also defined in the `vgg_block` function)\n",
        "in succession. This grouping of convolutions is a pattern that has\n",
        "remained almost unchanged over the past decade, although the specific choice of\n",
        "operations has undergone considerable modifications.\n",
        "The variable `arch` consists of a list of tuples (one per block),\n",
        "where each contains two values: the number of convolutional layers\n",
        "and the number of output channels,\n",
        "which are precisely the arguments required to call\n",
        "the `vgg_block` function. As such, VGG defines a *family* of networks rather than just\n",
        "a specific manifestation. To build a specific network we simply iterate over `arch` to compose the blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ed8ba4",
      "metadata": {
        "id": "35ed8ba4"
      },
      "outputs": [],
      "source": [
        "class VGG(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        conv_blks = []\n",
        "        for (num_convs, out_channels) in arch:\n",
        "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
        "        self.net = nn.Sequential(\n",
        "            *conv_blks, nn.Flatten(),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4413c3f",
      "metadata": {
        "id": "f4413c3f"
      },
      "source": [
        "The original VGG network had five convolutional blocks,\n",
        "among which the first two have one convolutional layer each\n",
        "and the latter three contain two convolutional layers each.\n",
        "The first block has 64 output channels\n",
        "and each subsequent block doubles the number of output channels,\n",
        "until that number reaches 512.\n",
        "Since this network uses eight convolutional layers\n",
        "and three fully connected layers, it is often called VGG-11.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc110465",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc110465",
        "outputId": "57933444-fdca-423e-e1f5-3889a81cb7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
            "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
            "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
            "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
            "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
            "Flatten output shape:\t torch.Size([1, 25088])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 4096])\n",
            "ReLU output shape:\t torch.Size([1, 4096])\n",
            "Dropout output shape:\t torch.Size([1, 4096])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n",
        "    (1, 1, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb49027b",
      "metadata": {
        "id": "cb49027b"
      },
      "source": [
        "As you can see, we halve height and width at each block,\n",
        "finally reaching a height and width of 7\n",
        "before flattening the representations\n",
        "for processing by the fully connected part of the network.\n",
        ":citet:`Simonyan.Zisserman.2014` described several other variants of VGG.\n",
        "In fact, it has become the norm to propose *families* of networks with\n",
        "different speed--accuracy trade-off when introducing a new architecture.\n",
        "\n",
        "## Training\n",
        "\n",
        "[**Since VGG-11 is computationally more demanding than AlexNet\n",
        "we construct a network with a smaller number of channels.**]\n",
        "This is more than sufficient for training on Fashion-MNIST.\n",
        "The [**model training**] process is similar to that of AlexNet in :numref:`sec_alexnet`.\n",
        "Again observe the close match between validation and training loss,\n",
        "suggesting only a small amount of overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed532028",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "ed532028",
        "outputId": "71c64e09-b2e1-4b7c-cc2c-d112e378c3bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T08:22:46.976936</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 238.965625 183.35625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"md5576d4da7\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#md5576d4da7\" x=\"30.103125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#md5576d4da7\" x=\"69.163125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#md5576d4da7\" x=\"108.223125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#md5576d4da7\" x=\"147.283125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#md5576d4da7\" x=\"186.343125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#md5576d4da7\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"mdc64a0b793\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mdc64a0b793\" x=\"30.103125\" y=\"127.412139\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 131.211357) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mdc64a0b793\" x=\"30.103125\" y=\"95.752907\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 99.552126) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mdc64a0b793\" x=\"30.103125\" y=\"64.093676\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.5 -->\n      <g transform=\"translate(7.2 67.892894) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mdc64a0b793\" x=\"30.103125\" y=\"32.434444\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 36.233663) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path d=\"M 34.954394 13.5 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 49.633125 105.85169 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_15\"/>\n   <g id=\"line2d_16\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 49.633125 105.85169 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 49.633125 117.681433 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 49.633125 105.85169 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 49.633125 117.681433 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 49.633125 105.85169 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 49.633125 117.681433 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 49.633125 117.681433 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \nL 190.861259 138.290927 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \nL 190.861259 138.290927 \nL 200.605438 138.57212 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \nL 190.861259 138.290927 \nL 200.605438 138.57212 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \nL 205.873125 138.735231 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \nL 190.861259 138.290927 \nL 200.605438 138.57212 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \nL 205.873125 138.735231 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \nL 205.873125 103.110422 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \nL 190.861259 138.290927 \nL 200.605438 138.57212 \nL 210.349618 139.007581 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \nL 205.873125 138.735231 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \nL 205.873125 103.110422 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \nL 190.861259 138.290927 \nL 200.605438 138.57212 \nL 210.349618 139.007581 \nL 220.093797 139.295028 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \nL 205.873125 138.735231 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \nL 205.873125 103.110422 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \nL 190.861259 138.290927 \nL 200.605438 138.57212 \nL 210.349618 139.007581 \nL 220.093797 139.295028 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \nL 205.873125 138.735231 \nL 225.403125 139.5 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \nL 205.873125 103.110422 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 44.174839 \nL 54.442752 109.606555 \nL 64.186931 119.408731 \nL 73.93111 123.447382 \nL 83.675289 126.859405 \nL 93.419468 129.514507 \nL 103.163647 130.51417 \nL 112.907826 132.708665 \nL 122.652006 133.26168 \nL 132.396185 134.770742 \nL 142.140364 135.24387 \nL 151.884543 135.874648 \nL 161.628722 136.796586 \nL 171.372901 137.582534 \nL 181.11708 137.498381 \nL 190.861259 138.290927 \nL 200.605438 138.57212 \nL 210.349618 139.007581 \nL 220.093797 139.295028 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 49.633125 105.85169 \nL 69.163125 125.40979 \nL 88.693125 129.86567 \nL 108.223125 132.784423 \nL 127.753125 134.91695 \nL 147.283125 134.368328 \nL 166.813125 137.398468 \nL 186.343125 137.777259 \nL 205.873125 138.735231 \nL 225.403125 139.5 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 49.633125 117.681433 \nL 69.163125 108.313908 \nL 88.693125 106.585674 \nL 108.223125 105.602585 \nL 127.753125 104.462953 \nL 147.283125 104.957628 \nL 166.813125 103.711547 \nL 186.343125 103.473602 \nL 205.873125 103.110422 \nL 225.403125 102.84743 \n\" clip-path=\"url(#p6971804885)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 145.8 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 145.8 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 138.8125 60.06875 \nL 218.403125 60.06875 \nQ 220.403125 60.06875 220.403125 58.06875 \nL 220.403125 14.2 \nQ 220.403125 12.2 218.403125 12.2 \nL 138.8125 12.2 \nQ 136.8125 12.2 136.8125 14.2 \nL 136.8125 58.06875 \nQ 136.8125 60.06875 138.8125 60.06875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_127\">\n     <path d=\"M 140.8125 20.298438 \nL 150.8125 20.298438 \nL 160.8125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- train_loss -->\n     <g transform=\"translate(168.8125 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_128\">\n     <path d=\"M 140.8125 35.254688 \nL 150.8125 35.254688 \nL 160.8125 35.254688 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- val_loss -->\n     <g transform=\"translate(168.8125 38.754688) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_129\">\n     <path d=\"M 140.8125 50.210938 \nL 150.8125 50.210938 \nL 160.8125 50.210938 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- val_acc -->\n     <g transform=\"translate(168.8125 53.710938) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6971804885\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157480ef",
      "metadata": {
        "id": "157480ef"
      },
      "source": [
        "## Summary\n",
        "\n",
        "One might argue that VGG is the first truly modern convolutional neural network. While AlexNet introduced many of the components of what make deep learning effective at scale, it is VGG that arguably introduced key properties such as blocks of multiple convolutions and a preference for deep and narrow networks. It is also the first network that is actually an entire family of similarly parametrized models, giving the practitioner ample trade-off between complexity and speed. This is also the place where modern deep learning frameworks shine. It is no longer necessary to generate XML configuration files to specify a network but rather, to assemble said networks through simple Python code.\n",
        "\n",
        "More recently ParNet :cite:`Goyal.Bochkovskiy.Deng.ea.2021` demonstrated that it is possible to achieve competitive performance using a much more shallow architecture through a large number of parallel computations. This is an exciting development and there is hope that it will influence architecture designs in the future. For the remainder of the chapter, though, we will follow the path of scientific progress over the past decade.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "\n",
        "1. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory.\n",
        "    1. Compare the number of parameters needed for AlexNet and VGG.\n",
        "    1. Compare the number of floating point operations used in the convolutional layers and in the fully connected layers.\n",
        "    1. How could you reduce the computational cost created by the fully connected layers?\n",
        "1. When displaying the dimensions associated with the various layers of the network, we only see the information associated with eight blocks (plus some auxiliary transforms), even though the network has 11 layers. Where did the remaining three layers go?\n",
        "1. Use Table 1 in the VGG paper :cite:`Simonyan.Zisserman.2014` to construct other common models, such as VGG-16 or VGG-19.\n",
        "1. Upsampling the resolution in Fashion-MNIST eight-fold from $28 \\times 28$ to $224 \\times 224$ dimensions is very wasteful. Try modifying the network architecture and resolution conversion, e.g., to 56 or to 84 dimensions for its input instead. Can you do so without reducing the accuracy of the network? Consult the VGG paper :cite:`Simonyan.Zisserman.2014` for ideas on adding more nonlinearities prior to downsampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.1\n",
        "arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
        "vgg = VGG(arch=arch)\n",
        "X = torch.randn(1,3, 224, 224)\n",
        "_ = vgg(X)\n",
        "params = {'conv':0, 'lr':0}\n",
        "for idx, module in enumerate(vgg.net):\n",
        "    if type(module) == nn.Sequential:\n",
        "        stat_params(module,params)\n",
        "    if type(module) == nn.Linear:\n",
        "        num = sum(p.numel() for p in module.parameters())\n",
        "        params['lr'] += num\n",
        "summary(vgg, (3, 224, 224))\n",
        "params\n",
        "\n",
        "{'conv': 9220480, 'lr': 119586826}\n"
      ],
      "metadata": {
        "id": "xwQWZn3PiKQQ"
      },
      "id": "xwQWZn3PiKQQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2\n",
        "x = torch.randn(1,3, 224, 224)\n",
        "params = {'conv':0, 'lr':0}\n",
        "for idx, module in enumerate(vgg.net):\n",
        "    if type(module) == nn.Sequential:\n",
        "        x = stat_comp(module, params, x)\n",
        "    if type(module) == nn.Linear:\n",
        "        params['lr'] += sum(p.numel() for p in module.parameters())\n",
        "params\n",
        "{'conv': 7485456384, 'lr': 119586826}\n"
      ],
      "metadata": {
        "id": "L0qkdShuiQtM"
      },
      "id": "L0qkdShuiQtM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 To reduce the computational cost of fully connected layers in neural networks, here are some effective strategies:\n",
        "\n",
        "1. **Global Average Pooling (GAP)**: Replace fully connected layers with GAP, which computes the average of each feature map, reducing parameters and computations.\n",
        "\n",
        "2. **Replace with 1x1 Convolutions**: Convert fully connected layers to 1x1 convolutions, enabling weight sharing and reducing parameter count.\n",
        "\n",
        "3. **Network Pruning**: Remove unnecessary connections or neurons through pruning, maintaining accuracy while reducing computation.\n",
        "\n",
        "4. **Low-Rank Approximations**: Use techniques like Singular Value Decomposition (SVD) to approximate weight matrices and reduce computations.\n",
        "\n",
        "5. **Dimensionality Reduction**: Apply methods like PCA to reduce input size and computation in fully connected layers.\n",
        "\n",
        "6. **Quantization**: Lower precision (e.g., 8-bit) for weight representation to save memory and speed up computations.\n",
        "\n",
        "7. **Knowledge Distillation**: Train a smaller student model to replicate the behavior of a larger network for efficiency.\n",
        "\n",
        "8. **Model Compression**: Techniques like weight sharing and tensor factorization help reduce size and complexity.\n",
        "\n",
        "These approaches help lower the computational burden of fully connected layers while preserving performance, though fine-tuning and experimentation are needed to optimize results."
      ],
      "metadata": {
        "id": "Jy2zSvmvibLH"
      },
      "id": "Jy2zSvmvibLH"
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "VGG(\n",
        "  (net): Sequential(\n",
        "    (0): Sequential(\n",
        "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (1): Sequential(\n",
        "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (2): Sequential(\n",
        "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (3): Sequential(\n",
        "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (4): Sequential(\n",
        "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (5): Flatten(start_dim=1, end_dim=-1)\n",
        "    (6): Linear(in_features=25088, out_features=4096, bias=True)\n",
        "    (7): ReLU()\n",
        "    (8): Dropout(p=0.5, inplace=False)\n",
        "    (9): Linear(in_features=4096, out_features=4096, bias=True)\n",
        "    (10): ReLU()\n",
        "    (11): Dropout(p=0.5, inplace=False)\n",
        "    (12): Linear(in_features=4096, out_features=10, bias=True)\n",
        "  )\n",
        ")"
      ],
      "metadata": {
        "id": "zftJGuJLijwo"
      },
      "id": "zftJGuJLijwo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "arch16=((2, 64), (2, 128), (3, 256), (32, 512), (3, 512))\n",
        "vgg16 = VGG(arch=arch16)\n",
        "vgg16\n",
        "\n",
        "VGG(\n",
        "  (net): Sequential(\n",
        "    (0): Sequential(\n",
        "      (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (1): Sequential(\n",
        "      (0): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (2): Sequential(\n",
        "      (0): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (5): ReLU()\n",
        "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (3): Sequential(\n",
        "      (0): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (5): ReLU()\n",
        "      (6): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (7): ReLU()\n",
        "      (8): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (9): ReLU()\n",
        "      (10): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (11): ReLU()\n",
        "      (12): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (13): ReLU()\n",
        "      (14): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (15): ReLU()\n",
        "      (16): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (17): ReLU()\n",
        "      (18): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (19): ReLU()\n",
        "      (20): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (21): ReLU()\n",
        "      (22): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (23): ReLU()\n",
        "      (24): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (25): ReLU()\n",
        "      (26): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (27): ReLU()\n",
        "      (28): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (29): ReLU()\n",
        "      (30): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (31): ReLU()\n",
        "      (32): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (33): ReLU()\n",
        "      (34): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (35): ReLU()\n",
        "      (36): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (37): ReLU()\n",
        "      (38): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (39): ReLU()\n",
        "      (40): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (41): ReLU()\n",
        "      (42): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (43): ReLU()\n",
        "      (44): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (45): ReLU()\n",
        "      (46): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (47): ReLU()\n",
        "      (48): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (49): ReLU()\n",
        "      (50): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (51): ReLU()\n",
        "      (52): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (53): ReLU()\n",
        "      (54): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (55): ReLU()\n",
        "      (56): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (57): ReLU()\n",
        "      (58): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (59): ReLU()\n",
        "      (60): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (61): ReLU()\n",
        "      (62): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (63): ReLU()\n",
        "      (64): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (4): Sequential(\n",
        "      (0): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (5): ReLU()\n",
        "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    )\n",
        "    (5): Flatten(start_dim=1, end_dim=-1)\n",
        "    (6): LazyLinear(in_features=0, out_features=4096, bias=True)\n",
        "    (7): ReLU()\n",
        "    (8): Dropout(p=0.5, inplace=False)\n",
        "    (9): LazyLinear(in_features=0, out_features=4096, bias=True)\n",
        "    (10): ReLU()\n",
        "    (11): Dropout(p=0.5, inplace=False)\n",
        "    (12): LazyLinear(in_features=0, out_features=10, bias=True)\n",
        "  )\n",
        ")\n"
      ],
      "metadata": {
        "id": "upP-uJUlin__"
      },
      "id": "upP-uJUlin__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "model = VGG(arch=((3, 128), (3, 256)), lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(28, 28))\n",
        "trainer.fit(model, data)\n"
      ],
      "metadata": {
        "id": "xzntszZKivM4"
      },
      "id": "xzntszZKivM4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dd565a2f",
      "metadata": {
        "id": "dd565a2f"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/78)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d9bd16",
      "metadata": {
        "id": "50d9bd16"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4098e4aa",
      "metadata": {
        "id": "4098e4aa"
      },
      "source": [
        "# Network in Network (NiN)\n",
        ":label:`sec_nin`\n",
        "\n",
        "LeNet, AlexNet, and VGG all share a common design pattern:\n",
        "extract features exploiting *spatial* structure\n",
        "via a sequence of convolutions and pooling layers\n",
        "and post-process the representations via fully connected layers.\n",
        "The improvements upon LeNet by AlexNet and VGG mainly lie\n",
        "in how these later networks widen and deepen these two modules.\n",
        "\n",
        "This design poses two major challenges.\n",
        "First, the fully connected layers at the end\n",
        "of the architecture consume tremendous numbers of parameters. For instance, even a simple\n",
        "model such as VGG-11 requires a monstrous matrix, occupying almost\n",
        "400MB of RAM in single precision (FP32). This is a significant impediment to computation, in particular on\n",
        "mobile and embedded devices. After all, even high-end mobile phones sport no more than 8GB of RAM. At the time VGG was invented, this was an order of magnitude less (the iPhone 4S had 512MB). As such, it would have been difficult to justify spending the majority of memory on an image classifier.\n",
        "\n",
        "Second, it is equally impossible to add fully connected layers\n",
        "earlier in the network to increase the degree of nonlinearity: doing so would destroy the\n",
        "spatial structure and require potentially even more memory.\n",
        "\n",
        "The *network in network* (*NiN*) blocks :cite:`Lin.Chen.Yan.2013` offer an alternative,\n",
        "capable of solving both problems in one simple strategy.\n",
        "They were proposed based on a very simple insight: (i) use $1 \\times 1$ convolutions to add\n",
        "local nonlinearities across the channel activations and (ii) use global average pooling to integrate\n",
        "across all locations in the last representation layer. Note that global average pooling would not\n",
        "be effective, were it not for the added nonlinearities. Let's dive into this in detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11675e1",
      "metadata": {
        "id": "d11675e1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95132206",
      "metadata": {
        "id": "95132206"
      },
      "source": [
        "## (**NiN Blocks**)\n",
        "\n",
        "Recall :numref:`subsec_1x1`. In it we said that the inputs and outputs of convolutional layers\n",
        "consist of four-dimensional tensors with axes\n",
        "corresponding to the example, channel, height, and width.\n",
        "Also recall that the inputs and outputs of fully connected layers\n",
        "are typically two-dimensional tensors corresponding to the example and feature.\n",
        "The idea behind NiN is to apply a fully connected layer\n",
        "at each pixel location (for each height and width).\n",
        "The resulting $1 \\times 1$ convolution can be thought of as\n",
        "a fully connected layer acting independently on each pixel location.\n",
        "\n",
        ":numref:`fig_nin` illustrates the main structural\n",
        "differences between VGG and NiN, and their blocks.\n",
        "Note both the difference in the NiN blocks (the initial convolution is followed by $1 \\times 1$ convolutions, whereas VGG retains $3 \\times 3$ convolutions) and at the end where we no longer require a giant fully connected layer.\n",
        "\n",
        "![Comparing the architectures of VGG and NiN, and of their blocks.](http://d2l.ai/_images/nin.svg)\n",
        ":width:`600px`\n",
        ":label:`fig_nin`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538cefc3",
      "metadata": {
        "id": "538cefc3"
      },
      "outputs": [],
      "source": [
        "def nin_block(out_channels, kernel_size, strides, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\n",
        "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c944308",
      "metadata": {
        "id": "5c944308"
      },
      "source": [
        "## [**NiN Model**]\n",
        "\n",
        "NiN uses the same initial convolution sizes as AlexNet (it was proposed shortly thereafter).\n",
        "The kernel sizes are $11\\times 11$, $5\\times 5$, and $3\\times 3$, respectively,\n",
        "and the numbers of output channels match those of AlexNet. Each NiN block is followed by a max-pooling layer\n",
        "with a stride of 2 and a window shape of $3\\times 3$.\n",
        "\n",
        "The second significant difference between NiN and both AlexNet and VGG\n",
        "is that NiN avoids fully connected layers altogether.\n",
        "Instead, NiN uses a NiN block with a number of output channels equal to the number of label classes, followed by a *global* average pooling layer,\n",
        "yielding a vector of logits.\n",
        "This design significantly reduces the number of required model parameters, albeit at the expense of a potential increase in training time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "200542dc",
      "metadata": {
        "id": "200542dc"
      },
      "outputs": [],
      "source": [
        "class NiN(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nin_block(96, kernel_size=11, strides=4, padding=0),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nin_block(256, kernel_size=5, strides=1, padding=2),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nin_block(384, kernel_size=3, strides=1, padding=1),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.Dropout(0.5),\n",
        "            nin_block(num_classes, kernel_size=3, strides=1, padding=1),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten())\n",
        "        self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7562f54",
      "metadata": {
        "id": "f7562f54"
      },
      "source": [
        "We create a data example to see [**the output shape of each block**].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a1fe2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0a1fe2c",
        "outputId": "3965bf71-444f-4332-9c20-e684be1b3e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
            "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
            "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
            "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
            "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
            "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
            "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
            "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
            "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
            "Flatten output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "NiN().layer_summary((1, 1, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ac0209",
      "metadata": {
        "id": "b4ac0209"
      },
      "source": [
        "## [**Training**]\n",
        "\n",
        "As before we use Fashion-MNIST to train the model using the same\n",
        "optimizer that we used for AlexNet and VGG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111948ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "111948ca",
        "outputId": "a70109aa-14bb-43bb-f21d-7f671e5aedb1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T08:34:03.794177</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 238.965625 183.35625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m97d053142c\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m97d053142c\" x=\"30.103125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m97d053142c\" x=\"69.163125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m97d053142c\" x=\"108.223125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m97d053142c\" x=\"147.283125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m97d053142c\" x=\"186.343125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m97d053142c\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m9271bb713e\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m9271bb713e\" x=\"30.103125\" y=\"119.604005\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 123.403224) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m9271bb713e\" x=\"30.103125\" y=\"90.190511\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 93.989729) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m9271bb713e\" x=\"30.103125\" y=\"60.777016\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.5 -->\n      <g transform=\"translate(7.2 64.576235) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m9271bb713e\" x=\"30.103125\" y=\"31.363522\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 35.162741) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path d=\"M 34.954394 13.5 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 49.633125 13.859078 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_15\"/>\n   <g id=\"line2d_16\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 49.633125 13.859078 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 49.633125 139.5 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 49.633125 13.859078 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 49.633125 139.5 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 49.633125 13.859078 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 49.633125 139.5 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 49.633125 139.5 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \nL 190.861259 112.247095 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \nL 190.861259 112.247095 \nL 200.605438 112.628436 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \nL 190.861259 112.247095 \nL 200.605438 112.628436 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \nL 205.873125 114.348617 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \nL 190.861259 112.247095 \nL 200.605438 112.628436 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \nL 205.873125 114.348617 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \nL 205.873125 103.256711 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \nL 190.861259 112.247095 \nL 200.605438 112.628436 \nL 210.349618 113.905871 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \nL 205.873125 114.348617 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \nL 205.873125 103.256711 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \nL 190.861259 112.247095 \nL 200.605438 112.628436 \nL 210.349618 113.905871 \nL 220.093797 113.859986 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \nL 205.873125 114.348617 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \nL 205.873125 103.256711 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \nL 190.861259 112.247095 \nL 200.605438 112.628436 \nL 210.349618 113.905871 \nL 220.093797 113.859986 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \nL 205.873125 114.348617 \nL 225.403125 115.79988 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \nL 205.873125 103.256711 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.645964 \nL 54.442752 14.350918 \nL 64.186931 20.542369 \nL 73.93111 34.430353 \nL 83.675289 55.140741 \nL 93.419468 74.520656 \nL 103.163647 85.193008 \nL 112.907826 92.19398 \nL 122.652006 98.497702 \nL 132.396185 102.611212 \nL 142.140364 105.147716 \nL 151.884543 102.859941 \nL 161.628722 108.786746 \nL 171.372901 109.918515 \nL 181.11708 111.539141 \nL 190.861259 112.247095 \nL 200.605438 112.628436 \nL 210.349618 113.905871 \nL 220.093797 113.859986 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 49.633125 13.859078 \nL 69.163125 31.032719 \nL 88.693125 36.158822 \nL 108.223125 95.217258 \nL 127.753125 101.322413 \nL 147.283125 100.665085 \nL 166.813125 109.472525 \nL 186.343125 111.874112 \nL 205.873125 114.348617 \nL 225.403125 115.79988 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 134.380563 \nL 88.693125 129.115687 \nL 108.223125 109.62692 \nL 127.753125 107.456977 \nL 147.283125 108.358696 \nL 166.813125 104.746002 \nL 186.343125 103.937363 \nL 205.873125 103.256711 \nL 225.403125 102.523701 \n\" clip-path=\"url(#pd65003379a)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 145.8 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 145.8 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 138.8125 60.06875 \nL 218.403125 60.06875 \nQ 220.403125 60.06875 220.403125 58.06875 \nL 220.403125 14.2 \nQ 220.403125 12.2 218.403125 12.2 \nL 138.8125 12.2 \nQ 136.8125 12.2 136.8125 14.2 \nL 136.8125 58.06875 \nQ 136.8125 60.06875 138.8125 60.06875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_127\">\n     <path d=\"M 140.8125 20.298438 \nL 150.8125 20.298438 \nL 160.8125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- train_loss -->\n     <g transform=\"translate(168.8125 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_128\">\n     <path d=\"M 140.8125 35.254688 \nL 150.8125 35.254688 \nL 160.8125 35.254688 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- val_loss -->\n     <g transform=\"translate(168.8125 38.754688) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_129\">\n     <path d=\"M 140.8125 50.210938 \nL 150.8125 50.210938 \nL 160.8125 50.210938 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- val_acc -->\n     <g transform=\"translate(168.8125 53.710938) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd65003379a\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = NiN(lr=0.05)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182ce04f",
      "metadata": {
        "id": "182ce04f"
      },
      "source": [
        "## Summary\n",
        "\n",
        "NiN has dramatically fewer parameters than AlexNet and VGG. This stems primarily from the fact that it needs no giant fully connected layers. Instead, it uses global average pooling to aggregate across all image locations after the last stage of the network body. This obviates the need for expensive (learned) reduction operations and replaces them by a simple average. What surprised researchers at the time was the fact that this averaging operation did not harm accuracy. Note that averaging across a low-resolution representation (with many channels) also adds to the amount of translation invariance that the network can handle.\n",
        "\n",
        "Choosing fewer convolutions with wide kernels and replacing them by $1 \\times 1$ convolutions aids the quest for fewer parameters further. It can cater for a significant amount of nonlinearity across channels within any given location. Both $1 \\times 1$ convolutions and global average pooling significantly influenced subsequent CNN designs.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Why are there two $1\\times 1$ convolutional layers per NiN block? Increase their number to three. Reduce their number to one. What changes?\n",
        "1. What changes if you replace the $1 \\times 1$ convolutions by $3 \\times 3$ convolutions?\n",
        "1. What happens if you replace the global average pooling by a fully connected layer (speed, accuracy, number of parameters)?\n",
        "1. Calculate the resource usage for NiN.\n",
        "    1. What is the number of parameters?\n",
        "    1. What is the amount of computation?\n",
        "    1. What is the amount of memory needed during training?\n",
        "    1. What is the amount of memory needed during prediction?\n",
        "1. What are possible problems with reducing the $384 \\times 5 \\times 5$ representation to a $10 \\times 5 \\times 5$ representation in one step?\n",
        "1. Use the structural design decisions in VGG that led to VGG-11, VGG-16, and VGG-19 to design a family of NiN-like networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.. In a **Network in Network (NiN)** architecture, each NiN block typically contains two consecutive **1x1 convolutional layers**. These layers play an essential role in reducing dimensionality, introducing non-linearity, and allowing for efficient feature transformation.\n",
        "\n",
        "### Why are there two 1x1 convolutional layers per NiN block?\n",
        "- **Dimensionality Reduction**: The first 1x1 convolutional layer is often used to reduce the dimensionality of the feature maps, lowering the number of channels while retaining spatial information. This helps reduce computational cost.\n",
        "- **Feature Transformation**: The second 1x1 convolution introduces another layer of non-linearity and performs further feature transformation, allowing the network to learn more complex patterns with minimal increase in computational cost.\n",
        "- **Non-linearity**: The layers are often followed by activation functions (e.g., ReLU), adding non-linear capabilities between the transformations.\n",
        "\n",
        "### Increasing to Three 1x1 Convolutional Layers\n",
        "If you increase the number of **1x1 convolutions to three per NiN block**, the following changes would occur:\n",
        "\n",
        "1. **Increased Model Capacity**: Adding a third 1x1 convolution would increase the model's capacity to learn more complex feature transformations. This may improve performance on tasks where higher complexity is beneficial.\n",
        "   \n",
        "2. **Higher Computational Cost**: More layers increase the number of parameters and operations, raising memory usage and inference time. This can slow down the model, especially on large datasets or in resource-constrained environments.\n",
        "\n",
        "3. **Potential for Better Feature Abstraction**: With three layers, the network can abstract features more deeply, allowing for more complex patterns to be learned, possibly improving accuracy on challenging tasks. However, diminishing returns could occur if the added complexity isn't needed.\n",
        "\n",
        "4. **Risk of Overfitting**: The increased capacity can lead to overfitting, especially when training on smaller datasets or without regularization.\n",
        "\n",
        "### Reducing to One 1x1 Convolutional Layer\n",
        "If you reduce the number of **1x1 convolutions to one per NiN block**, the following changes would occur:\n",
        "\n",
        "1. **Lower Computational Cost**: Having just one 1x1 convolution layer reduces the number of parameters and computations, leading to a faster and more lightweight model.\n",
        "\n",
        "2. **Reduced Learning Capacity**: With only one 1x1 convolution layer, the network has less capacity to perform complex transformations. This can lead to reduced performance on tasks that require deeper feature extraction.\n",
        "\n",
        "3. **Shallower Feature Abstraction**: Without the second layer, the model may struggle to capture higher-level patterns, limiting its ability to generalize on complex datasets.\n",
        "\n",
        "4. **Improved Efficiency**: The model would run faster with fewer computations, which can be advantageous for real-time applications or devices with limited resources, but at the potential cost of accuracy.\n",
        "\n",
        "### Summary of Changes:\n",
        "- **Increase to Three Layers**: Improves feature abstraction and model capacity but increases computational cost and risk of overfitting.\n",
        "- **Reduce to One Layer**: Reduces computational cost and memory usage, but decreases learning capacity and the ability to capture complex patterns.\n",
        "\n",
        "The choice between one, two, or three 1x1 layers depends on the balance between model complexity and computational efficiency required for a given task."
      ],
      "metadata": {
        "id": "CNBjSTWRotTc"
      },
      "id": "CNBjSTWRotTc"
    },
    {
      "cell_type": "code",
      "source": [
        "arch = ((96,11,4,0,3),(256,5,1,2,3),(384,3,1,1,3),(10,3,1,1,3))\n",
        "model = Nin(arch)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "print(f'acc: {model.accuracy(y_hat,y).item():.2f}')\n",
        "\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "arch = ((96,11,4,0,[[1,0]]),(256,5,1,2,[[1,0]]),(384,3,1,1,[[1,0]]),(10,3,1,1,[[1,0]]))\n",
        "model = Nin(arch, lr=0.05)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "print(f'acc: {model.accuracy(y_hat,y).item():.2f}')\n"
      ],
      "metadata": {
        "id": "ioxq9KYRsv0U"
      },
      "id": "ioxq9KYRsv0U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "arch = ((96,11,4,0,[[3,1],[3,1]]),(256,5,1,2,[[3,1],[3,1]]),(384,3,1,1,[[3,1],[3,1]]),(10,3,1,1,[[3,1],[3,1]]))\n",
        "model = Nin(arch)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "print(f'acc: {model.accuracy(y_hat,y).item():.2f}')\n"
      ],
      "metadata": {
        "id": "HRvolwf2s3mf"
      },
      "id": "HRvolwf2s3mf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "class MLPNin(d2l.Classifier):\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        layers = []\n",
        "        for i in range(len(arch)-1):\n",
        "            layers.append(nin_block(*arch[i]))\n",
        "            layers.append(nn.MaxPool2d(3, stride=2))\n",
        "        layers.append(nn.Dropout(0.5))\n",
        "        layers.append(nin_block(*arch[-1]))\n",
        "        layers.append(nn.Flatten())\n",
        "        layers.append(nn.LazyLinear(num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.net.apply(d2l.init_cnn)\n"
      ],
      "metadata": {
        "id": "k_VzgeGZs6D1"
      },
      "id": "k_VzgeGZs6D1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1\n",
        "arch = ((96,11,4,0,2),(256,5,1,2,2),(384,3,1,1,2),(10,3,1,1,2))\n",
        "model = Nin(arch)\n",
        "X = torch.randn(1,3, 224, 224)\n",
        "_ = model(X)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)\n",
        "Total parameters: 2015398\n"
      ],
      "metadata": {
        "id": "myadT_y-s8oa"
      },
      "id": "myadT_y-s8oa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.2\n",
        "from thop import profile\n",
        "flops, params = profile(model, inputs=(X,))\n",
        "print(\"Total FLOPs:\", flops)\n",
        "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
        "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
        "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
        "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
        "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
        "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
        "Total FLOPs: 830042124.0\n"
      ],
      "metadata": {
        "id": "0u5eahJGs8iG"
      },
      "id": "0u5eahJGs8iG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Initialize memory counters\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.empty_cache()\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "memory_stats = torch.cuda.memory_stats(device=device)\n",
        "# Print peak memory usage and other memory statistics\n",
        "print(\"Peak memory usage:\", memory_stats[\"allocated_bytes.all.peak\"] / (1024 ** 2), \"MB\")\n",
        "print(\"Current memory usage:\", memory_stats[\"allocated_bytes.all.current\"] / (1024 ** 2), \"MB\")\n"
      ],
      "metadata": {
        "id": "3gQcdMx8tDOY"
      },
      "id": "3gQcdMx8tDOY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.empty_cache()\n",
        "_ = model(X)\n",
        "memory_stats = torch.cuda.memory_stats(device=device)\n",
        "print(\"Peak memory usage:\", memory_stats[\"allocated_bytes.all.peak\"] / (1024 ** 2), \"MB\")\n",
        "print(\"Current memory usage:\", memory_stats[\"allocated_bytes.all.current\"] / (1024 ** 2), \"MB\")\n"
      ],
      "metadata": {
        "id": "OrFSfePQtFem"
      },
      "id": "OrFSfePQtFem",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Reducing the 384×5×5 representation to a 10×5×5 representation in one step in Network in Network (NiN) architecture can lead to several potential problems:\n",
        "\n",
        "Loss of Information: Reducing the representation from 384 channels to only 10 channels in a single step can result in a significant loss of information. Each channel contains specific features and patterns learned by the network, and reducing them abruptly might lead to loss of discriminative power.\n",
        "\n",
        "Underfitting: The reduced 10-channel representation might not have enough capacity to capture the complexity of the original input. This can result in the model underfitting the data, leading to poor generalization and performance.\n",
        "\n",
        "Information Bottleneck: Such a drastic reduction in the number of channels creates an information bottleneck, limiting the network’s ability to transform the input effectively. It can hinder the network’s learning capability and limit its expressive power.\n",
        "\n",
        "Reduced Expressiveness: Reducing the number of channels too quickly can limit the model’s ability to learn high-level features and hierarchical representations of the input data. Deep networks often rely on progressively learning more abstract features.\n",
        "\n",
        "Spatial Features: A representation of 10×5×5 doesn’t capture spatial features well. Important spatial patterns and relationships present in the original representation might be lost, making the network less capable of recognizing objects.\n",
        "\n",
        "Loss of Discriminative Power: With a smaller representation, the network might struggle to differentiate between different classes, leading to confusion and decreased accuracy.\n",
        "\n",
        "To mitigate these problems, it’s common to use intermediate layers with smaller reductions in the number of channels, allowing the network to learn gradually more abstract and complex features. The NiN architecture typically uses multiple consecutive NiN blocks to avoid these issues by applying multiple nonlinear transformations with\n",
        "1\n",
        "×\n",
        "1\n",
        "1×1 convolutions, gradually reducing the number of channels over several steps, and maintaining the network’s ability to learn meaningful representations.\n",
        "\n"
      ],
      "metadata": {
        "id": "XDPUvhRdsuFs"
      },
      "id": "XDPUvhRdsuFs"
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "arch = ((96,11,4,0,2),(256,5,1,2,2),(384,3,1,1,2),(10,3,1,1,2))\n",
        "nin = Nin(arch)\n",
        "nin\n",
        "Nin(\n",
        "  (net): Sequential(\n",
        "    (0): Sequential(\n",
        "      (0): LazyConv2d(0, 96, kernel_size=(11, 11), stride=(4, 4))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 96, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 96, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (2): Sequential(\n",
        "      (0): LazyConv2d(0, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (4): Sequential(\n",
        "      (0): LazyConv2d(0, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 384, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 384, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (6): Dropout(p=0.5, inplace=False)\n",
        "    (7): Sequential(\n",
        "      (0): LazyConv2d(0, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 10, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 10, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "    (9): Flatten(start_dim=1, end_dim=-1)\n",
        "  )\n",
        ")\n",
        "arch15 = ((64,3,2,1),\n",
        "          (256,3,1,1),\n",
        "          (256,3,1,1),\n",
        "          (384,3,1,1),\n",
        "          (10,3,1,1))\n",
        "nin15 = Nin(arch15)\n",
        "nin15\n",
        "\n",
        "\n",
        "Nin(\n",
        "  (net): Sequential(\n",
        "    (0): Sequential(\n",
        "      (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (2): Sequential(\n",
        "      (0): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (4): Sequential(\n",
        "      (0): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (6): Sequential(\n",
        "      (0): LazyConv2d(0, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 384, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 384, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (8): Dropout(p=0.5, inplace=False)\n",
        "    (9): Sequential(\n",
        "      (0): LazyConv2d(0, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "      (1): ReLU()\n",
        "      (2): LazyConv2d(0, 10, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (3): ReLU()\n",
        "      (4): LazyConv2d(0, 10, kernel_size=(1, 1), stride=(1, 1))\n",
        "      (5): ReLU()\n",
        "    )\n",
        "    (10): AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "    (11): Flatten(start_dim=1, end_dim=-1)\n",
        "  )\n",
        ")\n"
      ],
      "metadata": {
        "id": "okxYMDzFtL2z"
      },
      "id": "okxYMDzFtL2z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b01b3354",
      "metadata": {
        "id": "b01b3354"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2835de1c",
      "metadata": {
        "id": "2835de1c"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8979a666",
      "metadata": {
        "id": "8979a666"
      },
      "source": [
        "# Multi-Branch Networks  (GoogLeNet)\n",
        ":label:`sec_googlenet`\n",
        "\n",
        "In 2014, *GoogLeNet*\n",
        "won the ImageNet Challenge :cite:`Szegedy.Liu.Jia.ea.2015`, using a structure\n",
        "that combined the strengths of NiN :cite:`Lin.Chen.Yan.2013`, repeated blocks :cite:`Simonyan.Zisserman.2014`,\n",
        "and a cocktail of convolution kernels. It was arguably also the first network that exhibited a clear distinction among the stem (data ingest), body (data processing), and head (prediction) in a CNN. This design pattern has persisted ever since in the design of deep networks: the *stem* is given by the first two or three convolutions that operate on the image. They extract low-level features from the underlying images. This is followed by a *body* of convolutional blocks. Finally, the *head* maps the features obtained so far to the required classification, segmentation, detection, or tracking problem at hand.\n",
        "\n",
        "The key contribution in GoogLeNet was the design of the network body. It solved the problem of selecting\n",
        "convolution kernels in an ingenious way. While other works tried to identify which convolution, ranging from $1 \\times 1$ to $11 \\times 11$ would be best, it simply *concatenated* multi-branch convolutions.\n",
        "In what follows we introduce a slightly simplified version of GoogLeNet: the original design included a number of tricks for stabilizing training through intermediate loss functions, applied to multiple layers of the network.\n",
        "They are no longer necessary due to the availability of improved training algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066f8e89",
      "metadata": {
        "id": "066f8e89"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd207136",
      "metadata": {
        "id": "fd207136"
      },
      "source": [
        "## (**Inception Blocks**)\n",
        "\n",
        "The basic convolutional block in GoogLeNet is called an *Inception block*,\n",
        "stemming from the meme \"we need to go deeper\" from the movie *Inception*.\n",
        "\n",
        "![Structure of the Inception block.](http://d2l.ai/_images/inception.svg)\n",
        ":label:`fig_inception`\n",
        "\n",
        "As depicted in :numref:`fig_inception`,\n",
        "the inception block consists of four parallel branches.\n",
        "The first three branches use convolutional layers\n",
        "with window sizes of $1\\times 1$, $3\\times 3$, and $5\\times 5$\n",
        "to extract information from different spatial sizes.\n",
        "The middle two branches also add a $1\\times 1$ convolution of the input\n",
        "to reduce the number of channels, reducing the model's complexity.\n",
        "The fourth branch uses a $3\\times 3$ max-pooling layer,\n",
        "followed by a $1\\times 1$ convolutional layer\n",
        "to change the number of channels.\n",
        "The four branches all use appropriate padding to give the input and output the same height and width.\n",
        "Finally, the outputs along each branch are concatenated\n",
        "along the channel dimension and comprise the block's output.\n",
        "The commonly-tuned hyperparameters of the Inception block\n",
        "are the number of output channels per layer, i.e., how to allocate capacity among convolutions of different size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7deda9e0",
      "metadata": {
        "id": "7deda9e0"
      },
      "outputs": [],
      "source": [
        "class Inception(nn.Module):\n",
        "    # c1--c4 are the number of output channels for each branch\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super(Inception, self).__init__(**kwargs)\n",
        "        # Branch 1\n",
        "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
        "        # Branch 2\n",
        "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
        "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
        "        # Branch 3\n",
        "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
        "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
        "        # Branch 4\n",
        "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = F.relu(self.b1_1(x))\n",
        "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
        "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
        "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
        "        return torch.cat((b1, b2, b3, b4), dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a943308",
      "metadata": {
        "id": "2a943308"
      },
      "source": [
        "To gain some intuition for why this network works so well,\n",
        "consider the combination of the filters.\n",
        "They explore the image in a variety of filter sizes.\n",
        "This means that details at different extents\n",
        "can be recognized efficiently by filters of different sizes.\n",
        "At the same time, we can allocate different amounts of parameters\n",
        "for different filters.\n",
        "\n",
        "\n",
        "## [**GoogLeNet Model**]\n",
        "\n",
        "As shown in :numref:`fig_inception_full`, GoogLeNet uses a stack of a total of 9 inception blocks, arranged into three groups with max-pooling in between,\n",
        "and global average pooling in its head to generate its estimates.\n",
        "Max-pooling between inception blocks reduces the dimensionality.\n",
        "At its stem, the first module is similar to AlexNet and LeNet.\n",
        "\n",
        "![The GoogLeNet architecture.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/inception-full-90.svg?raw=1)\n",
        ":label:`fig_inception_full`\n",
        "\n",
        "We can now implement GoogLeNet piece by piece. Let's begin with the stem.\n",
        "The first module uses a 64-channel $7\\times 7$ convolutional layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b8f516",
      "metadata": {
        "id": "95b8f516"
      },
      "outputs": [],
      "source": [
        "class GoogleNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762e20f9",
      "metadata": {
        "id": "762e20f9"
      },
      "source": [
        "The second module uses two convolutional layers:\n",
        "first, a 64-channel $1\\times 1$ convolutional layer,\n",
        "followed by a $3\\times 3$ convolutional layer that triples the number of channels. This corresponds to the second branch in the Inception block and concludes the design of the body. At this point we have 192 channels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f1ae36",
      "metadata": {
        "id": "f2f1ae36"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b2(self):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\n",
        "        nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc6272e",
      "metadata": {
        "id": "dcc6272e"
      },
      "source": [
        "The third module connects two complete Inception blocks in series.\n",
        "The number of output channels of the first Inception block is\n",
        "$64+128+32+32=256$. This amounts to\n",
        "a ratio of the number of output channels\n",
        "among the four branches of $2:4:1:1$. To achieve this, we first reduce the input\n",
        "dimensions by $\\frac{1}{2}$ and by $\\frac{1}{12}$ in the second and third branch respectively\n",
        "to arrive at $96 = 192/2$ and $16 = 192/12$ channels respectively.\n",
        "\n",
        "The number of output channels of the second Inception block\n",
        "is increased to $128+192+96+64=480$, yielding a ratio of $128:192:96:64 = 4:6:3:2$. As before,\n",
        "we need to reduce the number of intermediate dimensions in the second and third channel. A\n",
        "scale of $\\frac{1}{2}$ and $\\frac{1}{8}$ respectively suffices, yielding $128$ and $32$ channels\n",
        "respectively. This is captured by the arguments of the following `Inception` block constructors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a37805",
      "metadata": {
        "id": "33a37805"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b3(self):\n",
        "    return nn.Sequential(Inception(64, (96, 128), (16, 32), 32),\n",
        "                         Inception(128, (128, 192), (32, 96), 64),\n",
        "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112f5cd2",
      "metadata": {
        "id": "112f5cd2"
      },
      "source": [
        "The fourth module is more complicated.\n",
        "It connects five Inception blocks in series,\n",
        "and they have $192+208+48+64=512$, $160+224+64+64=512$,\n",
        "$128+256+64+64=512$, $112+288+64+64=528$,\n",
        "and $256+320+128+128=832$ output channels, respectively.\n",
        "The number of channels assigned to these branches is similar\n",
        "to that in the third module:\n",
        "the second branch with the $3\\times 3$ convolutional layer\n",
        "outputs the largest number of channels,\n",
        "followed by the first branch with only the $1\\times 1$ convolutional layer,\n",
        "the third branch with the $5\\times 5$ convolutional layer,\n",
        "and the fourth branch with the $3\\times 3$ max-pooling layer.\n",
        "The second and third branches will first reduce\n",
        "the number of channels according to the ratio.\n",
        "These ratios are slightly different in different Inception blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2beb4b7a",
      "metadata": {
        "id": "2beb4b7a"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b4(self):\n",
        "    return nn.Sequential(Inception(192, (96, 208), (16, 48), 64),\n",
        "                         Inception(160, (112, 224), (24, 64), 64),\n",
        "                         Inception(128, (128, 256), (24, 64), 64),\n",
        "                         Inception(112, (144, 288), (32, 64), 64),\n",
        "                         Inception(256, (160, 320), (32, 128), 128),\n",
        "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703d40c6",
      "metadata": {
        "id": "703d40c6"
      },
      "source": [
        "The fifth module has two Inception blocks with $256+320+128+128=832$\n",
        "and $384+384+128+128=1024$ output channels.\n",
        "The number of channels assigned to each branch\n",
        "is the same as that in the third and fourth modules,\n",
        "but differs in specific values.\n",
        "It should be noted that the fifth block is followed by the output layer.\n",
        "This block uses the global average pooling layer\n",
        "to change the height and width of each channel to 1, just as in NiN.\n",
        "Finally, we turn the output into a two-dimensional array\n",
        "followed by a fully connected layer\n",
        "whose number of outputs is the number of label classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b836927",
      "metadata": {
        "id": "8b836927"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b5(self):\n",
        "    return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),\n",
        "                         Inception(384, (192, 384), (48, 128), 128),\n",
        "                         nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62f89c54",
      "metadata": {
        "id": "62f89c54"
      },
      "source": [
        "Now that we defined all blocks `b1` through `b5`, it is just a matter of assembling them all into a full network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe47c47d",
      "metadata": {
        "id": "fe47c47d"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def __init__(self, lr=0.1, num_classes=10):\n",
        "    super(GoogleNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),\n",
        "                             self.b5(), nn.LazyLinear(num_classes))\n",
        "    self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94378d8",
      "metadata": {
        "id": "b94378d8"
      },
      "source": [
        "The GoogLeNet model is computationally complex. Note the large number of\n",
        "relatively arbitrary hyperparameters in terms of the number of channels chosen, the number of blocks prior to dimensionality reduction, the relative partitioning of capacity across channels, etc. Much of it is due to the\n",
        "fact that at the time when GoogLeNet was introduced, automatic tools for network definition or design exploration\n",
        "were not yet available. For instance, by now we take it for granted that a competent deep learning framework is capable of inferring dimensionalities of input tensors automatically. At the time, many such configurations had to be specified explicitly by the experimenter, thus often slowing down active experimentation. Moreover, the tools needed for automatic exploration were still in flux and initial experiments largely amounted to costly brute-force exploration, genetic algorithms, and similar strategies.\n",
        "\n",
        "For now the only modification we will carry out is to\n",
        "[**reduce the input height and width from 224 to 96\n",
        "to have a reasonable training time on Fashion-MNIST.**]\n",
        "This simplifies the computation. Let's have a look at the\n",
        "changes in the shape of the output between the various modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b695b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83b695b7",
        "outputId": "d49d973c-cf74-4d69-fb6b-52259017a6ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
            "Sequential output shape:\t torch.Size([1, 192, 12, 12])\n",
            "Sequential output shape:\t torch.Size([1, 480, 6, 6])\n",
            "Sequential output shape:\t torch.Size([1, 832, 3, 3])\n",
            "Sequential output shape:\t torch.Size([1, 1024])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "model = GoogleNet().layer_summary((1, 1, 96, 96))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3be870",
      "metadata": {
        "id": "2f3be870"
      },
      "source": [
        "## [**Training**]\n",
        "\n",
        "As before, we train our model using the Fashion-MNIST dataset.\n",
        " We transform it to $96 \\times 96$ pixel resolution\n",
        " before invoking the training procedure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52cffee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "d52cffee",
        "outputId": "479aa65b-3bd6-4a27-ae9a-73dbfa2a006f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T08:42:11.113147</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 238.965625 183.35625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m57454c2836\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m57454c2836\" x=\"30.103125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m57454c2836\" x=\"69.163125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m57454c2836\" x=\"108.223125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m57454c2836\" x=\"147.283125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m57454c2836\" x=\"186.343125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m57454c2836\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m3c5de78d09\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m3c5de78d09\" x=\"30.103125\" y=\"122.091807\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 125.891026) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m3c5de78d09\" x=\"30.103125\" y=\"91.970033\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 95.769252) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m3c5de78d09\" x=\"30.103125\" y=\"61.848259\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.5 -->\n      <g transform=\"translate(7.2 65.647478) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m3c5de78d09\" x=\"30.103125\" y=\"31.726485\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 35.525704) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path d=\"M 34.954394 13.5 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 49.633125 13.586313 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_15\"/>\n   <g id=\"line2d_16\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 49.633125 13.586313 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 49.633125 139.5 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 49.633125 13.586313 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 49.633125 139.5 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 49.633125 13.586313 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 49.633125 139.5 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 49.633125 139.5 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \nL 190.861259 116.936037 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \nL 190.861259 116.936037 \nL 200.605438 118.305449 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \nL 190.861259 116.936037 \nL 200.605438 118.305449 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \nL 205.873125 117.152191 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \nL 190.861259 116.936037 \nL 200.605438 118.305449 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \nL 205.873125 117.152191 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \nL 205.873125 105.023199 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \nL 190.861259 116.936037 \nL 200.605438 118.305449 \nL 210.349618 120.821751 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \nL 205.873125 117.152191 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \nL 205.873125 105.023199 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \nL 190.861259 116.936037 \nL 200.605438 118.305449 \nL 210.349618 120.821751 \nL 220.093797 121.777792 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \nL 205.873125 117.152191 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \nL 205.873125 105.023199 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \nL 190.861259 116.936037 \nL 200.605438 118.305449 \nL 210.349618 120.821751 \nL 220.093797 121.777792 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \nL 205.873125 117.152191 \nL 225.403125 121.547638 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \nL 205.873125 105.023199 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 13.553742 \nL 54.442752 13.59964 \nL 64.186931 13.650473 \nL 73.93111 13.720112 \nL 83.675289 13.821042 \nL 93.419468 14.011596 \nL 103.163647 14.413822 \nL 112.907826 15.78454 \nL 122.652006 26.180212 \nL 132.396185 60.380434 \nL 142.140364 90.72684 \nL 151.884543 101.671103 \nL 161.628722 106.781863 \nL 171.372901 110.853407 \nL 181.11708 114.151883 \nL 190.861259 116.936037 \nL 200.605438 118.305449 \nL 210.349618 120.821751 \nL 220.093797 121.777792 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 49.633125 13.586313 \nL 69.163125 13.690257 \nL 88.693125 13.901505 \nL 108.223125 14.819133 \nL 127.753125 49.216899 \nL 147.283125 93.459533 \nL 166.813125 107.005954 \nL 186.343125 114.143359 \nL 205.873125 117.152191 \nL 225.403125 121.547638 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 49.633125 139.5 \nL 69.163125 135.973083 \nL 88.693125 137.069287 \nL 108.223125 133.727057 \nL 127.753125 130.015454 \nL 147.283125 114.632855 \nL 166.813125 107.8352 \nL 186.343125 106.500691 \nL 205.873125 105.023199 \nL 225.403125 103.390809 \n\" clip-path=\"url(#pe37f35b869)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 145.8 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 145.8 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 138.8125 60.06875 \nL 218.403125 60.06875 \nQ 220.403125 60.06875 220.403125 58.06875 \nL 220.403125 14.2 \nQ 220.403125 12.2 218.403125 12.2 \nL 138.8125 12.2 \nQ 136.8125 12.2 136.8125 14.2 \nL 136.8125 58.06875 \nQ 136.8125 60.06875 138.8125 60.06875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_127\">\n     <path d=\"M 140.8125 20.298438 \nL 150.8125 20.298438 \nL 160.8125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- train_loss -->\n     <g transform=\"translate(168.8125 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_128\">\n     <path d=\"M 140.8125 35.254688 \nL 150.8125 35.254688 \nL 160.8125 35.254688 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- val_loss -->\n     <g transform=\"translate(168.8125 38.754688) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_129\">\n     <path d=\"M 140.8125 50.210938 \nL 150.8125 50.210938 \nL 160.8125 50.210938 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- val_acc -->\n     <g transform=\"translate(168.8125 53.710938) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe37f35b869\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = GoogleNet(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14eaa685",
      "metadata": {
        "id": "14eaa685"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "A key feature of GoogLeNet is that it is actually *cheaper* to compute than its predecessors\n",
        "while simultaneously providing improved accuracy. This marks the beginning of a much more deliberate\n",
        "network design that trades off the cost of evaluating a network with a reduction in errors. It also marks the beginning of experimentation at a block level with network design hyperparameters, even though it was entirely manual at the time. We will revisit this topic in :numref:`sec_cnn-design` when discussing strategies for network structure exploration.\n",
        "\n",
        "Over the following sections we will encounter a number of design choices (e.g., batch normalization, residual connections, and channel grouping) that allow us to improve networks significantly. For now, you can be proud to have implemented what is arguably the first truly modern CNN.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. GoogLeNet was so successful that it went through a number of iterations, progressively improving speed and accuracy. Try to implement and run some of them. They include the following:\n",
        "    1. Add a batch normalization layer :cite:`Ioffe.Szegedy.2015`, as described later in :numref:`sec_batch_norm`.\n",
        "    1. Make adjustments to the Inception block (width, choice and order of convolutions), as described in :citet:`Szegedy.Vanhoucke.Ioffe.ea.2016`.\n",
        "    1. Use label smoothing for model regularization, as described in :citet:`Szegedy.Vanhoucke.Ioffe.ea.2016`.\n",
        "    1. Make further adjustments to the Inception block by adding residual connection :cite:`Szegedy.Ioffe.Vanhoucke.ea.2017`, as described later in :numref:`sec_resnet`.\n",
        "1. What is the minimum image size needed for GoogLeNet to work?\n",
        "1. Can you design a variant of GoogLeNet that works on Fashion-MNIST's native resolution of $28 \\times 28$ pixels? How would you need to change the stem, the body, and the head of the network, if anything at all?\n",
        "1. Compare the model parameter sizes of AlexNet, VGG, NiN, and GoogLeNet. How do the latter two network\n",
        "   architectures significantly reduce the model parameter size?\n",
        "1. Compare the amount of computation needed in GoogLeNet and AlexNet. How does this affect the design of an accelerator chip, e.g., in terms of memory size, memory bandwidth, cache size, the amount of computation, and the benefit of specialized operations?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "class NormInception(nn.Module):\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super().__init__(*kwargs)\n",
        "        self.b1 = nn.Sequential(nn.LazyConv2d(c1, kernel_size=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU())\n",
        "        self.b2 = nn.Sequential(nn.LazyConv2d(c2[0], kernel_size=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c2[1], kernel_size=3, padding=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU())\n",
        "        self.b3 = nn.Sequential(nn.LazyConv2d(c3[0], kernel_size=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[1], kernel_size=5, padding=2),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU())\n",
        "        self.b4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.LazyConv2d(c4, kernel_size=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        o1 = self.b1(x)\n",
        "        o2 = self.b2(x)\n",
        "        o3 = self.b3(x)\n",
        "        o4 = self.b4(x)\n",
        "        return torch.cat((o1,o2,o3,o4),dim=1)\n",
        "\n",
        "class NormGoogleNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "                             nn.LazyBatchNorm2d(),\n",
        "                             nn.ReLU(),\n",
        "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def b2(self):\n",
        "        return nn.Sequential(nn.LazyConv2d(64, kernel_size=1),\n",
        "                             nn.LazyBatchNorm2d(),nn.ReLU(),\n",
        "                             nn.LazyConv2d(192, kernel_size=3, padding=1),\n",
        "                             nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def b3(self):\n",
        "        return nn.Sequential(NormInception(64, (96, 128), (16, 32), 32),\n",
        "                             NormInception(128, (128, 192), (32, 96), 64),\n",
        "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def b4(self):\n",
        "        return nn.Sequential(NormInception(192, (96, 208), (16, 48), 64),\n",
        "                             NormInception(160, (112, 224), (24, 64), 64),\n",
        "                             NormInception(128, (128, 256), (24, 64), 64),\n",
        "                             NormInception(112, (144, 288), (32, 64), 64),\n",
        "                             NormInception(256, (160, 320), (32, 128), 128),\n",
        "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def b5(self):\n",
        "        return nn.Sequential(NormInception(256, (160, 320), (32, 128), 128),\n",
        "                             NormInception(384, (192, 384), (48, 128), 128),\n",
        "                             nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())\n",
        "\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),\n",
        "                                 self.b5(), nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)\n",
        "\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super().__init__(*kwargs)\n",
        "        self.b1 = nn.Sequential(nn.LazyConv2d(c1, kernel_size=1),\n",
        "                                nn.ReLU())\n",
        "        self.b2 = nn.Sequential(nn.LazyConv2d(c2[0], kernel_size=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c2[1], kernel_size=3, padding=1),\n",
        "                                nn.ReLU())\n",
        "        self.b3 = nn.Sequential(nn.LazyConv2d(c3[0], kernel_size=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[1], kernel_size=3, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[2], kernel_size=3, padding=1),\n",
        "                                nn.ReLU())\n",
        "        self.b4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "                                nn.LazyConv2d(c4, kernel_size=1),\n",
        "                                nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        o1 = self.b1(x)\n",
        "        o2 = self.b2(x)\n",
        "        o3 = self.b3(x)\n",
        "        o4 = self.b4(x)\n",
        "        return torch.cat((o1,o2,o3,o4),dim=1)\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super().__init__(*kwargs)\n",
        "        self.b1 = nn.Sequential(nn.LazyConv2d(c1, kernel_size=1),\n",
        "                                nn.ReLU())\n",
        "        self.b2 = nn.Sequential(nn.LazyConv2d(c2[0], kernel_size=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c2[1], kernel_size=(1,3), padding=(0,1)),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c2[2], kernel_size=(3,1), padding=(1,0)),\n",
        "                                nn.ReLU())\n",
        "        self.b3 = nn.Sequential(nn.LazyConv2d(c3[0], kernel_size=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[1], kernel_size=(1,3), padding=(0,1)),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[2], kernel_size=(3,1), padding=(1,0)),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[3], kernel_size=(1,3), padding=(0,1)),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[4], kernel_size=(3,1), padding=(1,0)),\n",
        "                                nn.ReLU())\n",
        "        self.b4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "                                nn.LazyConv2d(c4, kernel_size=1),\n",
        "                                nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        o1 = self.b1(x)\n",
        "        o2 = self.b2(x)\n",
        "        o3 = self.b3(x)\n",
        "        o4 = self.b4(x)\n",
        "        return torch.cat((o1,o2,o3,o4),dim=1)\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super().__init__(*kwargs)\n",
        "        self.b1 = nn.Sequential(nn.LazyConv2d(c1, kernel_size=1),\n",
        "                                nn.ReLU())\n",
        "        self.b2 = nn.Sequential(nn.LazyConv2d(c2[0], kernel_size=1),\n",
        "                                nn.ReLU())\n",
        "        self.b2_1 = nn.Sequential(nn.LazyConv2d(c2[1], kernel_size=(1,3), padding=(0,1)),\n",
        "                                nn.ReLU())\n",
        "        self.b2_2 = nn.Sequential(nn.LazyConv2d(c2[2], kernel_size=(3,1), padding=(1,0)),\n",
        "                                nn.ReLU())\n",
        "        self.b3 = nn.Sequential(nn.LazyConv2d(c3[0], kernel_size=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[1], kernel_size=3, padding=1),\n",
        "                                nn.ReLU())\n",
        "        self.b3_1 = nn.Sequential(nn.LazyConv2d(c3[2], kernel_size=(1,3), padding=(0,1)),\n",
        "                                nn.ReLU())\n",
        "        self.b3_2 = nn.Sequential(nn.LazyConv2d(c3[3], kernel_size=(3,1), padding=(1,0)),\n",
        "                                nn.ReLU())\n",
        "        self.b4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "                                nn.LazyConv2d(c4, kernel_size=1),\n",
        "                                nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        o1 = self.b1(x)\n",
        "        o2 = self.b2(x)\n",
        "        o2_1 = self.b2_1(o2)\n",
        "        o2_2 = self.b2_2(o2)\n",
        "        o3 = self.b3(x)\n",
        "        o3_1 = self.b3_1(o3)\n",
        "        o3_2 = self.b3_2(o2)\n",
        "        o4 = self.b4(x)\n",
        "        return torch.cat((o1,o2_1,o2_2,o3_1,o3_2,o4),dim=1)\n",
        "\n",
        "\n",
        "class LSRGoogleNet(GoogleNet):\n",
        "    def __init__(self, eps=0, lr=0.1, num_classes=10):\n",
        "        super().__init__(lr=lr, num_classes=num_classes)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def loss(self, y_hat, y, averaged=True):\n",
        "        y_hat = y_hat.reshape((-1, y_hat.shape[-1]))\n",
        "        y = y.reshape((-1,))\n",
        "        u = torch.ones(y.shape).tye(torch.float32)/y.shape[-1]\n",
        "        lsr_loss = (1-self.eps)*F.cross_entropy(y_hat, y, reduction='mean' if averaged else 'none')\n",
        "        +self.eps*F.cross_entropy(y_hat, u, reduction='mean' if averaged else 'none')\n",
        "        return lsr_loss\n",
        "\n",
        "\n",
        "class ResInception(nn.Module):\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super().__init__(*kwargs)\n",
        "        self.b1 = nn.Sequential(nn.LazyConv2d(c1, kernel_size=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU())\n",
        "        self.b2 = nn.Sequential(nn.LazyConv2d(c2[0], kernel_size=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c2[1], kernel_size=3, padding=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU())\n",
        "        self.b3 = nn.Sequential(nn.LazyConv2d(c3[0], kernel_size=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU(),\n",
        "                                nn.LazyConv2d(c3[1], kernel_size=5, padding=2),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU())\n",
        "        self.b4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.LazyConv2d(c4, kernel_size=1),\n",
        "                                nn.LazyBatchNorm2d(),\n",
        "                                nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        o1 = self.b1(x)+x\n",
        "        o2 = self.b2(x)+x\n",
        "        o3 = self.b3(x)+x\n",
        "        o4 = self.b4(x)+x\n",
        "        return torch.cat((o1,o2,o3,o4),dim=1)\n",
        "\n",
        "class ResGoogleNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "                             nn.LazyBatchNorm2d(),\n",
        "                             nn.ReLU(),\n",
        "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def b2(self):\n",
        "        return nn.Sequential(nn.LazyConv2d(64, kernel_size=1),\n",
        "                             nn.LazyBatchNorm2d(),nn.ReLU(),\n",
        "                             nn.LazyConv2d(192, kernel_size=3, padding=1),\n",
        "                             nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def b3(self):\n",
        "        return nn.Sequential(ResInception(64, (96, 128), (16, 32), 32),\n",
        "                             ResInception(128, (128, 192), (32, 96), 64),\n",
        "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def b4(self):\n",
        "        return nn.Sequential(ResInception(192, (96, 208), (16, 48), 64),\n",
        "                             ResInception(160, (112, 224), (24, 64), 64),\n",
        "                             ResInception(128, (128, 256), (24, 64), 64),\n",
        "                             ResInception(112, (144, 288), (32, 64), 64),\n",
        "                             ResInception(256, (160, 320), (32, 128), 128),\n",
        "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def b5(self):\n",
        "        return nn.Sequential(ResInception(256, (160, 320), (32, 128), 128),\n",
        "                             ResInception(384, (192, 384), (48, 128), 128),\n",
        "                             nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())\n",
        "\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),\n",
        "                                 self.b5(), nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)\n"
      ],
      "metadata": {
        "id": "vZgeVTZYtg9s"
      },
      "id": "vZgeVTZYtg9s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "'''As GoogleNet halve the image 5 times, so the mininum image size needed is\n",
        "2^5=32'''\n",
        "\n",
        "\n",
        "model = GoogleNet(lr=0.01)\n",
        "X = torch.randn(1,1,32,32)\n",
        "model(X)\n",
        "for m in model.net:\n",
        "    X = m(X)\n",
        "    print(X.shape)\n",
        "torch.Size([1, 64, 8, 8])\n",
        "torch.Size([1, 192, 4, 4])\n",
        "torch.Size([1, 480, 2, 2])\n",
        "torch.Size([1, 832, 1, 1])\n",
        "torch.Size([1, 1024])\n",
        "torch.Size([1, 10])\n",
        "model = GoogleNet(lr=0.01)\n",
        "X = torch.randn(1,1,64,64)\n",
        "model(X)\n",
        "for m in model.net:\n",
        "    X = m(X)\n",
        "    print(X.shape)\n",
        "torch.Size([1, 64, 16, 16])\n",
        "torch.Size([1, 192, 8, 8])\n",
        "torch.Size([1, 480, 4, 4])\n",
        "torch.Size([1, 832, 2, 2])\n",
        "torch.Size([1, 1024])\n",
        "torch.Size([1, 10])"
      ],
      "metadata": {
        "id": "lBPTePDJtrkP"
      },
      "id": "lBPTePDJtrkP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "class SmallGoogLeNet():\n",
        "    def b1(self):\n",
        "        return nn.Sequential(nn.LazyConv2d(64, kernel_size=5, stride=1, padding=2),\n",
        "                             nn.ReLU(),\n",
        "                             # nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "                            )\n",
        "\n",
        "     def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.b1(),self.b2(), self.b3(), self.b4(),\n",
        "                                 self.b5(), nn.LazyLinear(num_classes))\n",
        "        self.net.apply(d2l.init_cnn)\n"
      ],
      "metadata": {
        "id": "Lod1nLmItybr"
      },
      "id": "Lod1nLmItybr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The substantial reduction in parameters in both NiN and GoogLeNet is accomplished by utilizing **1x1 convolutions** and the **inception module**, respectively. These methods enable the models to efficiently capture features while keeping the parameter count lower. In both architectures, the 1x1 convolutions serve as **bottleneck layers**, reducing the dimensionality of feature maps, which in turn decreases the number of parameters in the following layers."
      ],
      "metadata": {
        "id": "vo-1vx0lt50t"
      },
      "id": "vo-1vx0lt50t"
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogleNet(lr=0.01)\n",
        "X = torch.randn(1,3, 224, 224)\n",
        "_ = model(X)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)\n",
        "Total parameters: 5983802"
      ],
      "metadata": {
        "id": "dOAQLXtqt8rl"
      },
      "id": "dOAQLXtqt8rl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. GoogLeNet and AlexNet are both deep neural network architectures, but their computational demands differ significantly due to their design. Comparing the computation requirements of these models can provide insights for accelerator chip design.\n",
        "\n",
        "### Computation Requirements:\n",
        "- **GoogLeNet**: Leverages the inception module, which uses multiple parallel convolution and pooling paths. This design increases parallelism, reducing computation in each path. Additionally, 1x1 convolutions help lower the number of parameters and computational load.\n",
        "- **AlexNet**: Features a simpler, more traditional architecture with fewer parallel paths, relying on convolutional and pooling layers. This leads to higher computational demands due to uniform filter sizes and the overall depth of the architecture.\n",
        "\n",
        "### Memory Size and Bandwidth:\n",
        "- **GoogLeNet**: Parallel paths in the inception module enable more efficient memory usage by storing intermediate results separately, potentially optimizing memory bandwidth.\n",
        "- **AlexNet**: Requires larger memory space for intermediate results due to its deeper structure and uniform filter sizes, leading to higher memory bandwidth needs.\n",
        "\n",
        "### Cache Size:\n",
        "- **GoogLeNet**: The parallelism in GoogLeNet could benefit from a larger cache to handle the intermediate data from multiple paths efficiently.\n",
        "- **AlexNet**: Its deeper structure may also benefit from a large cache, but due to the sequential nature of its operations, temporal locality might make caching more straightforward.\n",
        "\n",
        "### Computation:\n",
        "- **GoogLeNet**: The use of parallelism and 1x1 convolutions leads to less overall computation, especially given its performance.\n",
        "- **AlexNet**: Requires more computation due to the lack of dimensionality reduction and more uniform layer structure.\n",
        "\n",
        "### Specialized Operations:\n",
        "Both architectures can benefit from accelerator chips, particularly for hardware-optimized operations like 1x1 convolutions and depth-wise separable convolutions (more common in GoogLeNet).\n",
        "\n",
        "### Accelerator Chip Design Considerations:\n",
        "- **Memory**: GoogLeNet may require more efficient memory structures for parallel paths, while AlexNet may need larger memory capacity to support deeper layers.\n",
        "- **Memory Bandwidth**: Sufficient bandwidth is needed to accommodate the distinct memory access patterns of each architecture.\n",
        "- **Cache**: Larger cache sizes would help reduce memory access latency and improve efficiency for both architectures.\n",
        "- **Specialized Operations**: Adding hardware units for 1x1 convolutions or other common operations could enhance performance.\n",
        "- **Parallelism**: Optimizing for parallelism is key for architectures like GoogLeNet that heavily utilize parallel paths.\n",
        "\n",
        "In conclusion, the differences between GoogLeNet and AlexNet influence the design of accelerator chips in terms of memory, bandwidth, cache, and specialized operations. Designing chips that cater to these models' specific needs can significantly improve performance and efficiency."
      ],
      "metadata": {
        "id": "hw08fmv4uC8_"
      },
      "id": "hw08fmv4uC8_"
    },
    {
      "cell_type": "markdown",
      "id": "1f196dce",
      "metadata": {
        "id": "1f196dce"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/82)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7df70349",
      "metadata": {
        "id": "7df70349"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8502f85",
      "metadata": {
        "id": "c8502f85"
      },
      "source": [
        "# Batch Normalization\n",
        ":label:`sec_batch_norm`\n",
        "\n",
        "Training deep neural networks is difficult.\n",
        "Getting them to converge in a reasonable amount of time can be tricky.\n",
        "In this section, we describe *batch normalization*, a popular and effective technique\n",
        "that consistently accelerates the convergence of deep networks :cite:`Ioffe.Szegedy.2015`.\n",
        "Together with residual blocks---covered later in :numref:`sec_resnet`---batch normalization\n",
        "has made it possible for practitioners to routinely train networks with over 100 layers.\n",
        "A secondary (serendipitous) benefit of batch normalization lies in its inherent regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b44765",
      "metadata": {
        "id": "f7b44765"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85aa6aae",
      "metadata": {
        "id": "85aa6aae"
      },
      "source": [
        "## Training Deep Networks\n",
        "\n",
        "When working with data, we often preprocess before training.\n",
        "Choices regarding data preprocessing often make an enormous difference in the final results.\n",
        "Recall our application of MLPs to predicting house prices (:numref:`sec_kaggle_house`).\n",
        "Our first step when working with real data\n",
        "was to standardize our input features to have\n",
        "zero mean $\\boldsymbol{\\mu} = 0$ and unit variance $\\boldsymbol{\\Sigma} = \\boldsymbol{1}$ across multiple observations :cite:`friedman1987exploratory`, frequently rescaling the latter so  that the diagonal is unity, i.e., $\\Sigma_{ii} = 1$.\n",
        "Yet another strategy is to rescale vectors to unit length, possibly zero mean *per observation*.\n",
        "This can work well, e.g., for spatial sensor data. These preprocessing techniques and many others, are\n",
        "beneficial for keeping the estimation problem well controlled.\n",
        "For a review of feature selection and extraction see the article of :citet:`guyon2008feature`, for example.\n",
        "Standardizing vectors also has the nice side-effect of constraining the function complexity of functions that act upon it. For instance, the celebrated radius-margin bound :cite:`Vapnik95` in support vector machines and the Perceptron Convergence Theorem :cite:`Novikoff62` rely on inputs of bounded norm.\n",
        "\n",
        "Intuitively, this standardization plays nicely with our optimizers\n",
        "since it puts the parameters *a priori* on a similar scale.\n",
        "As such, it is only natural to ask whether a corresponding normalization step *inside* a deep network\n",
        "might not be beneficial. While this is not quite the reasoning that led to the invention of batch normalization :cite:`Ioffe.Szegedy.2015`, it is a useful way of understanding it and its cousin, layer normalization :cite:`Ba.Kiros.Hinton.2016`, within a unified framework.\n",
        "\n",
        "Second, for a typical MLP or CNN, as we train,\n",
        "the variables\n",
        "in intermediate layers (e.g., affine transformation outputs in MLP)\n",
        "may take values with widely varying magnitudes:\n",
        "whether along the layers from input to output, across units in the same layer,\n",
        "and over time due to our updates to the model parameters.\n",
        "The inventors of batch normalization postulated informally\n",
        "that this drift in the distribution of such variables could hamper the convergence of the network.\n",
        "Intuitively, we might conjecture that if one\n",
        "layer has variable activations that are 100 times that of another layer,\n",
        "this might necessitate compensatory adjustments in the learning rates. Adaptive solvers\n",
        "such as AdaGrad :cite:`Duchi.Hazan.Singer.2011`, Adam :cite:`Kingma.Ba.2014`, Yogi :cite:`Zaheer.Reddi.Sachan.ea.2018`, or Distributed Shampoo :cite:`anil2020scalable` aim to address this from the viewpoint of optimization, e.g., by adding aspects of second-order methods.\n",
        "The alternative is to prevent the problem from occurring, simply by adaptive normalization.\n",
        "\n",
        "Third, deeper networks are complex and tend to be more liable to overfitting.\n",
        "This means that regularization becomes more critical. A common technique for regularization is noise\n",
        "injection. This has been known for a long time, e.g., with regard to noise injection for the\n",
        "inputs :cite:`Bishop.1995`. It also forms the basis of dropout in :numref:`sec_dropout`. As it turns out, quite serendipitously, batch normalization conveys all three benefits: preprocessing, numerical stability, and regularization.\n",
        "\n",
        "Batch normalization is applied to individual layers, or optionally, to all of them:\n",
        "In each training iteration,\n",
        "we first normalize the inputs (of batch normalization)\n",
        "by subtracting their mean and\n",
        "dividing by their standard deviation,\n",
        "where both are estimated based on the statistics of the current minibatch.\n",
        "Next, we apply a scale coefficient and an offset to recover the lost degrees\n",
        "of freedom. It is precisely due to this *normalization* based on *batch* statistics\n",
        "that *batch normalization* derives its name.\n",
        "\n",
        "Note that if we tried to apply batch normalization with minibatches of size 1,\n",
        "we would not be able to learn anything.\n",
        "That is because after subtracting the means,\n",
        "each hidden unit would take value 0.\n",
        "As you might guess, since we are devoting a whole section to batch normalization,\n",
        "with large enough minibatches the approach proves effective and stable.\n",
        "One takeaway here is that when applying batch normalization,\n",
        "the choice of batch size is\n",
        "even more significant than without batch normalization, or at least,\n",
        "suitable calibration is needed as we might adjust batch size.\n",
        "\n",
        "Denote by $\\mathcal{B}$ a minibatch and let $\\mathbf{x} \\in \\mathcal{B}$ be an input to\n",
        "batch normalization ($\\textrm{BN}$). In this case the batch normalization is defined as follows:\n",
        "\n",
        "$$\\textrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$\n",
        ":eqlabel:`eq_batchnorm`\n",
        "\n",
        "In :eqref:`eq_batchnorm`,\n",
        "$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ is the  sample mean\n",
        "and $\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}$ is the sample standard deviation of the minibatch $\\mathcal{B}$.\n",
        "After applying standardization,\n",
        "the resulting minibatch\n",
        "has zero mean and unit variance.\n",
        "The choice of unit variance\n",
        "(rather than some other magic number) is arbitrary. We recover this degree of freedom\n",
        "by including an elementwise\n",
        "*scale parameter* $\\boldsymbol{\\gamma}$ and *shift parameter* $\\boldsymbol{\\beta}$\n",
        "that have the same shape as $\\mathbf{x}$. Both are parameters that\n",
        "need to be learned as part of model training.\n",
        "\n",
        "The variable magnitudes\n",
        "for intermediate layers cannot diverge during training\n",
        "since batch normalization actively centers and rescales them back\n",
        "to a given mean and size (via $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and ${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$).\n",
        "Practical experience confirms that, as alluded to when discussing feature rescaling, batch normalization seems to allow for more aggressive learning rates.\n",
        "We calculate $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and ${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$ in :eqref:`eq_batchnorm` as follows:\n",
        "\n",
        "$$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B} = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}\n",
        "\\textrm{ and }\n",
        "\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}})^2 + \\epsilon.$$\n",
        "\n",
        "Note that we add a small constant $\\epsilon > 0$\n",
        "to the variance estimate\n",
        "to ensure that we never attempt division by zero,\n",
        "even in cases where the empirical variance estimate might be very small or vanish.\n",
        "The estimates $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and ${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$ counteract the scaling issue\n",
        "by using noisy estimates of mean and variance.\n",
        "You might think that this noisiness should be a problem.\n",
        "On the contrary, it is actually beneficial.\n",
        "\n",
        "This turns out to be a recurring theme in deep learning.\n",
        "For reasons that are not yet well-characterized theoretically,\n",
        "various sources of noise in optimization\n",
        "often lead to faster training and less overfitting:\n",
        "this variation appears to act as a form of regularization.\n",
        ":citet:`Teye.Azizpour.Smith.2018` and :citet:`Luo.Wang.Shao.ea.2018`\n",
        "related the properties of batch normalization to Bayesian priors and penalties, respectively.\n",
        "In particular, this sheds some light on the puzzle\n",
        "of why batch normalization works best for moderate minibatch sizes in the 50--100 range.\n",
        "This particular size of minibatch seems to inject just the \"right amount\" of noise per layer, both in terms of scale via $\\hat{\\boldsymbol{\\sigma}}$, and in terms of offset via $\\hat{\\boldsymbol{\\mu}}$: a\n",
        "larger minibatch regularizes less due to the more stable estimates, whereas tiny minibatches\n",
        "destroy useful signal due to high variance. Exploring this direction further, considering alternative types\n",
        "of preprocessing and filtering may yet lead to other effective types of regularization.\n",
        "\n",
        "Fixing a trained model, you might think\n",
        "that we would prefer using the entire dataset\n",
        "to estimate the mean and variance.\n",
        "Once training is complete, why would we want\n",
        "the same image to be classified differently,\n",
        "depending on the batch in which it happens to reside?\n",
        "During training, such exact calculation is infeasible\n",
        "because the intermediate variables\n",
        "for all data examples\n",
        "change every time we update our model.\n",
        "However, once the model is trained,\n",
        "we can calculate the means and variances\n",
        "of each layer's variables based on the entire dataset.\n",
        "Indeed this is standard practice for\n",
        "models employing batch normalization;\n",
        "thus batch normalization layers function differently\n",
        "in *training mode* (normalizing by minibatch statistics)\n",
        "than in *prediction mode* (normalizing by dataset statistics).\n",
        "In this form they closely resemble the behavior of dropout regularization of :numref:`sec_dropout`,\n",
        "where noise is only injected during training.\n",
        "\n",
        "\n",
        "## Batch Normalization Layers\n",
        "\n",
        "Batch normalization implementations for fully connected layers\n",
        "and convolutional layers are slightly different.\n",
        "One key difference between batch normalization and other layers\n",
        "is that because the former operates on a full minibatch at a time,\n",
        "we cannot just ignore the batch dimension\n",
        "as we did before when introducing other layers.\n",
        "\n",
        "### Fully Connected Layers\n",
        "\n",
        "When applying batch normalization to fully connected layers,\n",
        ":citet:`Ioffe.Szegedy.2015`, in their original paper inserted batch normalization after the affine transformation\n",
        "and *before* the nonlinear activation function. Later applications experimented with\n",
        "inserting batch normalization right *after* activation functions.\n",
        "Denoting the input to the fully connected layer by $\\mathbf{x}$,\n",
        "the affine transformation\n",
        "by $\\mathbf{W}\\mathbf{x} + \\mathbf{b}$ (with the weight parameter $\\mathbf{W}$ and the bias parameter $\\mathbf{b}$),\n",
        "and the activation function by $\\phi$,\n",
        "we can express the computation of a batch-normalization-enabled,\n",
        "fully connected layer output $\\mathbf{h}$ as follows:\n",
        "\n",
        "$$\\mathbf{h} = \\phi(\\textrm{BN}(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) ).$$\n",
        "\n",
        "Recall that mean and variance are computed\n",
        "on the *same* minibatch\n",
        "on which the transformation is applied.\n",
        "\n",
        "### Convolutional Layers\n",
        "\n",
        "Similarly, with convolutional layers,\n",
        "we can apply batch normalization after the convolution\n",
        "but before the nonlinear activation function. The key difference from batch normalization\n",
        "in fully connected layers is that we apply the operation on a per-channel basis\n",
        "*across all locations*. This is compatible with our assumption of translation\n",
        "invariance that led to convolutions: we assumed that the specific location of a pattern\n",
        "within an image was not critical for the purpose of understanding.\n",
        "\n",
        "Assume that our minibatches contain $m$ examples\n",
        "and that for each channel,\n",
        "the output of the convolution has height $p$ and width $q$.\n",
        "For convolutional layers, we carry out each batch normalization\n",
        "over the $m \\cdot p \\cdot q$ elements per output channel simultaneously.\n",
        "Thus, we collect the values over all spatial locations\n",
        "when computing the mean and variance\n",
        "and consequently\n",
        "apply the same mean and variance\n",
        "within a given channel\n",
        "to normalize the value at each spatial location.\n",
        "Each channel has its own scale and shift parameters,\n",
        "both of which are scalars.\n",
        "\n",
        "### Layer Normalization\n",
        ":label:`subsec_layer-normalization-in-bn`\n",
        "\n",
        "Note that in the context of convolutions the batch normalization is well defined even for\n",
        "minibatches of size 1: after all, we have all the locations across an image to average. Consequently,\n",
        "mean and variance are well defined, even if it is just within a single observation. This consideration\n",
        "led :citet:`Ba.Kiros.Hinton.2016` to introduce the notion of *layer normalization*. It works just like\n",
        "a batch norm, only that it is applied to one observation at a time. Consequently both the offset and the scaling factor are scalars. For an $n$-dimensional vector $\\mathbf{x}$, layer norms are given by\n",
        "\n",
        "$$\\mathbf{x} \\rightarrow \\textrm{LN}(\\mathbf{x}) =  \\frac{\\mathbf{x} - \\hat{\\mu}}{\\hat\\sigma},$$\n",
        "\n",
        "where scaling and offset are applied coefficient-wise\n",
        "and given by\n",
        "\n",
        "$$\\hat{\\mu} \\stackrel{\\textrm{def}}{=} \\frac{1}{n} \\sum_{i=1}^n x_i \\textrm{ and }\n",
        "\\hat{\\sigma}^2 \\stackrel{\\textrm{def}}{=} \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})^2 + \\epsilon.$$\n",
        "\n",
        "As before we add a small offset $\\epsilon > 0$ to prevent division by zero. One of the major benefits of using layer normalization is that it prevents divergence. After all, ignoring $\\epsilon$, the output of the layer normalization is scale independent. That is, we have $\\textrm{LN}(\\mathbf{x}) \\approx \\textrm{LN}(\\alpha \\mathbf{x})$ for any choice of $\\alpha \\neq 0$. This becomes an equality for $|\\alpha| \\to \\infty$ (the approximate equality is due to the offset $\\epsilon$ for the variance).\n",
        "\n",
        "Another advantage of the layer normalization is that it does not depend on the minibatch size. It is also independent of whether we are in training or test regime. In other words, it is simply a deterministic transformation that standardizes the activations to a given scale. This can be very beneficial in preventing divergence in optimization. We skip further details and recommend that interested readers consult the original paper.\n",
        "\n",
        "### Batch Normalization During Prediction\n",
        "\n",
        "As we mentioned earlier, batch normalization typically behaves differently\n",
        "in training mode than in prediction mode.\n",
        "First, the noise in the sample mean and the sample variance\n",
        "arising from estimating each on minibatches\n",
        "is no longer desirable once we have trained the model.\n",
        "Second, we might not have the luxury\n",
        "of computing per-batch normalization statistics.\n",
        "For example,\n",
        "we might need to apply our model to make one prediction at a time.\n",
        "\n",
        "Typically, after training, we use the entire dataset\n",
        "to compute stable estimates of the variable statistics\n",
        "and then fix them at prediction time.\n",
        "Hence, batch normalization behaves differently during training than at test time.\n",
        "Recall that dropout also exhibits this characteristic.\n",
        "\n",
        "## (**Implementation from Scratch**)\n",
        "\n",
        "To see how batch normalization works in practice, we implement one from scratch below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a79b8f2",
      "metadata": {
        "id": "9a79b8f2"
      },
      "outputs": [],
      "source": [
        "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
        "    # Use is_grad_enabled to determine whether we are in training mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of X, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used\n",
        "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
        "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_mean.data, moving_var.data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49370dea",
      "metadata": {
        "id": "49370dea"
      },
      "source": [
        "We can now [**create a proper `BatchNorm` layer.**]\n",
        "Our layer will maintain proper parameters\n",
        "for scale `gamma` and shift `beta`,\n",
        "both of which will be updated in the course of training.\n",
        "Additionally, our layer will maintain\n",
        "moving averages of the means and variances\n",
        "for subsequent use during model prediction.\n",
        "\n",
        "Putting aside the algorithmic details,\n",
        "note the design pattern underlying our implementation of the layer.\n",
        "Typically, we define the mathematics in a separate function, say `batch_norm`.\n",
        "We then integrate this functionality into a custom layer,\n",
        "whose code mostly addresses bookkeeping matters,\n",
        "such as moving data to the right device context,\n",
        "allocating and initializing any required variables,\n",
        "keeping track of moving averages (here for mean and variance), and so on.\n",
        "This pattern enables a clean separation of mathematics from boilerplate code.\n",
        "Also note that for the sake of convenience\n",
        "we did not worry about automatically inferring the input shape here;\n",
        "thus we need to specify the number of features throughout.\n",
        "By now all modern deep learning frameworks offer automatic detection of size and shape in the\n",
        "high-level batch normalization APIs (in practice we will use this instead).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a591dd1",
      "metadata": {
        "id": "8a591dd1"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and\n",
        "        # 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
        "        # the device where X is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.1)\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e8bc89f",
      "metadata": {
        "id": "7e8bc89f"
      },
      "source": [
        "We used `momentum` to govern the aggregation over past mean and variance estimates. This is somewhat of a misnomer as it has nothing whatsoever to do with the *momentum* term of optimization. Nonetheless, it is the commonly adopted name for this term and in deference to API naming convention we use the same variable name in our code.\n",
        "\n",
        "## [**LeNet with Batch Normalization**]\n",
        "\n",
        "To see how to apply `BatchNorm` in context,\n",
        "below we apply it to a traditional LeNet model (:numref:`sec_lenet`).\n",
        "Recall that batch normalization is applied\n",
        "after the convolutional layers or fully connected layers\n",
        "but before the corresponding activation functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21c51c36",
      "metadata": {
        "id": "21c51c36"
      },
      "outputs": [],
      "source": [
        "class BNLeNetScratch(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120),\n",
        "            BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),\n",
        "            BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e842386f",
      "metadata": {
        "id": "e842386f"
      },
      "source": [
        "As before, we will [**train our network on the Fashion-MNIST dataset**].\n",
        "This code is virtually identical to that when we first trained LeNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064cdd64",
      "metadata": {
        "id": "064cdd64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "5b868e59-4cf5-442d-945c-a4eff05cf62d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T08:44:28.480258</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 238.965625 183.35625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m5235a2f2ab\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m5235a2f2ab\" x=\"30.103125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m5235a2f2ab\" x=\"69.163125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m5235a2f2ab\" x=\"108.223125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m5235a2f2ab\" x=\"147.283125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m5235a2f2ab\" x=\"186.343125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m5235a2f2ab\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"mf713b8e419\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mf713b8e419\" x=\"30.103125\" y=\"118.829744\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 122.628963) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mf713b8e419\" x=\"30.103125\" y=\"81.499825\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 85.299044) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mf713b8e419\" x=\"30.103125\" y=\"44.169906\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 47.969125) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_10\">\n    <path d=\"M 34.954394 13.5 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_11\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 49.633125 81.897784 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_14\"/>\n   <g id=\"line2d_15\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 49.633125 81.897784 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 49.633125 46.986632 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 49.633125 81.897784 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 49.633125 46.986632 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 49.633125 81.897784 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 49.633125 46.986632 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 49.633125 46.986632 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \nL 190.861259 136.390794 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \nL 190.861259 136.390794 \nL 200.605438 136.034158 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \nL 190.861259 136.390794 \nL 200.605438 136.034158 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \nL 205.873125 128.444617 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \nL 190.861259 136.390794 \nL 200.605438 136.034158 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \nL 205.873125 128.444617 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \nL 205.873125 29.783564 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \nL 190.861259 136.390794 \nL 200.605438 136.034158 \nL 210.349618 137.465632 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \nL 205.873125 128.444617 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \nL 205.873125 29.783564 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \nL 190.861259 136.390794 \nL 200.605438 136.034158 \nL 210.349618 137.465632 \nL 220.093797 139.5 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \nL 205.873125 128.444617 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \nL 205.873125 29.783564 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \nL 190.861259 136.390794 \nL 200.605438 136.034158 \nL 210.349618 137.465632 \nL 220.093797 139.5 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \nL 205.873125 128.444617 \nL 225.403125 119.763098 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \nL 205.873125 29.783564 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 34.954394 13.5 \nL 44.698573 77.509311 \nL 54.442752 93.704854 \nL 64.186931 101.110779 \nL 73.93111 108.572326 \nL 83.675289 113.57763 \nL 93.419468 118.016642 \nL 103.163647 118.738768 \nL 112.907826 122.852999 \nL 122.652006 125.224387 \nL 132.396185 127.081877 \nL 142.140364 128.128267 \nL 151.884543 131.416808 \nL 161.628722 131.129537 \nL 171.372901 133.44864 \nL 181.11708 135.013847 \nL 190.861259 136.390794 \nL 200.605438 136.034158 \nL 210.349618 137.465632 \nL 220.093797 139.5 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 49.633125 81.897784 \nL 69.163125 97.535706 \nL 88.693125 109.26207 \nL 108.223125 109.403955 \nL 127.753125 116.72236 \nL 147.283125 103.24554 \nL 166.813125 96.841509 \nL 186.343125 89.055938 \nL 205.873125 128.444617 \nL 225.403125 119.763098 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 49.633125 46.986632 \nL 69.163125 40.02788 \nL 88.693125 37.259146 \nL 108.223125 37.775976 \nL 127.753125 33.696708 \nL 147.283125 40.729293 \nL 166.813125 39.898672 \nL 186.343125 46.008346 \nL 205.873125 29.783564 \nL 225.403125 33.179878 \n\" clip-path=\"url(#p6299c0582b)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 145.8 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 145.8 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 87.957813 100.434375 \nL 167.548438 100.434375 \nQ 169.548438 100.434375 169.548438 98.434375 \nL 169.548438 54.565625 \nQ 169.548438 52.565625 167.548438 52.565625 \nL 87.957813 52.565625 \nQ 85.957813 52.565625 85.957813 54.565625 \nL 85.957813 98.434375 \nQ 85.957813 100.434375 87.957813 100.434375 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_126\">\n     <path d=\"M 89.957813 60.664063 \nL 99.957813 60.664063 \nL 109.957813 60.664063 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- train_loss -->\n     <g transform=\"translate(117.957813 64.164063) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_127\">\n     <path d=\"M 89.957813 75.620313 \nL 99.957813 75.620313 \nL 109.957813 75.620313 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- val_loss -->\n     <g transform=\"translate(117.957813 79.120313) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_128\">\n     <path d=\"M 89.957813 90.576563 \nL 99.957813 90.576563 \nL 109.957813 90.576563 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- val_acc -->\n     <g transform=\"translate(117.957813 94.076563) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6299c0582b\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "model = BNLeNetScratch(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27eb3037",
      "metadata": {
        "id": "27eb3037"
      },
      "source": [
        "Let's [**have a look at the scale parameter `gamma`\n",
        "and the shift parameter `beta`**] learned\n",
        "from the first batch normalization layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4969fdc2",
      "metadata": {
        "id": "4969fdc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ae652f-9a42-4c35-b962-a6397e32aeb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2.0432, 1.8855, 1.7452, 1.0524, 1.9871, 2.1660], device='cuda:0',\n",
              "        grad_fn=<ViewBackward0>),\n",
              " tensor([ 1.2571,  1.0746,  1.7295, -0.8311, -1.3756, -0.4510], device='cuda:0',\n",
              "        grad_fn=<ViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6befcbce",
      "metadata": {
        "id": "6befcbce"
      },
      "source": [
        "## [**Concise Implementation**]\n",
        "\n",
        "Compared with the `BatchNorm` class,\n",
        "which we just defined ourselves,\n",
        "we can use the `BatchNorm` class defined in high-level APIs from the deep learning framework directly.\n",
        "The code looks virtually identical\n",
        "to our implementation above, except that we no longer need to provide additional arguments for it to get the dimensions right.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef2ab147",
      "metadata": {
        "id": "ef2ab147"
      },
      "outputs": [],
      "source": [
        "class BNLeNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
        "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
        "            nn.Sigmoid(), nn.LazyLinear(num_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bb4f92",
      "metadata": {
        "id": "75bb4f92"
      },
      "source": [
        "Below, we [**use the same hyperparameters to train our model.**]\n",
        "Note that as usual, the high-level API variant runs much faster\n",
        "because its code has been compiled to C++ or CUDA\n",
        "while our custom implementation must be interpreted by Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d6aaf49",
      "metadata": {
        "id": "0d6aaf49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "aaf1d025-6380-4434-f980-f3c4d98d1a93"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"186.11141pt\" viewBox=\"0 0 238.965625 186.11141\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T08:46:36.101280</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 186.11141 \nL 238.965625 186.11141 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 148.55516 \nL 225.403125 148.55516 \nL 225.403125 9.95516 \nL 30.103125 9.95516 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"mc39d7d2cec\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mc39d7d2cec\" x=\"30.103125\" y=\"148.55516\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 163.153598) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#mc39d7d2cec\" x=\"69.163125\" y=\"148.55516\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 163.153598) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#mc39d7d2cec\" x=\"108.223125\" y=\"148.55516\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 163.153598) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mc39d7d2cec\" x=\"147.283125\" y=\"148.55516\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 163.153598) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#mc39d7d2cec\" x=\"186.343125\" y=\"148.55516\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 163.153598) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mc39d7d2cec\" x=\"225.403125\" y=\"148.55516\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 163.153598) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 176.831723) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m24a1c1fbdf\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m24a1c1fbdf\" x=\"30.103125\" y=\"123.490287\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 127.289506) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m24a1c1fbdf\" x=\"30.103125\" y=\"85.993265\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 89.792483) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m24a1c1fbdf\" x=\"30.103125\" y=\"48.496242\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 52.29546) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m24a1c1fbdf\" x=\"30.103125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 14.798438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path d=\"M 34.954394 16.25516 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 49.633125 87.764015 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_15\"/>\n   <g id=\"line2d_16\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 49.633125 87.764015 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 49.633125 47.487619 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 49.633125 87.764015 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 49.633125 47.487619 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 49.633125 87.764015 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 49.633125 47.487619 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 49.633125 47.487619 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \nL 190.861259 139.378419 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \nL 190.861259 139.378419 \nL 200.605438 140.077291 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \nL 190.861259 139.378419 \nL 200.605438 140.077291 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \nL 205.873125 131.231015 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \nL 190.861259 139.378419 \nL 200.605438 140.077291 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \nL 205.873125 131.231015 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \nL 205.873125 35.732718 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \nL 190.861259 139.378419 \nL 200.605438 140.077291 \nL 210.349618 141.181886 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \nL 205.873125 131.231015 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \nL 205.873125 35.732718 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \nL 190.861259 139.378419 \nL 200.605438 140.077291 \nL 210.349618 141.181886 \nL 220.093797 142.25516 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \nL 205.873125 131.231015 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \nL 205.873125 35.732718 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \nL 190.861259 139.378419 \nL 200.605438 140.077291 \nL 210.349618 141.181886 \nL 220.093797 142.25516 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \nL 205.873125 131.231015 \nL 225.403125 132.5287 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \nL 205.873125 35.732718 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 34.954394 16.25516 \nL 44.698573 84.985516 \nL 54.442752 100.710272 \nL 64.186931 108.220853 \nL 73.93111 114.273936 \nL 83.675289 117.840115 \nL 93.419468 121.943668 \nL 103.163647 125.707296 \nL 112.907826 127.458594 \nL 122.652006 128.916692 \nL 132.396185 131.282693 \nL 142.140364 133.458257 \nL 151.884543 134.834122 \nL 161.628722 135.703128 \nL 171.372901 136.555671 \nL 181.11708 138.391797 \nL 190.861259 139.378419 \nL 200.605438 140.077291 \nL 210.349618 141.181886 \nL 220.093797 142.25516 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 49.633125 87.764015 \nL 69.163125 96.989814 \nL 88.693125 111.797818 \nL 108.223125 112.610332 \nL 127.753125 122.052076 \nL 147.283125 119.034397 \nL 166.813125 128.098578 \nL 186.343125 90.757768 \nL 205.873125 131.231015 \nL 225.403125 132.5287 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 49.633125 47.487619 \nL 69.163125 46.727444 \nL 88.693125 41.443301 \nL 108.223125 41.294974 \nL 127.753125 38.82904 \nL 147.283125 40.961239 \nL 166.813125 36.196239 \nL 186.343125 51.492444 \nL 205.873125 35.732718 \nL 225.403125 34.583185 \n\" clip-path=\"url(#p098d8d6536)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 148.55516 \nL 30.103125 9.95516 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 148.55516 \nL 225.403125 9.95516 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 148.55516 \nL 225.403125 148.55516 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 9.95516 \nL 225.403125 9.95516 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 87.957813 103.189535 \nL 167.548438 103.189535 \nQ 169.548438 103.189535 169.548438 101.189535 \nL 169.548438 57.320785 \nQ 169.548438 55.320785 167.548438 55.320785 \nL 87.957813 55.320785 \nQ 85.957813 55.320785 85.957813 57.320785 \nL 85.957813 101.189535 \nQ 85.957813 103.189535 87.957813 103.189535 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_127\">\n     <path d=\"M 89.957813 63.419223 \nL 99.957813 63.419223 \nL 109.957813 63.419223 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- train_loss -->\n     <g transform=\"translate(117.957813 66.919223) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_128\">\n     <path d=\"M 89.957813 78.375473 \nL 99.957813 78.375473 \nL 109.957813 78.375473 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- val_loss -->\n     <g transform=\"translate(117.957813 81.875473) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_129\">\n     <path d=\"M 89.957813 93.331723 \nL 99.957813 93.331723 \nL 109.957813 93.331723 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- val_acc -->\n     <g transform=\"translate(117.957813 96.831723) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p098d8d6536\">\n   <rect x=\"30.103125\" y=\"9.95516\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "model = BNLeNet(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353f1805",
      "metadata": {
        "id": "353f1805"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "Intuitively, batch normalization is thought\n",
        "to make the optimization landscape smoother.\n",
        "However, we must be careful to distinguish between\n",
        "speculative intuitions and true explanations\n",
        "for the phenomena that we observe when training deep models.\n",
        "Recall that we do not even know why simpler\n",
        "deep neural networks (MLPs and conventional CNNs)\n",
        "generalize well in the first place.\n",
        "Even with dropout and weight decay,\n",
        "they remain so flexible that their ability to generalize to unseen data\n",
        "likely needs significantly more refined learning-theoretic generalization guarantees.\n",
        "\n",
        "The original paper proposing batch normalization :cite:`Ioffe.Szegedy.2015`, in addition to introducing a powerful and useful tool,\n",
        "offered an explanation for why it works:\n",
        "by reducing *internal covariate shift*.\n",
        "Presumably by *internal covariate shift* they\n",
        "meant something like the intuition expressed above---the\n",
        "notion that the distribution of variable values changes\n",
        "over the course of training.\n",
        "However, there were two problems with this explanation:\n",
        "i) This drift is very different from *covariate shift*,\n",
        "rendering the name a misnomer. If anything, it is closer to concept drift.\n",
        "ii) The explanation offers an under-specified intuition\n",
        "but leaves the question of *why precisely this technique works*\n",
        "an open question wanting for a rigorous explanation.\n",
        "Throughout this book, we aim to convey the intuitions that practitioners\n",
        "use to guide their development of deep neural networks.\n",
        "However, we believe that it is important\n",
        "to separate these guiding intuitions\n",
        "from established scientific fact.\n",
        "Eventually, when you master this material\n",
        "and start writing your own research papers\n",
        "you will want to be clear to delineate\n",
        "between technical claims and hunches.\n",
        "\n",
        "Following the success of batch normalization,\n",
        "its explanation in terms of *internal covariate shift*\n",
        "has repeatedly surfaced in debates in the technical literature\n",
        "and broader discourse about how to present machine learning research.\n",
        "In a memorable speech given while accepting a Test of Time Award\n",
        "at the 2017 NeurIPS conference,\n",
        "Ali Rahimi used *internal covariate shift*\n",
        "as a focal point in an argument likening\n",
        "the modern practice of deep learning to alchemy.\n",
        "Subsequently, the example was revisited in detail\n",
        "in a position paper outlining\n",
        "troubling trends in machine learning :cite:`Lipton.Steinhardt.2018`.\n",
        "Other authors\n",
        "have proposed alternative explanations for the success of batch normalization,\n",
        "some :cite:`Santurkar.Tsipras.Ilyas.ea.2018`\n",
        "claiming that batch normalization's success comes despite exhibiting behavior\n",
        "that is in some ways opposite to those claimed in the original paper.\n",
        "\n",
        "\n",
        "We note that the *internal covariate shift*\n",
        "is no more worthy of criticism than any of\n",
        "thousands of similarly vague claims\n",
        "made every year in the technical machine learning literature.\n",
        "Likely, its resonance as a focal point of these debates\n",
        "owes to its broad recognizability for the target audience.\n",
        "Batch normalization has proven an indispensable method,\n",
        "applied in nearly all deployed image classifiers,\n",
        "earning the paper that introduced the technique\n",
        "tens of thousands of citations. We conjecture, though, that the guiding principles\n",
        "of regularization through noise injection, acceleration through rescaling and lastly preprocessing\n",
        "may well lead to further inventions of layers and techniques in the future.\n",
        "\n",
        "On a more practical note, there are a number of aspects worth remembering about batch normalization:\n",
        "\n",
        "* During model training, batch normalization continuously adjusts the intermediate output of\n",
        "  the network by utilizing the mean and standard deviation of the minibatch, so that the\n",
        "  values of the intermediate output in each layer throughout the neural network are more stable.\n",
        "* Batch normalization is slightly different for fully connected layers than for convolutional layers. In fact,\n",
        "  for convolutional layers, layer normalization can sometimes be used as an alternative.\n",
        "* Like a dropout layer, batch normalization layers have different behaviors\n",
        "  in training mode than in prediction mode.\n",
        "* Batch normalization is useful for regularization and improving convergence in optimization. By contrast,\n",
        "  the original motivation of reducing internal covariate shift seems not to be a valid explanation.\n",
        "* For more robust models that are less sensitive to input perturbations, consider removing batch normalization :cite:`wang2022removing`.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Should we remove the bias parameter from the fully connected layer or the convolutional layer before the batch normalization? Why?\n",
        "1. Compare the learning rates for LeNet with and without batch normalization.\n",
        "    1. Plot the increase in validation accuracy.\n",
        "    1. How large can you make the learning rate before the optimization fails in both cases?\n",
        "1. Do we need batch normalization in every layer? Experiment with it.\n",
        "1. Implement a \"lite\" version of batch normalization that only removes the mean, or alternatively one that\n",
        "   only removes the variance. How does it behave?\n",
        "1. Fix the parameters `beta` and `gamma`. Observe and analyze the results.\n",
        "1. Can you replace dropout by batch normalization? How does the behavior change?\n",
        "1. Research ideas: think of other normalization transforms that you can apply:\n",
        "    1. Can you apply the probability integral transform?\n",
        "    1. Can you use a full-rank covariance estimate? Why should you probably not do that?\n",
        "    1. Can you use other compact matrix variants (block-diagonal, low-displacement rank, Monarch, etc.)?\n",
        "    1. Does a sparsification compression act as a regularizer?\n",
        "    1. Are there other projections (e.g., convex cone, symmetry group-specific transforms) that you can use?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Yes, it is recommended to remove the bias parameter from both the fully connected and convolutional layers before applying batch normalization. This is because batch normalization already includes a learnable shift (offset) parameter that effectively replaces the role of the bias. Keeping the bias would be redundant, as it would be canceled out by the normalization process, leading to unnecessary computation.\n",
        "\n",
        "Batch normalization is a technique that helps stabilize and accelerate the training of deep neural networks. It normalizes the activations within a layer by subtracting the mean and dividing by the standard deviation of the mini-batch. The normalized activations are then scaled and shifted using learnable parameters: the scale (gamma) and shift (beta) parameters.\n",
        "\n",
        "When using batch normalization, it’s generally recommended to remove the bias parameter from the fully connected (linear) layer before applying batch normalization. This recommendation comes from the idea that the normalization process already includes shifting the activations with the beta parameter of batch normalization. Adding an additional bias term can introduce redundancy and might negatively impact the learning process.\n",
        "\n",
        "For convolutional layers, the use of bias parameters is less clear-cut. Whether to use bias parameters before batch normalization in convolutional layers depends on your specific use case and the design choices you are making.\n",
        "\n",
        "Here’s a general guideline for both cases:\n",
        "\n",
        "Fully Connected (Linear) Layer:\n",
        "\n",
        "Remove Bias: It’s recommended to remove the bias parameter from the fully connected layer before applying batch normalization. This helps avoid the potential redundancy between the bias and beta parameters of batch normalization.\n",
        "Convolutional Layer:\n",
        "\n",
        "With Bias: Some architectures and setups use bias parameters in convolutional layers before batch normalization. The bias parameter can still provide flexibility in modeling, especially in the early stages of the network.\n",
        "Without Bias: If you decide to remove the bias parameter from convolutional layers before batch normalization, you’re essentially letting batch normalization handle both the shifting and scaling of the activations.\n",
        "Remember that the effectiveness of these choices can also depend on the specific architecture, dataset, and optimization process you’re using. It’s often a good idea to experiment with different configurations to find the best setup for your particular use case.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sWdTaOpcucAL"
      },
      "id": "sWdTaOpcucAL"
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import warnings\n",
        "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
        "import d2l\n",
        "from torchsummary import summary\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class LeNet(d2l.Classifier):  #@save\n",
        "    \"\"\"The LeNet-5 model.\"\"\"\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120), nn.Sigmoid(),\n",
        "            nn.LazyLinear(84), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))\n",
        "\n",
        "class BNLeNet(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
        "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
        "            nn.Sigmoid(), nn.LazyLinear(num_classes))\n",
        "\n",
        "def stat_model_acc(model, data, plot_flag):\n",
        "    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "    trainer = d2l.Trainer(max_epochs=10, num_gpus=1,plot_flag=plot_flag)\n",
        "    trainer.fit(model, data)\n",
        "    X,y = next(iter(data.get_dataloader(False)))\n",
        "    X = X.to('cuda')\n",
        "    y = y.to('cuda')\n",
        "    y_hat = model(X)\n",
        "    return model.accuracy(y_hat,y).item()\n",
        "\n",
        "\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "lr_list = [0.001,0.01,0.03,0.1,0.3]\n",
        "le_accs= []\n",
        "ble_accs = []\n",
        "for lr in lr_list[:1]:\n",
        "    le = LeNet(lr=lr)\n",
        "    ble = BNLeNet(lr=lr)\n",
        "    le_acc.append(stat_model_acc(le, data, False))\n",
        "    ble_acc.append(stat_model_acc(ble, data, False))\n",
        "\n"
      ],
      "metadata": {
        "id": "yd2oMeDuuiFU"
      },
      "id": "yd2oMeDuuiFU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The decision to apply batch normalization in every layer of a neural network is more of a design choice rather than a strict rule, influenced by the specific problem, architecture, and training dynamics. This choice can significantly affect the model’s convergence, performance, and training stability. Here are some considerations to guide your decision:\n",
        "\n",
        "### Advantages of Applying Batch Normalization in Every Layer:\n",
        "- **Stabilized Training**: Normalizing activations in every layer helps stabilize training by reducing internal covariate shifts, leading to faster convergence and more consistent gradient flow.\n",
        "- **Regularization**: Batch normalization acts as an inherent regularizer, helping to prevent overfitting. Consistent application throughout the network may enhance regularization.\n",
        "- **Deeper Architectures**: For very deep networks, applying batch normalization in every layer can alleviate gradient vanishing or exploding issues, enabling the training of deeper models.\n",
        "- **Less Sensitivity to Initialization**: It can make the model less sensitive to weight initialization, allowing for larger learning rates and more aggressive optimization techniques.\n",
        "\n",
        "### Considerations Against Applying Batch Normalization in Every Layer:\n",
        "- **Reduced Model Capacity**: Frequent application may limit the network’s ability to fit training data, possibly leading to underfitting, particularly in smaller models.\n",
        "- **Slower Training**: Implementing batch normalization at every layer increases computational overhead, potentially slowing down training, especially on resource-limited hardware.\n",
        "- **Loss of Expressiveness**: Over-normalization can eliminate valuable information from activations, potentially restricting the model’s expressiveness and its ability to memorize useful patterns.\n",
        "- **Instability with Small Batches**: Since batch normalization relies on batch statistics, it may perform poorly with very small batches, leading to instability.\n",
        "\n",
        "### Guidelines and Best Practices:\n",
        "- **Experiment**: It’s advisable to try different configurations, including selective application or applying it in every layer, to assess their effects on validation performance, convergence speed, and generalization.\n",
        "- **Network Depth**: Deeper networks often gain more benefits from consistent batch normalization due to issues with vanishing gradients. Shallower networks may perform well with selective application.\n",
        "- **Use Validation**: Monitor validation performance during training to identify potential overfitting due to excessive batch normalization.\n",
        "- **Small Datasets**: Exercise caution with normalization on small datasets; validate performance to achieve the right balance.\n",
        "- **Different Architectures**: Different architectures may respond uniquely to batch normalization; what works for one may not be optimal for another.\n",
        "\n",
        "In summary, while there are benefits to applying batch normalization in every layer, it is crucial to weigh the trade-offs and experiment with various configurations. The choice should align with the specific use case, model architecture, dataset characteristics, and computational limitations."
      ],
      "metadata": {
        "id": "xJNybo97uryh"
      },
      "id": "xJNybo97uryh"
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "def lite_batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum, mean_flag):\n",
        "    # Use is_grad_enabled to determine whether we are in training mode\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        if mean_flag:\n",
        "            X_hat = X - moving_mean\n",
        "        else:\n",
        "            X_hat = X / torch.sqrt(moving_var + eps)\n",
        "    else:\n",
        "        assert len(X.shape) in (2, 4)\n",
        "        if len(X.shape) == 2:\n",
        "            # When using a fully connected layer, calculate the mean and\n",
        "            # variance on the feature dimension\n",
        "            mean = X.mean(dim=0)\n",
        "            var = ((X - mean) ** 2).mean(dim=0)\n",
        "        else:\n",
        "            # When using a two-dimensional convolutional layer, calculate the\n",
        "            # mean and variance on the channel dimension (axis=1). Here we\n",
        "            # need to maintain the shape of X, so that the broadcasting\n",
        "            # operation can be carried out later\n",
        "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
        "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
        "        # In training mode, the current mean and variance are used\n",
        "        if mean_flag:\n",
        "            X_hat = X - mean\n",
        "        else:\n",
        "            X_hat = X / torch.sqrt(moving_var + eps)\n",
        "        # Update the mean and variance using moving average\n",
        "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
        "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_mean.data, moving_var.data\n",
        "\n",
        "class LiteBatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims, mean_flag=True):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0 and\n",
        "        # 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "        self.mean_flag = mean_flag\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
        "        # the device where X is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.1, mean_flag=self.mean_flag)\n",
        "        return Y\n",
        "\n",
        "\n",
        "class LiteBNLeNetScratch(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), LiteBatchNorm(6, num_dims=4, mean_flag=mean_flag),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), LiteBatchNorm(16, num_dims=4, mean_flag=mean_flag),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120),\n",
        "            LiteBatchNorm(120, num_dims=2, mean_flag=mean_flag), nn.Sigmoid(), nn.LazyLinear(84),\n",
        "            LiteBatchNorm(84, num_dims=2, mean_flag=mean_flag), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))\n",
        "model = LiteBNLeNetScratch(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "model.accuracy(y_hat,y).item()\n",
        "\n",
        "\n",
        "model = LiteBNLeNetScratch(lr=0.1,mean_flag=False)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "X,y = next(iter(data.get_dataloader(False)))\n",
        "X = X.to('cuda')\n",
        "y = y.to('cuda')\n",
        "y_hat = model(X)\n",
        "model.accuracy(y_hat,y).item()\n"
      ],
      "metadata": {
        "id": "xcx7eppOu0QY"
      },
      "id": "xcx7eppOu0QY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. class FixedBatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims, beta=None, gamma=None):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = torch.ones(shape) if gamma is None else gamma\n",
        "        self.beta = torch.zeros(shape) if beta is None else beta\n",
        "        # The variables that are not model parameters are initialized to 0 and\n",
        "        # 1\n",
        "        self.moving_mean = torch.zeros(shape)\n",
        "        self.moving_var = torch.ones(shape)\n",
        "        self.mean_flag = mean_flag\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
        "        # the device where X is located\n",
        "        if self.moving_mean.device != X.device:\n",
        "            self.moving_mean = self.moving_mean.to(X.device)\n",
        "            self.moving_var = self.moving_var.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var\n",
        "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
        "            X, self.gamma, self.beta, self.moving_mean,\n",
        "            self.moving_var, eps=1e-5, momentum=0.1, mean_flag=self.mean_flag)\n",
        "        return Y\n",
        "\n",
        "\n",
        "class FixedBNLeNetScratch(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10, beta=None, gamma=None):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), FixedBatchNorm(6, num_dims=4, beta=beta, gamma=gamma),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), FixedBatchNorm(16, num_dims=4, beta=beta, gamma=gamma),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120),\n",
        "            FixedBatchNorm(120, num_dims=2, beta=beta, gamma=gamma), nn.Sigmoid(), nn.LazyLinear(84),\n",
        "            FixedBatchNorm(84, num_dims=2, beta=beta, gamma=gamma), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))\n"
      ],
      "metadata": {
        "id": "VMq40irKu3O-"
      },
      "id": "VMq40irKu3O-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Dropout and batch normalization are distinct regularization techniques used in neural networks, both aimed at reducing overfitting but functioning in different manners. Dropout randomly deactivates certain units (neurons) during training, while batch normalization normalizes the activations within each layer. Because they have different roles, substituting one for the other may not produce equivalent outcomes."
      ],
      "metadata": {
        "id": "GHgV350GuzZ6"
      },
      "id": "GHgV350GuzZ6"
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "def gen_sort_cdf(data):\n",
        "    sorts = []\n",
        "    cdfs = []\n",
        "    for i in range(data.shape[1]):\n",
        "        sort = np.sort(data[:, i])\n",
        "        cdf = np.arange(1, len(sort) + 1) / len(sort)\n",
        "        sorts.append(torch.tensor(sort).reshape(-1,1))\n",
        "        cdfs.append(torch.tensor(cdf).reshape(-1,1))\n",
        "    return torch.cat(sorts, dim=1), torch.cat(cdfs, dim=1)\n",
        "\n",
        "def pit_col(sorted_data, cdf_values, data):\n",
        "    # sorted_data = np.sort(org_data)\n",
        "    # cdf_values = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
        "    transformed_data = np.interp(data, sorted_data, cdf_values)\n",
        "    return\n",
        "\n",
        "def pit(sorted_data, cdf_values, data):\n",
        "    return torch.cat([pit_col(sorted_data[:,i], cdf_values[:,i], data[:, i]) for i in range(data.shape[1])], dim=1)\n",
        "\n",
        "def batch_pit_norm(X, gamma, beta, moving_sorted, moving_cdf, momentum):\n",
        "    # Use is_grad_enabled to determine whether we are in training mode\n",
        "    assert len(X.shape) in (2, 4)\n",
        "    shape  = X.shape\n",
        "    if len(shape) == 4:\n",
        "        X = torch.transpose(X,0,1).reshape(shape[1],-1)\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        X_hat = pit(moving_sorted, moving_cdf, cdfs, X)\n",
        "    else:\n",
        "        sorts, cdfs = gen_sort_cdf(data)\n",
        "        X_hat = pit(sorts, cdfs, X)\n",
        "        moving_sorted = (1.0 - momentum) * moving_sorted + momentum * sorts\n",
        "        moving_cdf = (1.0 - momentum) * moving_cdf + momentum * cdfs\n",
        "    X_hat = X.reshape(shape)\n",
        "        # Update the mean and variance using moving average\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_sorted, moving_cdf\n",
        "\n",
        "class PitBatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0\n",
        "        self.moving_sorted = torch.zeros(shape)\n",
        "        self.moving_cdf = torch.zeros(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
        "        # the device where X is located\n",
        "        if self.moving_sorted.device != X.device:\n",
        "            self.moving_sorted = self.moving_sorted.to(X.device)\n",
        "            self.moving_cdf = self.moving_cdf.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var\n",
        "        Y, self.moving_sorted, self.moving_cdf = batch_pit_norm(\n",
        "            X, self.gamma, self.beta, self.moving_sorted,\n",
        "            self.moving_cdf, momentum=0.1)\n",
        "        return Y\n",
        "\n",
        "\n",
        "class PitBNLeNetScratch(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), PitBatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), PitBatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120),\n",
        "            PitBatchNorm(6, num_dims=4), nn.Sigmoid(), nn.LazyLinear(84),\n",
        "            PitBatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))\n",
        "# model = PitBNLeNetScratch(lr=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "-IVxRENyu-pw"
      },
      "id": "-IVxRENyu-pw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a full-rank covariance estimate in place of the standard normalization transform (mean and variance) in batch normalization is an intriguing concept, but it's generally not advisable for several reasons:\n",
        "\n",
        "1. **Computational Complexity**: Calculating the full-rank covariance matrix is more computationally intensive than determining the mean and variance. The covariance matrix has a quadratic complexity in relation to input dimensions, whereas mean and variance computations are linear. This added complexity can significantly slow down training.\n",
        "\n",
        "2. **Dimension Mismatch**: Batch normalization is applied independently to each channel (feature) in the input. When estimating the covariance matrix, one typically needs to account for interactions between different channels, potentially resulting in a higher-dimensional covariance matrix. This could complicate the normalization process.\n",
        "\n",
        "3. **Instability**: Calculating the full-rank covariance matrix with small batch sizes or high-dimensional data can result in numerical instability and poorly conditioned covariance matrices. Regularization techniques may be required to ensure stability.\n",
        "\n",
        "4. **Overfitting**: Introducing a full-rank covariance matrix could add extra learnable parameters, increasing the risk of overfitting, especially if the network is relatively small or the dataset lacks diversity.\n",
        "\n",
        "5. **Loss of Orthogonality**: A key advantage of batch normalization is maintaining orthogonality between weight updates and gradient updates during backpropagation. The inclusion of a full-rank covariance matrix might disrupt this orthogonality, leading to slower convergence or training instability.\n",
        "\n",
        "6. **Normalization Properties**: Batch normalization is intended to independently normalize each channel’s activations. A full-rank covariance estimate may introduce interdependencies among channels, which could undermine the normalization's effectiveness.\n",
        "\n",
        "7. **Lack of Empirical Support**: The conventional approach to batch normalization using mean and variance has been empirically validated across various network architectures and tasks. In contrast, there is limited evidence supporting the use of a full-rank covariance estimate.\n",
        "\n",
        "In summary, although the notion of employing a full-rank covariance matrix in batch normalization is compelling, it is not commonly practiced due to potential issues related to computational complexity, instability, and the misalignment with batch normalization's design principles. The standard normalization techniques (mean and variance) have proven effective in stabilizing and accelerating neural network training."
      ],
      "metadata": {
        "id": "Y6OYI2xcvHS4"
      },
      "id": "Y6OYI2xcvHS4"
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_frcov_norm(X, gamma, beta, moving_cov_matrix, momentum):\n",
        "    # Use is_grad_enabled to determine whether we are in training mode\n",
        "    assert len(X.shape) in (2, 4)\n",
        "    shape  = X.shape\n",
        "    if len(shape) == 4:\n",
        "        X = torch.transpose(X,0,1).reshape(shape[1],-1)\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        eigenvalues, eigenvectors = torch.linalg.eig(moving_cov_matrix)\n",
        "        X_hat = X @ eigenvectors.type(torch.float32)\n",
        "    else:\n",
        "        centered_data = X - X.mean(dim=0)\n",
        "        cov_matrix = (centered_data.conj().T @ centered_data) / (X.shape[0] - 1)\n",
        "        eigenvalues, eigenvectors = torch.linalg.eig(cov_matrix)\n",
        "        X_hat = X @ eigenvectors.type(torch.float32)\n",
        "        moving_cov_matrix = (1.0 - momentum) * moving_cov_matrix + momentum * cov_matrix\n",
        "    X_hat = X.reshape(shape)\n",
        "        # Update the mean and variance using moving average\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_cov_matrix\n",
        "\n",
        "class FrcovBatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0\n",
        "        self.moving_cov_matrix = torch.zeros(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
        "        # the device where X is located\n",
        "        if self.moving_cov_matrix.device != X.device:\n",
        "            self.moving_cov_matrix = self.moving_cov_matrix.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var\n",
        "        Y, self.moving_cov_matrix = batch_frcov_norm(\n",
        "            X, self.gamma, self.beta, self.moving_cov_matrix,\n",
        "            momentum=0.1)\n",
        "        return Y\n",
        "\n",
        "\n",
        "class FrcovBNLeNetScratch(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), FrcovBatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), FrcovBatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120),\n",
        "            FrcovBatchNorm(6, num_dims=4), nn.Sigmoid(), nn.LazyLinear(84),\n",
        "            FrcovBatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))\n",
        "# model = FrcovBNLeNetScratch(lr=0.1)\n"
      ],
      "metadata": {
        "id": "YBVkZfUovUDj"
      },
      "id": "YBVkZfUovUDj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_bdcov_norm(X, gamma, beta, moving_cov_matrix, momentum):\n",
        "    # Use is_grad_enabled to determine whether we are in training mode\n",
        "    assert len(X.shape) in (2, 4)\n",
        "    shape  = X.shape\n",
        "    if len(shape) == 4:\n",
        "        X = torch.transpose(X,0,1).reshape(shape[1],-1)\n",
        "    if not torch.is_grad_enabled():\n",
        "        # In prediction mode, use mean and variance obtained by moving average\n",
        "        diagonal_matrix = torch.diag_embed(moving_cov_matrix)\n",
        "        block_diagonal_matrix = torch.sum(diagonal_matrix, dim=0)\n",
        "        X_hat = X @ block_diagonal_matrix\n",
        "    else:\n",
        "        centered_data = X - X.mean(dim=0)\n",
        "        cov_matrix = (centered_data.conj().T @ centered_data) / (X.shape[0] - 1)\n",
        "        diagonal_matrix = torch.diag_embed(moving_cov_matrix)\n",
        "        block_diagonal_matrix = torch.sum(diagonal_matrix, dim=0)\n",
        "        X_hat = X @ block_diagonal_matrix\n",
        "        moving_cov_matrix = (1.0 - momentum) * moving_cov_matrix + momentum * cov_matrix\n",
        "    X_hat = X.reshape(shape)\n",
        "        # Update the mean and variance using moving average\n",
        "    Y = gamma * X_hat + beta  # Scale and shift\n",
        "    return Y, moving_cov_matrix\n",
        "\n",
        "class BdcovBatchNorm(nn.Module):\n",
        "    # num_features: the number of outputs for a fully connected layer or the\n",
        "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
        "    # fully connected layer and 4 for a convolutional layer\n",
        "    def __init__(self, num_features, num_dims):\n",
        "        super().__init__()\n",
        "        if num_dims == 2:\n",
        "            shape = (1, num_features)\n",
        "        else:\n",
        "            shape = (1, num_features, 1, 1)\n",
        "        # The scale parameter and the shift parameter (model parameters) are\n",
        "        # initialized to 1 and 0, respectively\n",
        "        self.gamma = nn.Parameter(torch.ones(shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(shape))\n",
        "        # The variables that are not model parameters are initialized to 0\n",
        "        self.moving_cov_matrix = torch.zeros(shape)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
        "        # the device where X is located\n",
        "        if self.moving_cov_matrix.device != X.device:\n",
        "            self.moving_cov_matrix = self.moving_cov_matrix.to(X.device)\n",
        "        # Save the updated moving_mean and moving_var\n",
        "        Y, self.moving_cov_matrix = batch_bdcov_norm(\n",
        "            X, self.gamma, self.beta, self.moving_cov_matrix,\n",
        "            momentum=0.1)\n",
        "        return Y\n",
        "\n",
        "\n",
        "class BdcovBNLeNetScratch(d2l.Classifier):\n",
        "    def __init__(self, lr=0.1, num_classes=10, mean_flag=True):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5), BdcovBatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), BdcovBatchNorm(6, num_dims=4),\n",
        "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(), nn.LazyLinear(120),\n",
        "            BdcovBatchNorm(6, num_dims=4), nn.Sigmoid(), nn.LazyLinear(84),\n",
        "            BdcovBatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))\n"
      ],
      "metadata": {
        "id": "lAXO_m3gvU8s"
      },
      "id": "lAXO_m3gvU8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, sparsification compression can serve as a form of regularization in machine learning models. Sparsification involves converting certain weights or parameters in a model to zero, resulting in a sparse representation. This process can regularize the model's learning and help prevent overfitting.\n",
        "\n",
        "Here’s how sparsification compression functions as a regularizer:\n",
        "\n",
        "1. **Reduced Model Complexity**: By reducing the number of active parameters, sparsification simplifies the model representation. This helps prevent the model from fitting noise in the training data and allows it to focus on the most relevant features.\n",
        "\n",
        "2. **Prevention of Overfitting**: A sparse model, having fewer degrees of freedom, is less likely to overfit the training data. Overfitting occurs when a model becomes overly complex, capturing noise instead of the underlying patterns. Sparsification limits this complexity and helps the model generalize better to unseen data.\n",
        "\n",
        "3. **Improved Generalization**: Regularization techniques like sparsification typically enhance generalization performance. By directing the model’s attention toward the most informative features, it becomes more robust and performs better on new, unseen data.\n",
        "\n",
        "4. **Interpretability**: Sparse models tend to be more interpretable because they emphasize the most influential features. This can provide insights into the decision-making process of the model, facilitating understanding and debugging.\n",
        "\n",
        "5. **Efficiency**: Sparse models are computationally more efficient, requiring fewer computations during inference. This efficiency is particularly advantageous for deploying models in resource-constrained environments.\n",
        "\n",
        "It’s essential to recognize that while sparsification compression offers regularization benefits, the extent of regularization depends on the level of sparsity and the method employed. Various techniques, such as L1 regularization (lasso), dropout, or specific neural network pruning methods, can induce sparsity and act as regularization across different contexts. However, the exact regularization effect may vary based on the specific problem, dataset, and model architecture."
      ],
      "metadata": {
        "id": "IeoTjXnyvdgk"
      },
      "id": "IeoTjXnyvdgk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there are various types of projections and transforms utilized in different mathematical and computational contexts. These projections often help achieve specific properties, structures, or constraints on data or mathematical objects. Here are several examples:\n",
        "\n",
        "1. **Convex Cone Projection**: Convex cones are sets of vectors closed under linear combinations with non-negative coefficients. Projecting onto a convex cone involves identifying the point within the cone that is closest to a given vector. This type of projection is commonly used in optimization problems that require solutions to meet certain constraints.\n",
        "\n",
        "2. **Symmetry Group-Specific Transforms**: In some cases, you may want to transform data or objects to respect particular symmetries. For instance, in crystallography, Fourier transforms can help reveal the symmetry of a crystal lattice. In image processing, transforms may be employed that maintain rotational or translational symmetries.\n",
        "\n",
        "3. **Orthogonal Projection**: This involves finding the nearest point in a subspace to a given vector. Orthogonal projection is frequently applied in linear algebra and optimization, where the goal is to find the best approximation of a vector within a specific subspace.\n",
        "\n",
        "4. **Quantization**: This projection-like operation maps continuous values to a discrete set of values. It is often used in signal processing and data compression to reduce the number of possible values while minimizing information loss.\n",
        "\n",
        "5. **Manifold Embedding**: Manifold learning techniques aim to map high-dimensional data into lower-dimensional spaces while preserving certain properties or structures. Techniques such as Isomap, Locally Linear Embedding (LLE), and t-Distributed Stochastic Neighbor Embedding (t-SNE) are examples of manifold embedding methods.\n",
        "\n",
        "6. **Orthogonal Procrustes Problem**: In linear algebra, this problem seeks to find an orthogonal transformation (either rotation or reflection) that best aligns two sets of points. It is commonly used in computer graphics, shape analysis, and alignment tasks.\n",
        "\n",
        "These examples illustrate just a few of the many projections and transforms employed across various fields. The choice of a specific projection or transform depends on the problem and the particular properties or constraints one aims to achieve."
      ],
      "metadata": {
        "id": "JiNHT3fGvepw"
      },
      "id": "JiNHT3fGvepw"
    },
    {
      "cell_type": "markdown",
      "id": "3c9f5a14",
      "metadata": {
        "id": "3c9f5a14"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/84)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17807647",
      "metadata": {
        "id": "17807647"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11b4915",
      "metadata": {
        "id": "e11b4915"
      },
      "source": [
        "# Residual Networks (ResNet) and ResNeXt\n",
        ":label:`sec_resnet`\n",
        "\n",
        "As we design ever deeper networks it becomes imperative to understand how adding layers can increase the complexity and expressiveness of the network.\n",
        "Even more important is the ability to design networks where adding layers makes networks strictly more expressive rather than just different.\n",
        "To make some progress we need a bit of mathematics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e5d075",
      "metadata": {
        "id": "d6e5d075"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f46d0878",
      "metadata": {
        "id": "f46d0878"
      },
      "source": [
        "## Function Classes\n",
        "\n",
        "Consider $\\mathcal{F}$, the class of functions that a specific network architecture (together with learning rates and other hyperparameter settings) can reach.\n",
        "That is, for all $f \\in \\mathcal{F}$ there exists some set of parameters (e.g., weights and biases) that can be obtained through training on a suitable dataset.\n",
        "Let's assume that $f^*$ is the \"truth\" function that we really would like to find.\n",
        "If it is in $\\mathcal{F}$, we are in good shape but typically we will not be quite so lucky.\n",
        "Instead, we will try to find some $f^*_\\mathcal{F}$ which is our best bet within $\\mathcal{F}$.\n",
        "For instance,\n",
        "given a dataset with features $\\mathbf{X}$\n",
        "and labels $\\mathbf{y}$,\n",
        "we might try finding it by solving the following optimization problem:\n",
        "\n",
        "$$f^*_\\mathcal{F} \\stackrel{\\textrm{def}}{=} \\mathop{\\mathrm{argmin}}_f L(\\mathbf{X}, \\mathbf{y}, f) \\textrm{ subject to } f \\in \\mathcal{F}.$$\n",
        "\n",
        "We know that regularization :cite:`tikhonov1977solutions,morozov2012methods` may control complexity of $\\mathcal{F}$\n",
        "and achieve consistency, so a larger size of training data\n",
        "generally leads to better $f^*_\\mathcal{F}$.\n",
        "It is only reasonable to assume that if we design a different and more powerful architecture $\\mathcal{F}'$ we should arrive at a better outcome. In other words, we would expect that $f^*_{\\mathcal{F}'}$ is \"better\" than $f^*_{\\mathcal{F}}$. However, if $\\mathcal{F} \\not\\subseteq \\mathcal{F}'$ there is no guarantee that this should even happen. In fact, $f^*_{\\mathcal{F}'}$ might well be worse.\n",
        "As illustrated by :numref:`fig_functionclasses`,\n",
        "for non-nested function classes, a larger function class does not always move closer to the \"truth\" function $f^*$. For instance,\n",
        "on the left of :numref:`fig_functionclasses`,\n",
        "though $\\mathcal{F}_3$ is closer to $f^*$ than $\\mathcal{F}_1$, $\\mathcal{F}_6$ moves away and there is no guarantee that further increasing the complexity can reduce the distance from $f^*$.\n",
        "With nested function classes\n",
        "where $\\mathcal{F}_1 \\subseteq \\cdots \\subseteq \\mathcal{F}_6$\n",
        "on the right of :numref:`fig_functionclasses`,\n",
        "we can avoid the aforementioned issue from the non-nested function classes.\n",
        "\n",
        "\n",
        "![For non-nested function classes, a larger (indicated by area) function class does not guarantee we will get closer to the \"truth\" function ($\\mathit{f}^*$). This does not happen in nested function classes.](http://d2l.ai/_images/functionclasses.svg)\n",
        ":label:`fig_functionclasses`\n",
        "\n",
        "Thus,\n",
        "only if larger function classes contain the smaller ones are we guaranteed that increasing them strictly increases the expressive power of the network.\n",
        "For deep neural networks,\n",
        "if we can\n",
        "train the newly-added layer into an identity function $f(\\mathbf{x}) = \\mathbf{x}$, the new model will be as effective as the original model. As the new model may get a better solution to fit the training dataset, the added layer might make it easier to reduce training errors.\n",
        "\n",
        "This is the question that :citet:`He.Zhang.Ren.ea.2016` considered when working on very deep computer vision models.\n",
        "At the heart of their proposed *residual network* (*ResNet*) is the idea that every additional layer should\n",
        "more easily\n",
        "contain the identity function as one of its elements.\n",
        "These considerations are rather profound but they led to a surprisingly simple\n",
        "solution, a *residual block*.\n",
        "With it, ResNet won the ImageNet Large Scale Visual Recognition Challenge in 2015. The design had a profound influence on how to\n",
        "build deep neural networks. For instance, residual blocks have been added to recurrent networks :cite:`prakash2016neural,kim2017residual`. Likewise, Transformers :cite:`Vaswani.Shazeer.Parmar.ea.2017` use them to stack many layers of networks efficiently. It is also used in graph neural networks :cite:`Kipf.Welling.2016` and, as a basic concept, it has been used extensively in computer vision :cite:`Redmon.Farhadi.2018,Ren.He.Girshick.ea.2015`.\n",
        "Note that residual networks are predated by highway networks :cite:`srivastava2015highway` that share some of the motivation, albeit without the elegant parametrization around the identity function.\n",
        "\n",
        "\n",
        "## (**Residual Blocks**)\n",
        ":label:`subsec_residual-blks`\n",
        "\n",
        "Let's focus on a local part of a neural network, as depicted in :numref:`fig_residual_block`. Denote the input by $\\mathbf{x}$.\n",
        "We assume that $f(\\mathbf{x})$, the desired underlying mapping we want to obtain by learning, is to be used as input to the activation function on the top.\n",
        "On the left,\n",
        "the portion within the dotted-line box\n",
        "must directly learn $f(\\mathbf{x})$.\n",
        "On the right,\n",
        "the portion within the dotted-line box\n",
        "needs to\n",
        "learn the *residual mapping* $g(\\mathbf{x}) = f(\\mathbf{x}) - \\mathbf{x}$,\n",
        "which is how the residual block derives its name.\n",
        "If the identity mapping $f(\\mathbf{x}) = \\mathbf{x}$ is the desired underlying mapping,\n",
        "the residual mapping amounts to $g(\\mathbf{x}) = 0$ and it is thus easier to learn:\n",
        "we only need to push the weights and biases\n",
        "of the\n",
        "upper weight layer (e.g., fully connected layer and convolutional layer)\n",
        "within the dotted-line box\n",
        "to zero.\n",
        "The right figure illustrates the *residual block* of ResNet,\n",
        "where the solid line carrying the layer input\n",
        "$\\mathbf{x}$ to the addition operator\n",
        "is called a *residual connection* (or *shortcut connection*).\n",
        "With residual blocks, inputs can\n",
        "forward propagate faster through the residual connections across layers.\n",
        "In fact,\n",
        "the residual block\n",
        "can be thought of as\n",
        "a special case of the multi-branch Inception block:\n",
        "it has two branches\n",
        "one of which is the identity mapping.\n",
        "\n",
        "![In a regular block (left), the portion within the dotted-line box must directly learn the mapping $\\mathit{f}(\\mathbf{x})$. In a residual block (right), the portion within the dotted-line box needs to learn the residual mapping $\\mathit{g}(\\mathbf{x}) = \\mathit{f}(\\mathbf{x}) - \\mathbf{x}$, making the identity mapping $\\mathit{f}(\\mathbf{x}) = \\mathbf{x}$ easier to learn.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/residual-block.svg?raw=1)\n",
        ":label:`fig_residual_block`\n",
        "\n",
        "\n",
        "ResNet has VGG's full $3\\times 3$ convolutional layer design. The residual block has two $3\\times 3$ convolutional layers with the same number of output channels. Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. Then, we skip these two convolution operations and add the input directly before the final ReLU activation function.\n",
        "This kind of design requires that the output of the two convolutional layers has to be of the same shape as the input, so that they can be added together. If we want to change the number of channels, we need to introduce an additional $1\\times 1$ convolutional layer to transform the input into the desired shape for the addition operation. Let's have a look at the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35fa7497",
      "metadata": {
        "id": "35fa7497"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):  #@save\n",
        "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
        "                                   stride=strides)\n",
        "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5a254f6",
      "metadata": {
        "id": "d5a254f6"
      },
      "source": [
        "This code generates two types of networks: one where we add the input to the output before applying the ReLU nonlinearity whenever `use_1x1conv=False`; and one where we adjust channels and resolution by means of a $1 \\times 1$ convolution before adding. :numref:`fig_resnet_block` illustrates this.\n",
        "\n",
        "![ResNet block with and without $1 \\times 1$ convolution, which transforms the input into the desired shape for the addition operation.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/resnet-block.svg?raw=1)\n",
        ":label:`fig_resnet_block`\n",
        "\n",
        "Now let's look at [**a situation where the input and output are of the same shape**], where $1 \\times 1$ convolution is not needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2057b8bc",
      "metadata": {
        "id": "2057b8bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5072af29-d3b2-4f14-8429-89939ccfc146"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "blk = Residual(3)\n",
        "X = torch.randn(4, 3, 6, 6)\n",
        "blk(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81b3b8b1",
      "metadata": {
        "id": "81b3b8b1"
      },
      "source": [
        "We also have the option to [**halve the output height and width while increasing the number of output channels**].\n",
        "In this case we use $1 \\times 1$ convolutions via `use_1x1conv=True`. This comes in handy at the beginning of each ResNet block to reduce the spatial dimensionality via `strides=2`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341c1c55",
      "metadata": {
        "id": "341c1c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a707ee89-d283-4243-841f-b43083e3d739"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 6, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "blk = Residual(6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "735745a2",
      "metadata": {
        "id": "735745a2"
      },
      "source": [
        "## [**ResNet Model**]\n",
        "\n",
        "The first two layers of ResNet are the same as those of the GoogLeNet we described before: the $7\\times 7$ convolutional layer with 64 output channels and a stride of 2 is followed by the $3\\times 3$ max-pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "393dd8de",
      "metadata": {
        "id": "393dd8de"
      },
      "outputs": [],
      "source": [
        "class ResNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537173c6",
      "metadata": {
        "id": "537173c6"
      },
      "source": [
        "GoogLeNet uses four modules made up of Inception blocks.\n",
        "However, ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels.\n",
        "The number of channels in the first module is the same as the number of input channels. Since a max-pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d92b69c",
      "metadata": {
        "id": "4d92b69c"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(ResNet)\n",
        "def block(self, num_residuals, num_channels, first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels))\n",
        "    return nn.Sequential(*blk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582781fd",
      "metadata": {
        "id": "582781fd"
      },
      "source": [
        "Then, we add all the modules to ResNet. Here, two residual blocks are used for each module. Lastly, just like GoogLeNet, we add a global average pooling layer, followed by the fully connected layer output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0019ee3f",
      "metadata": {
        "id": "0019ee3f"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(ResNet)\n",
        "def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1())\n",
        "    for i, b in enumerate(arch):\n",
        "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "    self.net.add_module('last', nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a9c2ac",
      "metadata": {
        "id": "91a9c2ac"
      },
      "source": [
        "There are four convolutional layers in each module (excluding the $1\\times 1$ convolutional layer). Together with the first $7\\times 7$ convolutional layer and the final fully connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18.\n",
        "By configuring different numbers of channels and residual blocks in the module, we can create different ResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture of ResNet is similar to that of GoogLeNet, ResNet's structure is simpler and easier to modify. All these factors have resulted in the rapid and widespread use of ResNet. :numref:`fig_resnet18` depicts the full ResNet-18.\n",
        "\n",
        "![The ResNet-18 architecture.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/resnet18-90.svg?raw=1)\n",
        ":label:`fig_resnet18`\n",
        "\n",
        "Before training ResNet, let's [**observe how the input shape changes across different modules in ResNet**]. As in all the previous architectures, the resolution decreases while the number of channels increases up until the point where a global average pooling layer aggregates all features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e60e34",
      "metadata": {
        "id": "61e60e34"
      },
      "outputs": [],
      "source": [
        "class ResNet18(ResNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "                       lr, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f153f6ed",
      "metadata": {
        "id": "f153f6ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd993655-1459-401e-b846-9921c8c71b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
            "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
            "Sequential output shape:\t torch.Size([1, 128, 12, 12])\n",
            "Sequential output shape:\t torch.Size([1, 256, 6, 6])\n",
            "Sequential output shape:\t torch.Size([1, 512, 3, 3])\n",
            "Sequential output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "ResNet18().layer_summary((1, 1, 96, 96))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c04c33c9",
      "metadata": {
        "id": "c04c33c9"
      },
      "source": [
        "## [**Training**]\n",
        "\n",
        "We train ResNet on the Fashion-MNIST dataset, just like before. ResNet is quite a powerful and flexible architecture. The plot capturing training and validation loss illustrates a significant gap between both graphs, with the training loss being considerably lower. For a network of this flexibility, more training data would offer distinct benefit in closing the gap and improving accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b87bb9",
      "metadata": {
        "id": "61b87bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "c7ddd4da-cc51-49ab-952e-e73600751db0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T08:54:09.002131</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 238.965625 183.35625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"ma01fe11b41\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma01fe11b41\" x=\"30.103125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#ma01fe11b41\" x=\"69.163125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#ma01fe11b41\" x=\"108.223125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#ma01fe11b41\" x=\"147.283125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#ma01fe11b41\" x=\"186.343125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#ma01fe11b41\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m2c9bd83238\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m2c9bd83238\" x=\"30.103125\" y=\"143.323058\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 147.122276) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m2c9bd83238\" x=\"30.103125\" y=\"114.753509\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 118.552728) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m2c9bd83238\" x=\"30.103125\" y=\"86.18396\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 89.983179) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m2c9bd83238\" x=\"30.103125\" y=\"57.614412\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 61.413631) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m2c9bd83238\" x=\"30.103125\" y=\"29.044863\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 32.844082) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 34.954394 63.578796 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 49.633125 91.674427 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_16\"/>\n   <g id=\"line2d_17\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 49.633125 91.674427 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 49.633125 18.995231 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 49.633125 91.674427 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 49.633125 18.995231 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 49.633125 91.674427 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 49.633125 18.995231 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 49.633125 18.995231 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \nL 190.861259 138.103979 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \nL 190.861259 138.103979 \nL 200.605438 136.74728 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \nL 190.861259 138.103979 \nL 200.605438 136.74728 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \nL 205.873125 97.26163 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \nL 190.861259 138.103979 \nL 200.605438 136.74728 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \nL 205.873125 97.26163 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \nL 205.873125 13.937923 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \nL 190.861259 138.103979 \nL 200.605438 136.74728 \nL 210.349618 139.5 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \nL 205.873125 97.26163 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \nL 205.873125 13.937923 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \nL 190.861259 138.103979 \nL 200.605438 136.74728 \nL 210.349618 139.5 \nL 220.093797 138.402078 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \nL 205.873125 97.26163 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \nL 205.873125 13.937923 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \nL 190.861259 138.103979 \nL 200.605438 136.74728 \nL 210.349618 139.5 \nL 220.093797 138.402078 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \nL 205.873125 97.26163 \nL 225.403125 19.106245 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \nL 205.873125 13.937923 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 34.954394 63.578796 \nL 44.698573 93.65515 \nL 54.442752 103.542983 \nL 64.186931 106.112223 \nL 73.93111 112.796834 \nL 83.675289 112.605124 \nL 93.419468 119.296213 \nL 103.163647 118.390967 \nL 112.907826 124.542056 \nL 122.652006 122.65106 \nL 132.396185 129.198806 \nL 142.140364 126.952664 \nL 151.884543 132.2625 \nL 161.628722 131.588339 \nL 171.372901 135.600339 \nL 181.11708 133.917629 \nL 190.861259 138.103979 \nL 200.605438 136.74728 \nL 210.349618 139.5 \nL 220.093797 138.402078 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 49.633125 91.674427 \nL 69.163125 100.616492 \nL 88.693125 97.196568 \nL 108.223125 99.918296 \nL 127.753125 102.218805 \nL 147.283125 98.606372 \nL 166.813125 81.561094 \nL 186.343125 101.923552 \nL 205.873125 97.26163 \nL 225.403125 19.106245 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_127\">\n    <path d=\"M 49.633125 18.995231 \nL 69.163125 16.282932 \nL 88.693125 16.678475 \nL 108.223125 16.028654 \nL 127.753125 15.548351 \nL 147.283125 15.618984 \nL 166.813125 19.772191 \nL 186.343125 13.5 \nL 205.873125 13.937923 \nL 225.403125 26.411673 \n\" clip-path=\"url(#p5d4bc259e4)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 145.8 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 145.8 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 138.8125 100.434375 \nL 218.403125 100.434375 \nQ 220.403125 100.434375 220.403125 98.434375 \nL 220.403125 54.565625 \nQ 220.403125 52.565625 218.403125 52.565625 \nL 138.8125 52.565625 \nQ 136.8125 52.565625 136.8125 54.565625 \nL 136.8125 98.434375 \nQ 136.8125 100.434375 138.8125 100.434375 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_128\">\n     <path d=\"M 140.8125 60.664063 \nL 150.8125 60.664063 \nL 160.8125 60.664063 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- train_loss -->\n     <g transform=\"translate(168.8125 64.164063) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_129\">\n     <path d=\"M 140.8125 75.620313 \nL 150.8125 75.620313 \nL 160.8125 75.620313 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- val_loss -->\n     <g transform=\"translate(168.8125 79.120313) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_130\">\n     <path d=\"M 140.8125 90.576563 \nL 150.8125 90.576563 \nL 160.8125 90.576563 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- val_acc -->\n     <g transform=\"translate(168.8125 94.076563) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5d4bc259e4\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = ResNet18(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc09eb06",
      "metadata": {
        "id": "bc09eb06"
      },
      "source": [
        "## ResNeXt\n",
        ":label:`subsec_resnext`\n",
        "\n",
        "One of the challenges one encounters in the design of ResNet is the trade-off between nonlinearity and dimensionality within a given block. That is, we could add more nonlinearity by increasing the number of layers, or by increasing the width of the convolutions. An alternative strategy is to increase the number of channels that can carry information between blocks. Unfortunately, the latter comes with a quadratic penalty since the computational cost of ingesting $c_\\textrm{i}$ channels and emitting $c_\\textrm{o}$ channels is proportional to $\\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o})$ (see our discussion in :numref:`sec_channels`).\n",
        "\n",
        "We can take some inspiration from the Inception block of :numref:`fig_inception` which has information flowing through the block in separate groups. Applying the idea of multiple independent groups to the ResNet block of :numref:`fig_resnet_block` led to the design of ResNeXt :cite:`Xie.Girshick.Dollar.ea.2017`.\n",
        "Different from the smorgasbord of transformations in Inception,\n",
        "ResNeXt adopts the *same* transformation in all branches,\n",
        "thus minimizing the need for manual tuning of each branch.\n",
        "\n",
        "![The ResNeXt block. The use of grouped convolution with $\\mathit{g}$ groups is $\\mathit{g}$ times faster than a dense convolution. It is a bottleneck residual block when the number of intermediate channels $\\mathit{b}$ is less than $\\mathit{c}$.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/resnext-block.svg?raw=1)\n",
        ":label:`fig_resnext_block`\n",
        "\n",
        "Breaking up a convolution from $c_\\textrm{i}$ to $c_\\textrm{o}$ channels into one of $g$ groups of size $c_\\textrm{i}/g$ generating $g$ outputs of size $c_\\textrm{o}/g$ is called, quite fittingly, a *grouped convolution*. The computational cost (proportionally) is reduced from $\\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o})$ to $\\mathcal{O}(g \\cdot (c_\\textrm{i}/g) \\cdot (c_\\textrm{o}/g)) = \\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o} / g)$, i.e., it is $g$ times faster. Even better, the number of parameters needed to generate the output is also reduced from a $c_\\textrm{i} \\times c_\\textrm{o}$ matrix to $g$ smaller matrices of size $(c_\\textrm{i}/g) \\times (c_\\textrm{o}/g)$, again a $g$ times reduction. In what follows we assume that both $c_\\textrm{i}$ and $c_\\textrm{o}$ are divisible by $g$.\n",
        "\n",
        "The only challenge in this design is that no information is exchanged between the $g$ groups. The ResNeXt block of\n",
        ":numref:`fig_resnext_block` amends this in two ways: the grouped convolution with a $3 \\times 3$ kernel is sandwiched in between two $1 \\times 1$ convolutions. The second one serves double duty in changing the number of channels back. The benefit is that we only pay the $\\mathcal{O}(c \\cdot b)$ cost for $1 \\times 1$ kernels and can make do with an $\\mathcal{O}(b^2 / g)$ cost for $3 \\times 3$ kernels. Similar to the residual block implementation in\n",
        ":numref:`subsec_residual-blks`, the residual connection is replaced (thus generalized) by a $1 \\times 1$ convolution.\n",
        "\n",
        "The right-hand figure in :numref:`fig_resnext_block` provides a much more concise summary of the resulting network block. It will also play a major role in the design of generic modern CNNs in :numref:`sec_cnn-design`. Note that the idea of grouped convolutions dates back to the implementation of AlexNet :cite:`Krizhevsky.Sutskever.Hinton.2012`. When distributing the network across two GPUs with limited memory, the implementation treated each GPU as its own channel with no ill effects.\n",
        "\n",
        "The following implementation of the `ResNeXtBlock` class takes as argument `groups` ($g$), with\n",
        "`bot_channels` ($b$) intermediate (bottleneck) channels. Lastly, when we need to reduce the height and width of the representation, we add a stride of $2$ by setting `use_1x1conv=True, strides=2`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa67ceaf",
      "metadata": {
        "id": "aa67ceaf"
      },
      "outputs": [],
      "source": [
        "class ResNeXtBlock(nn.Module):  #@save\n",
        "    \"\"\"The ResNeXt block.\"\"\"\n",
        "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
        "                 strides=1):\n",
        "        super().__init__()\n",
        "        bot_channels = int(round(num_channels * bot_mul))\n",
        "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
        "                                   stride=strides, padding=1,\n",
        "                                   groups=bot_channels//groups)\n",
        "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "        self.bn3 = nn.LazyBatchNorm2d()\n",
        "        if use_1x1conv:\n",
        "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "            self.bn4 = nn.LazyBatchNorm2d()\n",
        "        else:\n",
        "            self.conv4 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "        if self.conv4:\n",
        "            X = self.bn4(self.conv4(X))\n",
        "        return F.relu(Y + X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03b62951",
      "metadata": {
        "id": "03b62951"
      },
      "source": [
        "Its use is entirely analogous to that of the `ResNetBlock` discussed previously. For instance, when using (`use_1x1conv=False, strides=1`), the input and output are of the same shape. Alternatively, setting `use_1x1conv=True, strides=2` halves the output height and width.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6906e4",
      "metadata": {
        "id": "ec6906e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3468c362-2662-430f-8235-8d362ebd0a71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 32, 96, 96])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "blk = ResNeXtBlock(32, 16, 1)\n",
        "X = torch.randn(4, 32, 96, 96)\n",
        "blk(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a2283f",
      "metadata": {
        "id": "94a2283f"
      },
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "Nested function classes are desirable since they allow us to obtain strictly *more powerful* rather than also subtly *different* function classes when adding capacity. One way of accomplishing this is by letting additional layers to simply pass through the input to the output. Residual connections allow for this. As a consequence, this changes the inductive bias from simple functions being of the form $f(\\mathbf{x}) = 0$ to simple functions looking like $f(\\mathbf{x}) = \\mathbf{x}$.\n",
        "\n",
        "\n",
        "The residual mapping can learn the identity function more easily, such as pushing parameters in the weight layer to zero. We can train an effective *deep* neural network by having residual blocks. Inputs can forward propagate faster through the residual connections across layers. As a consequence, we can thus train much deeper networks. For instance, the original ResNet paper :cite:`He.Zhang.Ren.ea.2016` allowed for up to 152 layers. Another benefit of residual networks is that it allows us to add layers, initialized as the identity function, *during* the training process. After all, the default behavior of a layer is to let the data pass through unchanged. This can accelerate the training of very large networks in some cases.\n",
        "\n",
        "Prior to residual connections,\n",
        "bypassing paths with gating units were introduced\n",
        "to effectively train highway networks with over 100 layers\n",
        ":cite:`srivastava2015highway`.\n",
        "Using identity functions as bypassing paths,\n",
        "ResNet performed remarkably well\n",
        "on multiple computer vision tasks.\n",
        "Residual connections had a major influence on the design of subsequent deep neural networks, of either convolutional or sequential nature.\n",
        "As we will introduce later,\n",
        "the Transformer architecture :cite:`Vaswani.Shazeer.Parmar.ea.2017`\n",
        "adopts residual connections (together with other design choices) and is pervasive\n",
        "in areas as diverse as\n",
        "language, vision, speech, and reinforcement learning.\n",
        "\n",
        "ResNeXt is an example for how the design of convolutional neural networks has evolved over time: by being more frugal with computation and trading it off against the size of the activations (number of channels), it allows for faster and more accurate networks at lower cost. An alternative way of viewing grouped convolutions is to think of a block-diagonal matrix for the convolutional weights. Note that there are quite a few such \"tricks\" that lead to more efficient networks. For instance, ShiftNet :cite:`wu2018shift` mimicks the effects of a $3 \\times 3$ convolution, simply by adding shifted activations to the channels, offering increased function complexity, this time without any computational cost.\n",
        "\n",
        "A common feature of the designs we have discussed so far is that the network design is fairly manual, primarily relying on the ingenuity of the designer to find the \"right\" network hyperparameters. While clearly feasible, it is also very costly in terms of human time and there is no guarantee that the outcome is optimal in any sense. In :numref:`sec_cnn-design` we will discuss a number of strategies for obtaining high quality networks in a more automated fashion. In particular, we will review the notion of *network design spaces* that led to the RegNetX/Y models\n",
        ":cite:`Radosavovic.Kosaraju.Girshick.ea.2020`.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. What are the major differences between the Inception block in :numref:`fig_inception` and the residual block? How do they compare in terms of computation, accuracy, and the classes of functions they can describe?\n",
        "1. Refer to Table 1 in the ResNet paper :cite:`He.Zhang.Ren.ea.2016` to implement different variants of the network.\n",
        "1. For deeper networks, ResNet introduces a \"bottleneck\" architecture to reduce model complexity. Try to implement it.\n",
        "1. In subsequent versions of ResNet, the authors changed the \"convolution, batch normalization, and activation\" structure to the \"batch normalization, activation, and convolution\" structure. Make this improvement yourself. See Figure 1 in :citet:`He.Zhang.Ren.ea.2016*1` for details.\n",
        "1. Why can't we just increase the complexity of functions without bound, even if the function classes are nested?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Inception blocks and residual blocks are two distinct components commonly found in deep neural networks, particularly for image classification. Here’s a comparison of the key differences between the Inception block (or GoogLeNet inception module) and the residual block used in ResNet architectures:\n",
        "\n",
        "### Inception Block (GoogLeNet Inception Module)\n",
        "The Inception block, introduced in the GoogLeNet architecture, is designed to capture features at various scales by employing parallel convolutional layers with different sizes and pooling operations. Its primary goal is to build a rich hierarchy of features by integrating information from multiple receptive fields. The main characteristics of the Inception block include:\n",
        "\n",
        "- **Parallel Convolutions**: It comprises multiple convolutional layers with varying kernel sizes (e.g., 1x1, 3x3, 5x5). These parallel convolutions enable the capture of features at different scales, capturing both fine and coarse details.\n",
        "\n",
        "- **Pooling Operations**: The block also integrates pooling operations, such as max pooling, to reduce spatial dimensions and enhance translational invariance.\n",
        "\n",
        "- **Concatenation**: The outputs from the parallel convolutions and pooling operations are concatenated along the channel dimension, allowing the network to gather a diverse range of features.\n",
        "\n",
        "### Residual Block\n",
        "The residual block, introduced in the ResNet architecture, aims to tackle the vanishing gradient problem and facilitate the training of very deep networks. It introduces skip connections (or shortcut connections) that pass the input of a layer directly to a subsequent layer. The key features of the residual block are:\n",
        "\n",
        "- **Skip Connections**: It utilizes a skip connection that adds the original input (identity) to the output of the convolutional layers, creating a \"residual\" or \"shortcut\" pathway for information to flow through the network.\n",
        "\n",
        "- **Identity Mapping**: The concept behind the residual block is that the model can learn to adjust the weights of the convolutional layers to represent the residual (the difference between the input and output), helping to alleviate vanishing gradient issues.\n",
        "\n",
        "- **Batch Normalization**: Residual blocks typically incorporate batch normalization after each convolutional layer to stabilize and accelerate training.\n",
        "\n",
        "- **Two-Path Learning**: Essentially, the residual block learns a residual transformation, which can be viewed as a combination of \"what to add\" (learned by the convolutional layers) and \"what to keep\" (passed through the skip connection).\n",
        "\n",
        "### Summary\n",
        "In summary, the main difference between the Inception block and the residual block lies in their architectural goals and design principles. The Inception block focuses on capturing features at different scales through parallel operations, while the residual block emphasizes enabling the training of very deep networks by introducing skip connections that facilitate the learning of residual transformations. Both components have significantly advanced the capabilities of deep neural networks for various applications."
      ],
      "metadata": {
        "id": "Y-oNKHYRvm7o"
      },
      "id": "Y-oNKHYRvm7o"
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sys\n",
        "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
        "import d2l\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, convs, conv_1x1_channel, strides=1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i,conv in enumerate(convs):\n",
        "            num_channels, kernel_size, padding = conv\n",
        "            conv_strides = 1 if i != 0 else strides\n",
        "            layers.append(nn.LazyConv2d(num_channels, kernel_size=3, padding=1, stride=conv_strides))\n",
        "            layers.append(nn.LazyBatchNorm2d())\n",
        "            layers.append(nn.ReLU())\n",
        "        self.net = nn.Sequential(*layers[:-1])\n",
        "        self.conv = None\n",
        "        if conv_1x1_channel:\n",
        "            self.conv = nn.LazyConv2d(conv_1x1_channel, kernel_size=1, stride=strides)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = self.net(X)\n",
        "        if self.conv:\n",
        "            X = self.conv(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)\n",
        "\n",
        "class ResNet(d2l.Classifier):\n",
        "    def block(self, num_residuals, convs, conv_1x1_channel, first_block=False):\n",
        "        blk = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                blk.append(Residual(convs, conv_1x1_channel,strides=2))\n",
        "            else:\n",
        "                blk.append(Residual(convs, conv_1x1_channel))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        for i, b in enumerate(arch):\n",
        "            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "        self.net.add_module('last', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "        self.net.apply(d2l.init_cnn)\n",
        "\n",
        "def experiment(data, model):\n",
        "    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "    trainer.fit(model, data)\n",
        "    X,y = next(iter(data.get_dataloader(False)))\n",
        "    X = X.to('cuda')\n",
        "    y = y.to('cuda')\n",
        "    y_hat = model(X)\n",
        "    return model.accuracy(y_hat,y).item()\n",
        "data = d2l.FashionMNIST(batch_size=64, resize=(224, 224))\n",
        "arch18 = [(2,[(64,3,1)]*2,None),(2,[(128,3,1)]*2,None),(2,[(256,3,1)]*2,None),(2,[(512,3,1)]*2,None)]\n",
        "resnet18 = ResNet(arch=arch18, lr=0.01)\n",
        "experiment(data, resnet18)\n",
        "arch34 = [(3,[(64,3,1)]*2,None),(4,[(128,3,1)]*2,None),(6,[(256,3,1)]*2,None),(3,[(512,3,1)]*2,None)]\n",
        "resnet34 = ResNet(arch=arch34, lr=0.01)\n",
        "experiment(data, resnet34)\n"
      ],
      "metadata": {
        "id": "zQ2YjhylvooK"
      },
      "id": "zQ2YjhylvooK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "arch50 = [(3,[(64,1,0),(64,3,1)],256),(4,[(128,1,0),(128,3,1)],512),(6,[(256,1,0),(256,3,1)],1024),(3,[(512,1,0),(512,3,1)],2048)]\n",
        "resnet50 = ResNet(arch=arch50, lr=0.01)\n",
        "experiment(data, resnet50)\n",
        "arch101 = [(3,[(64,1,0),(64,3,1)],256),(4,[(128,1,0),(128,3,1)],512),(23,[(256,1,0),(256,3,1)],1024),(3,[(512,1,0),(512,3,1)],2048)]\n",
        "resnet101 = ResNet(arch=arch101, lr=0.01)\n",
        "experiment(data, resnet101)\n",
        "arch152 = [(3,[(64,1,0),(64,3,1)],256),(8,[(128,1,0),(128,3,1)],512),(36,[(256,1,0),(256,3,1)],1024),(3,[(512,1,0),(512,3,1)],2048)]\n",
        "resnet152 = ResNet(arch=arch152, lr=0.01)\n",
        "experiment(data, resnet152)\n"
      ],
      "metadata": {
        "id": "zhZT7q4Cvtw2"
      },
      "id": "zhZT7q4Cvtw2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "     self.conv = None\n",
        "        if conv_1x1_channel:\n",
        "            self.conv = nn.LazyConv2d(conv_1x1_channel, kernel_size=1, stride=strides)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = self.net(X)\n",
        "        if self.conv:\n",
        "            X = self.conv(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)\n",
        "\n",
        "class SubResNet(d2l.Classifier):\n",
        "    def block(self, num_residuals, convs, conv_1x1_channel, first_block=False):\n",
        "        blk = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                blk.append(SubResidual(convs, conv_1x1_channel,strides=2))\n",
        "            else:\n",
        "                blk.append(SubResidual(convs, conv_1x1_channel))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        for i, b in enumerate(arch):\n",
        "            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "        self.net.add_module('last', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "        self.net.apply(d2l.init_cnn)\n",
        "arch18 = [(2,[(64,3,1)]*2,None),(2,[(128,3,1)]*2,None),(2,[(256,3,1)]*2,None),(2,[(512,3,1)]*2,None)]\n",
        "resnet18 = SubResNet(arch=arch18, lr=0.01)\n",
        "experiment(data, resnet18)\n"
      ],
      "metadata": {
        "id": "OSSzd_k4vwKk"
      },
      "id": "OSSzd_k4vwKk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Increasing the complexity of functions indefinitely, even with nested function classes, can lead to several challenges in machine learning and model training. Here are some key reasons why this approach is impractical:\n",
        "\n",
        "### 1. Overfitting\n",
        "When model complexity is raised without regard for the true underlying patterns in the data, the model can start fitting the noise within the training set. This results in overfitting, where the model performs exceptionally well on training data but poorly on unseen data.\n",
        "\n",
        "### 2. Computational Complexity\n",
        "Complex models with numerous parameters demand more computational resources and time for both training and inference. This can lead to practical difficulties related to training duration, memory consumption, and scalability.\n",
        "\n",
        "### 3. Diminishing Returns\n",
        "Simply increasing model complexity does not guarantee proportionate improvements in performance. There comes a point where additional complexity yields only marginal or diminishing returns in accuracy.\n",
        "\n",
        "### 4. Generalization\n",
        "The primary objective of a model is to generalize effectively to new, unseen data. Excessive complexity can lead to over-specialization on the training data, hindering the model's ability to generalize to new instances.\n",
        "\n",
        "### 5. Regularization Challenges\n",
        "Without appropriate regularization techniques, heightened complexity can worsen overfitting. Regularization is essential for managing model complexity and mitigating overfitting risks.\n",
        "\n",
        "### 6. Interpretability\n",
        "Highly complex models can become hard to interpret, making it difficult to understand their decision-making processes and diagnose potential issues.\n",
        "\n",
        "### 7. Data Efficiency\n",
        "Simpler models are generally more data-efficient. Extremely complex models may require significantly larger amounts of training data to achieve good generalization.\n",
        "\n",
        "### 8. Bias-Variance Trade-off\n",
        "Increasing model complexity affects the balance between bias (leading to underfitting) and variance (leading to overfitting). Striking the right balance is crucial for optimal performance.\n",
        "\n",
        "Instead of pursuing unbounded complexity, it's more beneficial to select model architectures that maintain an appropriate balance between capacity and generalization. Techniques such as regularization, cross-validation, and ensemble methods can enhance model performance without unnecessarily escalating complexity. The ultimate goal is to develop models that can effectively capture the underlying patterns in the data while avoiding overfitting and computational inefficiencies."
      ],
      "metadata": {
        "id": "7jQv81HwvoO6"
      },
      "id": "7jQv81HwvoO6"
    },
    {
      "cell_type": "markdown",
      "id": "af80c347",
      "metadata": {
        "id": "af80c347"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/86)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7c7033",
      "metadata": {
        "id": "8e7c7033"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f122fb8e",
      "metadata": {
        "id": "f122fb8e"
      },
      "source": [
        "# Densely Connected Networks (DenseNet)\n",
        ":label:`sec_densenet`\n",
        "\n",
        "ResNet significantly changed the view of how to parametrize the functions in deep networks. *DenseNet* (dense convolutional network) is to some extent the logical extension of this :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`.\n",
        "DenseNet is characterized by both the connectivity pattern where\n",
        "each layer connects to all the preceding layers\n",
        "and the concatenation operation (rather than the addition operator in ResNet) to preserve and reuse features\n",
        "from earlier layers.\n",
        "To understand how to arrive at it, let's take a small detour to mathematics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898d7836",
      "metadata": {
        "id": "898d7836"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbc7e4b",
      "metadata": {
        "id": "abbc7e4b"
      },
      "source": [
        "## From ResNet to DenseNet\n",
        "\n",
        "Recall the Taylor expansion for functions. At the point $x = 0$ it can be written as\n",
        "\n",
        "$$f(x) = f(0) + x \\cdot \\left[f'(0) + x \\cdot \\left[\\frac{f''(0)}{2!}  + x \\cdot \\left[\\frac{f'''(0)}{3!}  + \\cdots \\right]\\right]\\right].$$\n",
        "\n",
        "\n",
        "The key point is that it decomposes a function into terms of increasingly higher order. In a similar vein, ResNet decomposes functions into\n",
        "\n",
        "$$f(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x}).$$\n",
        "\n",
        "That is, ResNet decomposes $f$ into a simple linear term and a more complex\n",
        "nonlinear one.\n",
        "What if we wanted to capture (not necessarily add) information beyond two terms?\n",
        "One such solution is DenseNet :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`.\n",
        "\n",
        "![The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation. ](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/densenet-block.svg?raw=1)\n",
        ":label:`fig_densenet_block`\n",
        "\n",
        "As shown in :numref:`fig_densenet_block`, the key difference between ResNet and DenseNet is that in the latter case outputs are *concatenated* (denoted by $[,]$) rather than added.\n",
        "As a result, we perform a mapping from $\\mathbf{x}$ to its values after applying an increasingly complex sequence of functions:\n",
        "\n",
        "$$\\mathbf{x} \\to \\left[\n",
        "\\mathbf{x},\n",
        "f_1(\\mathbf{x}),\n",
        "f_2\\left(\\left[\\mathbf{x}, f_1\\left(\\mathbf{x}\\right)\\right]\\right), f_3\\left(\\left[\\mathbf{x}, f_1\\left(\\mathbf{x}\\right), f_2\\left(\\left[\\mathbf{x}, f_1\\left(\\mathbf{x}\\right)\\right]\\right)\\right]\\right), \\ldots\\right].$$\n",
        "\n",
        "In the end, all these functions are combined in MLP to reduce the number of features again. In terms of implementation this is quite simple:\n",
        "rather than adding terms, we concatenate them. The name DenseNet arises from the fact that the dependency graph between variables becomes quite dense. The final layer of such a chain is densely connected to all previous layers. The dense connections are shown in :numref:`fig_densenet`.\n",
        "\n",
        "![Dense connections in DenseNet. Note how the dimensionality increases with depth.](http://d2l.ai/_images/densenet.svg)\n",
        ":label:`fig_densenet`\n",
        "\n",
        "The main components that comprise a DenseNet are *dense blocks* and *transition layers*. The former define how the inputs and outputs are concatenated, while the latter control the number of channels so that it is not too large,\n",
        "since the expansion $\\mathbf{x} \\to \\left[\\mathbf{x}, f_1(\\mathbf{x}),\n",
        "f_2\\left(\\left[\\mathbf{x}, f_1\\left(\\mathbf{x}\\right)\\right]\\right), \\ldots \\right]$ can be quite high-dimensional.\n",
        "\n",
        "\n",
        "## [**Dense Blocks**]\n",
        "\n",
        "DenseNet uses the modified \"batch normalization, activation, and convolution\"\n",
        "structure of ResNet (see the exercise in :numref:`sec_resnet`).\n",
        "First, we implement this convolution block structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d95675e",
      "metadata": {
        "id": "7d95675e"
      },
      "outputs": [],
      "source": [
        "def conv_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=3, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e66d06",
      "metadata": {
        "id": "b4e66d06"
      },
      "source": [
        "A *dense block* consists of multiple convolution blocks, each using the same number of output channels. In the forward propagation, however, we concatenate the input and output of each convolution block on the channel dimension. Lazy evaluation allows us to adjust the dimensionality automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805394e3",
      "metadata": {
        "id": "805394e3"
      },
      "outputs": [],
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, num_convs, num_channels):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layer = []\n",
        "        for i in range(num_convs):\n",
        "            layer.append(conv_block(num_channels))\n",
        "        self.net = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for blk in self.net:\n",
        "            Y = blk(X)\n",
        "            # Concatenate input and output of each block along the channels\n",
        "            X = torch.cat((X, Y), dim=1)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b17dfe15",
      "metadata": {
        "id": "b17dfe15"
      },
      "source": [
        "In the following example,\n",
        "we [**define a `DenseBlock` instance**] with two convolution blocks of 10 output channels.\n",
        "When using an input with three channels, we will get an output with  $3 + 10 + 10=23$ channels. The number of convolution block channels controls the growth in the number of output channels relative to the number of input channels. This is also referred to as the *growth rate*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e369c1ad",
      "metadata": {
        "id": "e369c1ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df71e41d-2630-4aea-cd16-a078ec35fe58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 23, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "blk = DenseBlock(2, 10)\n",
        "X = torch.randn(4, 3, 8, 8)\n",
        "Y = blk(X)\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c8df7b",
      "metadata": {
        "id": "69c8df7b"
      },
      "source": [
        "## [**Transition Layers**]\n",
        "\n",
        "Since each dense block will increase the number of channels, adding too many of them will lead to an excessively complex model. A *transition layer* is used to control the complexity of the model. It reduces the number of channels by using a $1\\times 1$ convolution. Moreover, it halves the height and width via average pooling with a stride of 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6160cc48",
      "metadata": {
        "id": "6160cc48"
      },
      "outputs": [],
      "source": [
        "def transition_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280d3329",
      "metadata": {
        "id": "280d3329"
      },
      "source": [
        "[**Apply a transition layer**] with 10 channels to the output of the dense block in the previous example.  This reduces the number of output channels to 10, and halves the height and width.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0cacfc",
      "metadata": {
        "id": "fc0cacfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83684563-23cf-457e-a709-8c3bc61eca6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 10, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "blk = transition_block(10)\n",
        "blk(Y).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9bc8e5c",
      "metadata": {
        "id": "b9bc8e5c"
      },
      "source": [
        "## [**DenseNet Model**]\n",
        "\n",
        "Next, we will construct a DenseNet model. DenseNet first uses the same single convolutional layer and max-pooling layer as in ResNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e0aaa1",
      "metadata": {
        "id": "79e0aaa1"
      },
      "outputs": [],
      "source": [
        "class DenseNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dccb085",
      "metadata": {
        "id": "6dccb085"
      },
      "source": [
        "Then, similar to the four modules made up of residual blocks that ResNet uses,\n",
        "DenseNet uses four dense blocks.\n",
        "As with ResNet, we can set the number of convolutional layers used in each dense block. Here, we set it to 4, consistent with the ResNet-18 model in :numref:`sec_resnet`. Furthermore, we set the number of channels (i.e., growth rate) for the convolutional layers in the dense block to 32, so 128 channels will be added to each dense block.\n",
        "\n",
        "In ResNet, the height and width are reduced between each module by a residual block with a stride of 2. Here, we use the transition layer to halve the height and width and halve the number of channels. Similar to ResNet, a global pooling layer and a fully connected layer are connected at the end to produce the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58137883",
      "metadata": {
        "id": "58137883"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(DenseNet)\n",
        "def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\n",
        "             lr=0.1, num_classes=10):\n",
        "    super(DenseNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1())\n",
        "    for i, num_convs in enumerate(arch):\n",
        "        self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,\n",
        "                                                          growth_rate))\n",
        "        # The number of output channels in the previous dense block\n",
        "        num_channels += num_convs * growth_rate\n",
        "        # A transition layer that halves the number of channels is added\n",
        "        # between the dense blocks\n",
        "        if i != len(arch) - 1:\n",
        "            num_channels //= 2\n",
        "            self.net.add_module(f'tran_blk{i+1}', transition_block(\n",
        "                num_channels))\n",
        "    self.net.add_module('last', nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04e481f",
      "metadata": {
        "id": "a04e481f"
      },
      "source": [
        "## [**Training**]\n",
        "\n",
        "Since we are using a deeper network here, in this section, we will reduce the input height and width from 224 to 96 to simplify the computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef87c44e",
      "metadata": {
        "id": "ef87c44e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "252ff137-ae1e-4172-b290-976da2e1aa26"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T09:00:21.372262</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 238.965625 183.35625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m4bf31ba27c\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m4bf31ba27c\" x=\"30.103125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m4bf31ba27c\" x=\"69.163125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m4bf31ba27c\" x=\"108.223125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m4bf31ba27c\" x=\"147.283125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m4bf31ba27c\" x=\"186.343125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m4bf31ba27c\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m63c9ca1e24\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m63c9ca1e24\" x=\"30.103125\" y=\"131.491519\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 135.290737) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m63c9ca1e24\" x=\"30.103125\" y=\"98.443566\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 102.242785) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m63c9ca1e24\" x=\"30.103125\" y=\"65.395613\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 69.194832) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m63c9ca1e24\" x=\"30.103125\" y=\"32.347661\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 36.146879) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path d=\"M 34.954394 14.757621 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 49.633125 63.92495 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_15\"/>\n   <g id=\"line2d_16\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 49.633125 63.92495 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 49.633125 34.645199 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 49.633125 63.92495 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 49.633125 34.645199 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 49.633125 63.92495 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 49.633125 34.645199 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 49.633125 34.645199 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \nL 190.861259 137.445865 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \nL 190.861259 137.445865 \nL 200.605438 136.269048 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \nL 190.861259 137.445865 \nL 200.605438 136.269048 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \nL 205.873125 122.873388 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \nL 190.861259 137.445865 \nL 200.605438 136.269048 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \nL 205.873125 122.873388 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \nL 205.873125 13.859501 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \nL 190.861259 137.445865 \nL 200.605438 136.269048 \nL 210.349618 139.5 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \nL 205.873125 122.873388 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \nL 205.873125 13.859501 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \nL 190.861259 137.445865 \nL 200.605438 136.269048 \nL 210.349618 139.5 \nL 220.093797 138.210895 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \nL 205.873125 122.873388 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \nL 205.873125 13.859501 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \nL 190.861259 137.445865 \nL 200.605438 136.269048 \nL 210.349618 139.5 \nL 220.093797 138.210895 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \nL 205.873125 122.873388 \nL 225.403125 123.492405 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \nL 205.873125 13.859501 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 34.954394 14.757621 \nL 44.698573 87.616622 \nL 54.442752 103.933815 \nL 64.186931 110.919975 \nL 73.93111 115.835806 \nL 83.675289 119.425857 \nL 93.419468 122.319663 \nL 103.163647 123.824553 \nL 112.907826 126.504225 \nL 122.652006 127.605203 \nL 132.396185 130.321944 \nL 142.140364 129.436961 \nL 151.884543 133.781741 \nL 161.628722 132.261981 \nL 171.372901 135.070829 \nL 181.11708 134.70676 \nL 190.861259 137.445865 \nL 200.605438 136.269048 \nL 210.349618 139.5 \nL 220.093797 138.210895 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 49.633125 63.92495 \nL 69.163125 34.190032 \nL 88.693125 113.59215 \nL 108.223125 112.984179 \nL 127.753125 75.770322 \nL 147.283125 122.691305 \nL 166.813125 111.376017 \nL 186.343125 124.635942 \nL 205.873125 122.873388 \nL 225.403125 123.492405 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 49.633125 34.645199 \nL 69.163125 36.328318 \nL 88.693125 16.670146 \nL 108.223125 17.405489 \nL 127.753125 28.500999 \nL 147.283125 14.970686 \nL 166.813125 17.732308 \nL 186.343125 13.5 \nL 205.873125 13.859501 \nL 225.403125 14.055593 \n\" clip-path=\"url(#p1ff7df024d)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 145.8 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 145.8 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 138.8125 100.434375 \nL 218.403125 100.434375 \nQ 220.403125 100.434375 220.403125 98.434375 \nL 220.403125 54.565625 \nQ 220.403125 52.565625 218.403125 52.565625 \nL 138.8125 52.565625 \nQ 136.8125 52.565625 136.8125 54.565625 \nL 136.8125 98.434375 \nQ 136.8125 100.434375 138.8125 100.434375 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_127\">\n     <path d=\"M 140.8125 60.664063 \nL 150.8125 60.664063 \nL 160.8125 60.664063 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- train_loss -->\n     <g transform=\"translate(168.8125 64.164063) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_128\">\n     <path d=\"M 140.8125 75.620313 \nL 150.8125 75.620313 \nL 160.8125 75.620313 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- val_loss -->\n     <g transform=\"translate(168.8125 79.120313) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_129\">\n     <path d=\"M 140.8125 90.576563 \nL 150.8125 90.576563 \nL 160.8125 90.576563 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- val_acc -->\n     <g transform=\"translate(168.8125 94.076563) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1ff7df024d\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = DenseNet(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacabdce",
      "metadata": {
        "id": "bacabdce"
      },
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "The main components that comprise DenseNet are dense blocks and transition layers. For the latter, we need to keep the dimensionality under control when composing the network by adding transition layers that shrink the number of channels again.\n",
        "In terms of cross-layer connections, in contrast to ResNet, where inputs and outputs are added together, DenseNet concatenates inputs and outputs on the channel dimension.\n",
        "Although these concatenation operations\n",
        "reuse features to achieve computational efficiency,\n",
        "unfortunately they lead to heavy GPU memory consumption.\n",
        "As a result,\n",
        "applying DenseNet may require more memory-efficient implementations that may increase training time :cite:`pleiss2017memory`.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Why do we use average pooling rather than max-pooling in the transition layer?\n",
        "1. One of the advantages mentioned in the DenseNet paper is that its model parameters are smaller than those of ResNet. Why is this the case?\n",
        "1. One problem for which DenseNet has been criticized is its high memory consumption.\n",
        "    1. Is this really the case? Try to change the input shape to $224\\times 224$ to compare the actual GPU memory consumption empirically.\n",
        "    1. Can you think of an alternative means of reducing the memory consumption? How would you need to change the framework?\n",
        "1. Implement the various DenseNet versions presented in Table 1 of the DenseNet paper :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`.\n",
        "1. Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price prediction task in :numref:`sec_kaggle_house`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In DenseNet architectures, transition layers are utilized to decrease the spatial dimensions (width and height) of feature maps while also reducing the number of feature maps (channels) before they are forwarded to the subsequent dense block. The decision between using average pooling and max-pooling in these transition layers is influenced by the design objectives and the desired properties of the network. In the context of DenseNet, average pooling is generally favored over max-pooling for several reasons:\n",
        "\n",
        "### 1. Feature Retention\n",
        "Average pooling calculates the mean value of the elements within a pooling region, which retains more information about the features than max-pooling, which only considers the maximum value. In DenseNet, where information from all preceding layers is concatenated, average pooling helps maintain a more comprehensive representation of features.\n",
        "\n",
        "### 2. Smoothing Effect\n",
        "Average pooling imparts a smoothing effect on the output feature maps, which can help mitigate the risk of overfitting by preventing the network from becoming overly sensitive to specific details in the data.\n",
        "\n",
        "### 3. Stability\n",
        "Average pooling is less affected by outliers compared to max-pooling, enhancing the network's robustness against noise or variations in the input data.\n",
        "\n",
        "### 4. Translation Invariance\n",
        "Average pooling offers a degree of translation invariance by considering the overall distribution of values in the pooling area. This is advantageous in situations where minor translations of the input should not significantly impact the output.\n",
        "\n",
        "### 5. Information Sharing\n",
        "Average pooling facilitates information sharing among adjacent pixels or units, aiding in the capture of global patterns and structures within the input data.\n",
        "\n",
        "While average pooling is typically preferred in transition layers, max-pooling may have its own benefits in certain scenarios. For instance, in architectures like convolutional neural networks (CNNs) that emphasize capturing local features and enhancing feature maps, max-pooling can be effective. However, in the context of DenseNet, which prioritizes maintaining a rich flow of information and minimizing the risk of information loss, average pooling is more aligned with the architecture's principles.\n",
        "\n",
        "Ultimately, the choice between average pooling and max-pooling hinges on the specific objectives of the network, the nature of the data, and the overarching design philosophy."
      ],
      "metadata": {
        "id": "86MHYLd1v9_n"
      },
      "id": "86MHYLd1v9_n"
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "data = d2l.FashionMNIST(batch_size=32, resize=(224, 224))\n",
        "arch18 = [(2,[(64,3,1)]*2,None),(2,[(128,3,1)]*2,128),(2,[(256,3,1)]*2,256),(2,[(512,3,1)]*2,512)]\n",
        "resnet18 = d2l.ResNet(arch=arch18, lr=0.01)\n",
        "resnet18.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "print(count_parameters(resnet18))\n",
        "summary(resnet18, (1, 224, 224))\n",
        "\n",
        "11523338\n",
        "----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "            Conv2d-1         [-1, 64, 112, 112]           3,200\n",
        "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
        "              ReLU-3         [-1, 64, 112, 112]               0\n",
        "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
        "            Conv2d-5           [-1, 64, 56, 56]          36,928\n",
        "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
        "              ReLU-7           [-1, 64, 56, 56]               0\n",
        "            Conv2d-8           [-1, 64, 56, 56]          36,928\n",
        "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
        "         Residual-10           [-1, 64, 56, 56]               0\n",
        "           Conv2d-11           [-1, 64, 56, 56]          36,928\n",
        "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
        "             ReLU-13           [-1, 64, 56, 56]               0\n",
        "           Conv2d-14           [-1, 64, 56, 56]          36,928\n",
        "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
        "         Residual-16           [-1, 64, 56, 56]               0\n",
        "           Conv2d-17          [-1, 128, 28, 28]          73,856\n",
        "      BatchNorm2d-18          [-1, 128, 28, 28]             256\n",
        "             ReLU-19          [-1, 128, 28, 28]               0\n",
        "           Conv2d-20          [-1, 128, 28, 28]         147,584\n",
        "      BatchNorm2d-21          [-1, 128, 28, 28]             256\n",
        "           Conv2d-22          [-1, 128, 28, 28]           8,320\n",
        "         Residual-23          [-1, 128, 28, 28]               0\n",
        "           Conv2d-24          [-1, 128, 28, 28]         147,584\n",
        "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
        "             ReLU-26          [-1, 128, 28, 28]               0\n",
        "           Conv2d-27          [-1, 128, 28, 28]         147,584\n",
        "      BatchNorm2d-28          [-1, 128, 28, 28]             256\n",
        "           Conv2d-29          [-1, 128, 28, 28]          16,512\n",
        "         Residual-30          [-1, 128, 28, 28]               0\n",
        "           Conv2d-31          [-1, 256, 14, 14]         295,168\n",
        "      BatchNorm2d-32          [-1, 256, 14, 14]             512\n",
        "             ReLU-33          [-1, 256, 14, 14]               0\n",
        "           Conv2d-34          [-1, 256, 14, 14]         590,080\n",
        "      BatchNorm2d-35          [-1, 256, 14, 14]             512\n",
        "           Conv2d-36          [-1, 256, 14, 14]          33,024\n",
        "         Residual-37          [-1, 256, 14, 14]               0\n",
        "           Conv2d-38          [-1, 256, 14, 14]         590,080\n",
        "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
        "             ReLU-40          [-1, 256, 14, 14]               0\n",
        "           Conv2d-41          [-1, 256, 14, 14]         590,080\n",
        "      BatchNorm2d-42          [-1, 256, 14, 14]             512\n",
        "           Conv2d-43          [-1, 256, 14, 14]          65,792\n",
        "         Residual-44          [-1, 256, 14, 14]               0\n",
        "           Conv2d-45            [-1, 512, 7, 7]       1,180,160\n",
        "      BatchNorm2d-46            [-1, 512, 7, 7]           1,024\n",
        "             ReLU-47            [-1, 512, 7, 7]               0\n",
        "           Conv2d-48            [-1, 512, 7, 7]       2,359,808\n",
        "      BatchNorm2d-49            [-1, 512, 7, 7]           1,024\n",
        "           Conv2d-50            [-1, 512, 7, 7]         131,584\n",
        "         Residual-51            [-1, 512, 7, 7]               0\n",
        "           Conv2d-52            [-1, 512, 7, 7]       2,359,808\n",
        "      BatchNorm2d-53            [-1, 512, 7, 7]           1,024\n",
        "             ReLU-54            [-1, 512, 7, 7]               0\n",
        "           Conv2d-55            [-1, 512, 7, 7]       2,359,808\n",
        "      BatchNorm2d-56            [-1, 512, 7, 7]           1,024\n",
        "           Conv2d-57            [-1, 512, 7, 7]         262,656\n",
        "         Residual-58            [-1, 512, 7, 7]               0\n",
        "AdaptiveAvgPool2d-59            [-1, 512, 1, 1]               0\n",
        "          Flatten-60                  [-1, 512]               0\n",
        "           Linear-61                   [-1, 10]           5,130\n",
        "================================================================\n",
        "Total params: 11,523,338\n",
        "Trainable params: 11,523,338\n",
        "Non-trainable params: 0\n",
        "----------------------------------------------------------------\n",
        "Input size (MB): 0.19\n",
        "Forward/backward pass size (MB): 57.05\n",
        "Params size (MB): 43.96\n",
        "Estimated Total Size (MB): 101.20\n",
        "----------------------------------------------------------------\n",
        "model = DenseNet(lr=0.01)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "print(count_parameters(model))\n",
        "summary(model, (1, 224, 224))\n",
        "758226\n",
        "----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "            Conv2d-1         [-1, 64, 112, 112]           3,200\n",
        "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
        "              ReLU-3         [-1, 64, 112, 112]               0\n",
        "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
        "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
        "              ReLU-6           [-1, 64, 56, 56]               0\n",
        "            Conv2d-7           [-1, 32, 56, 56]          18,464\n",
        "       BatchNorm2d-8           [-1, 96, 56, 56]             192\n",
        "              ReLU-9           [-1, 96, 56, 56]               0\n",
        "           Conv2d-10           [-1, 32, 56, 56]          27,680\n",
        "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
        "             ReLU-12          [-1, 128, 56, 56]               0\n",
        "           Conv2d-13           [-1, 32, 56, 56]          36,896\n",
        "      BatchNorm2d-14          [-1, 160, 56, 56]             320\n",
        "             ReLU-15          [-1, 160, 56, 56]               0\n",
        "           Conv2d-16           [-1, 32, 56, 56]          46,112\n",
        "       DenseBlock-17          [-1, 192, 56, 56]               0\n",
        "      BatchNorm2d-18          [-1, 192, 56, 56]             384\n",
        "             ReLU-19          [-1, 192, 56, 56]               0\n",
        "           Conv2d-20           [-1, 96, 56, 56]          18,528\n",
        "        AvgPool2d-21           [-1, 96, 28, 28]               0\n",
        "      BatchNorm2d-22           [-1, 96, 28, 28]             192\n",
        "             ReLU-23           [-1, 96, 28, 28]               0\n",
        "           Conv2d-24           [-1, 32, 28, 28]          27,680\n",
        "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
        "             ReLU-26          [-1, 128, 28, 28]               0\n",
        "           Conv2d-27           [-1, 32, 28, 28]          36,896\n",
        "      BatchNorm2d-28          [-1, 160, 28, 28]             320\n",
        "             ReLU-29          [-1, 160, 28, 28]               0\n",
        "           Conv2d-30           [-1, 32, 28, 28]          46,112\n",
        "      BatchNorm2d-31          [-1, 192, 28, 28]             384\n",
        "             ReLU-32          [-1, 192, 28, 28]               0\n",
        "           Conv2d-33           [-1, 32, 28, 28]          55,328\n",
        "       DenseBlock-34          [-1, 224, 28, 28]               0\n",
        "      BatchNorm2d-35          [-1, 224, 28, 28]             448\n",
        "             ReLU-36          [-1, 224, 28, 28]               0\n",
        "           Conv2d-37          [-1, 112, 28, 28]          25,200\n",
        "        AvgPool2d-38          [-1, 112, 14, 14]               0\n",
        "      BatchNorm2d-39          [-1, 112, 14, 14]             224\n",
        "             ReLU-40          [-1, 112, 14, 14]               0\n",
        "           Conv2d-41           [-1, 32, 14, 14]          32,288\n",
        "      BatchNorm2d-42          [-1, 144, 14, 14]             288\n",
        "             ReLU-43          [-1, 144, 14, 14]               0\n",
        "           Conv2d-44           [-1, 32, 14, 14]          41,504\n",
        "      BatchNorm2d-45          [-1, 176, 14, 14]             352\n",
        "             ReLU-46          [-1, 176, 14, 14]               0\n",
        "           Conv2d-47           [-1, 32, 14, 14]          50,720\n",
        "      BatchNorm2d-48          [-1, 208, 14, 14]             416\n",
        "             ReLU-49          [-1, 208, 14, 14]               0\n",
        "           Conv2d-50           [-1, 32, 14, 14]          59,936\n",
        "       DenseBlock-51          [-1, 240, 14, 14]               0\n",
        "      BatchNorm2d-52          [-1, 240, 14, 14]             480\n",
        "             ReLU-53          [-1, 240, 14, 14]               0\n",
        "           Conv2d-54          [-1, 120, 14, 14]          28,920\n",
        "        AvgPool2d-55            [-1, 120, 7, 7]               0\n",
        "      BatchNorm2d-56            [-1, 120, 7, 7]             240\n",
        "             ReLU-57            [-1, 120, 7, 7]               0\n",
        "           Conv2d-58             [-1, 32, 7, 7]          34,592\n",
        "      BatchNorm2d-59            [-1, 152, 7, 7]             304\n",
        "             ReLU-60            [-1, 152, 7, 7]               0\n",
        "           Conv2d-61             [-1, 32, 7, 7]          43,808\n",
        "      BatchNorm2d-62            [-1, 184, 7, 7]             368\n",
        "             ReLU-63            [-1, 184, 7, 7]               0\n",
        "           Conv2d-64             [-1, 32, 7, 7]          53,024\n",
        "      BatchNorm2d-65            [-1, 216, 7, 7]             432\n",
        "             ReLU-66            [-1, 216, 7, 7]               0\n",
        "           Conv2d-67             [-1, 32, 7, 7]          62,240\n",
        "       DenseBlock-68            [-1, 248, 7, 7]               0\n",
        "      BatchNorm2d-69            [-1, 248, 7, 7]             496\n",
        "             ReLU-70            [-1, 248, 7, 7]               0\n",
        "AdaptiveAvgPool2d-71            [-1, 248, 1, 1]               0\n",
        "          Flatten-72                  [-1, 248]               0\n",
        "           Linear-73                   [-1, 10]           2,490\n",
        "================================================================\n",
        "Total params: 758,226\n",
        "Trainable params: 758,226\n",
        "Non-trainable params: 0\n",
        "----------------------------------------------------------------\n",
        "Input size (MB): 0.19\n",
        "Forward/backward pass size (MB): 77.81\n",
        "Params size (MB): 2.89\n",
        "Estimated Total Size (MB): 80.89\n"
      ],
      "metadata": {
        "id": "yGwQL3wbv-UP"
      },
      "id": "yGwQL3wbv-UP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the combination of dense connections, efficient feature reuse, bottleneck layers, and reduced filters allows DenseNet to maintain a smaller parameter count while still achieving robust performance. This architectural efficiency contributes to DenseNet's advantages in terms of model size and computational requirements."
      ],
      "metadata": {
        "id": "gvZ1F9cuwOc5"
      },
      "id": "gvZ1F9cuwOc5"
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "data = d2l.FashionMNIST(batch_size=32, resize=(28, 28))\n",
        "model = DenseNet(lr=0.01)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.empty_cache()\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "memory_stats = torch.cuda.memory_stats(device=device)\n",
        "# Print peak memory usage and other memory statistics\n",
        "print(\"Peak memory usage:\", memory_stats[\"allocated_bytes.all.peak\"] / (1024 ** 2), \"MB\")\n",
        "print(\"Current memory usage:\", memory_stats[\"allocated_bytes.all.current\"] / (1024 ** 2), \"MB\")\n",
        "data = d2l.FashionMNIST(batch_size=32, resize=(224, 224))\n",
        "model = DenseNet(lr=0.01)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.empty_cache()\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n",
        "memory_stats = torch.cuda.memory_stats(device=device)\n",
        "# Print peak memory usage and other memory statistics\n",
        "print(\"Peak memory usage:\", memory_stats[\"allocated_bytes.all.peak\"] / (1024 ** 2), \"MB\")\n",
        "print(\"Current memory usage:\", memory_stats[\"allocated_bytes.all.current\"] / (1024 ** 2), \"MB\")\n"
      ],
      "metadata": {
        "id": "mgUAFZ8_wQf2"
      },
      "id": "mgUAFZ8_wQf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing memory consumption in a DenseNet architecture can be achieved through several strategies. One effective approach is to incorporate sparsity into the model, which minimizes the number of active connections and parameters. Here are some ways to modify the framework to accomplish this:\n",
        "\n",
        "### 1. Sparse Connectivity in Dense Blocks\n",
        "Instead of maintaining fully connected dense blocks, you can implement sparse connectivity patterns. This means that not every layer connects to all preceding layers in the dense block. You can randomly select a subset of feature maps from previous layers to concatenate with the current layer, which helps decrease the number of connections and memory usage.\n",
        "\n",
        "### 2. Channel Pruning\n",
        "Apply channel pruning techniques within the dense blocks to identify and remove less significant channels from the concatenation operation. This effectively reduces the number of active channels, leading to memory savings.\n",
        "\n",
        "### 3. Regularization and Compression\n",
        "Introduce regularization techniques, such as L1 regularization during training, to encourage certain weights to become exactly zero. Additionally, consider model compression methods like knowledge distillation or quantization to minimize the model's memory footprint.\n",
        "\n",
        "### 4. Low-Rank Approximations\n",
        "Utilize low-rank matrix factorization on the weight matrices in the dense blocks. This method approximates the weight matrices with lower-dimensional components, resulting in reduced memory requirements.\n",
        "\n",
        "### 5. Dynamic Allocation\n",
        "Implement dynamic memory allocation during inference to store only the necessary feature maps. This approach prevents the allocation of memory for feature maps that are no longer needed.\n",
        "\n",
        "### 6. Sparsity-Inducing Activation Functions\n",
        "Incorporate activation functions that promote sparsity, such as the ReLU6 function, which caps activations at a maximum value, potentially rendering some neurons inactive.\n",
        "\n",
        "### 7. Adaptive Dense Blocks\n",
        "Design adaptive dense blocks that adjust their connectivity patterns dynamically based on the data distribution. For instance, attention mechanisms can be employed to determine which previous feature maps to concatenate based on their relevance.\n",
        "\n",
        "Implementing these changes will necessitate modifications to the architecture, training process, and possibly the creation of custom layers or adjustments to existing ones. It’s essential to recognize that these techniques may involve a trade-off between memory reduction and model performance. Experimentation and fine-tuning of these strategies for your specific problem domain are recommended to find the optimal balance."
      ],
      "metadata": {
        "id": "Oh2GsGI0wiN0"
      },
      "id": "Oh2GsGI0wiN0"
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "def conv_block(num_channels, kernel_size, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=kernel_size, padding=padding))\n",
        "\n",
        "def transition_block(num_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "        nn.LazyConv2d(num_channels, kernel_size=1),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, convs, num_channels):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layer = []\n",
        "        for kernel_size, padding in convs:\n",
        "            layer.append(conv_block(num_channels, kernel_size, padding))\n",
        "        self.net = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for blk in self.net:\n",
        "            Y = blk(X)\n",
        "            # Concatenate input and output of each block along the channels\n",
        "            X = torch.cat((X, Y), dim=1)\n",
        "        return X\n",
        "\n",
        "class DenseNet(d2l.Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    def __init__(self, num_channels=64, growth_rate=32, arch=[[[3,1],[3,1]],[[3,1],[3,1]]],lr=0.1, num_classes=10):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.b1())\n",
        "        for i, convs in enumerate(arch):\n",
        "            self.net.add_module(f'dense_blk{i+1}', DenseBlock(convs, growth_rate))\n",
        "            # The number of output channels in the previous dense block\n",
        "            num_channels += len(convs) * growth_rate\n",
        "            # A transition layer that halves the number of channels is added\n",
        "            # between the dense blocks\n",
        "            if i != len(arch) - 1:\n",
        "                num_channels //= 2\n",
        "                self.net.add_module(f'tran_blk{i+1}', transition_block(\n",
        "                    num_channels))\n",
        "        self.net.add_module('last', nn.Sequential(\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "        self.net.apply(d2l.init_cnn)\n",
        "data = d2l.FashionMNIST(batch_size=32, resize=(224, 224))\n",
        "arch121 = ([[[1,0],[3,1]]*6,[[1,0],[3,1]]*12,[[1,0],[3,1]]*24,[[1,0],[3,1]]*16])\n",
        "densenet121 = DenseNet(lr=0.01, arch=arch121)\n",
        "densenet121.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "# print(count_parameters(model))\n",
        "summary(densenet121, (1, 224, 224))\n",
        "----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "            Conv2d-1         [-1, 64, 112, 112]           3,200\n",
        "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
        "              ReLU-3         [-1, 64, 112, 112]               0\n",
        "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
        "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
        "              ReLU-6           [-1, 64, 56, 56]               0\n",
        "            Conv2d-7           [-1, 32, 56, 56]           2,080\n",
        "       BatchNorm2d-8           [-1, 96, 56, 56]             192\n",
        "              ReLU-9           [-1, 96, 56, 56]               0\n",
        "           Conv2d-10           [-1, 32, 56, 56]          27,680\n",
        "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
        "             ReLU-12          [-1, 128, 56, 56]               0\n",
        "           Conv2d-13           [-1, 32, 56, 56]           4,128\n",
        "      BatchNorm2d-14          [-1, 160, 56, 56]             320\n",
        "             ReLU-15          [-1, 160, 56, 56]               0\n",
        "           Conv2d-16           [-1, 32, 56, 56]          46,112\n",
        "      BatchNorm2d-17          [-1, 192, 56, 56]             384\n",
        "             ReLU-18          [-1, 192, 56, 56]               0\n",
        "           Conv2d-19           [-1, 32, 56, 56]           6,176\n",
        "      BatchNorm2d-20          [-1, 224, 56, 56]             448\n",
        "             ReLU-21          [-1, 224, 56, 56]               0\n",
        "           Conv2d-22           [-1, 32, 56, 56]          64,544\n",
        "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
        "             ReLU-24          [-1, 256, 56, 56]               0\n",
        "           Conv2d-25           [-1, 32, 56, 56]           8,224\n",
        "      BatchNorm2d-26          [-1, 288, 56, 56]             576\n",
        "             ReLU-27          [-1, 288, 56, 56]               0\n",
        "           Conv2d-28           [-1, 32, 56, 56]          82,976\n",
        "      BatchNorm2d-29          [-1, 320, 56, 56]             640\n",
        "             ReLU-30          [-1, 320, 56, 56]               0\n",
        "           Conv2d-31           [-1, 32, 56, 56]          10,272\n",
        "      BatchNorm2d-32          [-1, 352, 56, 56]             704\n",
        "             ReLU-33          [-1, 352, 56, 56]               0\n",
        "           Conv2d-34           [-1, 32, 56, 56]         101,408\n",
        "      BatchNorm2d-35          [-1, 384, 56, 56]             768\n",
        "             ReLU-36          [-1, 384, 56, 56]               0\n",
        "           Conv2d-37           [-1, 32, 56, 56]          12,320\n",
        "      BatchNorm2d-38          [-1, 416, 56, 56]             832\n",
        "             ReLU-39          [-1, 416, 56, 56]               0\n",
        "           Conv2d-40           [-1, 32, 56, 56]         119,840\n",
        "       DenseBlock-41          [-1, 448, 56, 56]               0\n",
        "      BatchNorm2d-42          [-1, 448, 56, 56]             896\n",
        "             ReLU-43          [-1, 448, 56, 56]               0\n",
        "           Conv2d-44          [-1, 224, 56, 56]         100,576\n",
        "        AvgPool2d-45          [-1, 224, 28, 28]               0\n",
        "      BatchNorm2d-46          [-1, 224, 28, 28]             448\n",
        "             ReLU-47          [-1, 224, 28, 28]               0\n",
        "           Conv2d-48           [-1, 32, 28, 28]           7,200\n",
        "      BatchNorm2d-49          [-1, 256, 28, 28]             512\n",
        "             ReLU-50          [-1, 256, 28, 28]               0\n",
        "           Conv2d-51           [-1, 32, 28, 28]          73,760\n",
        "      BatchNorm2d-52          [-1, 288, 28, 28]             576\n",
        "             ReLU-53          [-1, 288, 28, 28]               0\n",
        "           Conv2d-54           [-1, 32, 28, 28]           9,248\n",
        "      BatchNorm2d-55          [-1, 320, 28, 28]             640\n",
        "             ReLU-56          [-1, 320, 28, 28]               0\n",
        "           Conv2d-57           [-1, 32, 28, 28]          92,192\n",
        "      BatchNorm2d-58          [-1, 352, 28, 28]             704\n",
        "             ReLU-59          [-1, 352, 28, 28]               0\n",
        "           Conv2d-60           [-1, 32, 28, 28]          11,296\n",
        "      BatchNorm2d-61          [-1, 384, 28, 28]             768\n",
        "             ReLU-62          [-1, 384, 28, 28]               0\n",
        "           Conv2d-63           [-1, 32, 28, 28]         110,624\n",
        "      BatchNorm2d-64          [-1, 416, 28, 28]             832\n",
        "             ReLU-65          [-1, 416, 28, 28]               0\n",
        "           Conv2d-66           [-1, 32, 28, 28]          13,344\n",
        "      BatchNorm2d-67          [-1, 448, 28, 28]             896\n",
        "             ReLU-68          [-1, 448, 28, 28]               0\n",
        "           Conv2d-69           [-1, 32, 28, 28]         129,056\n",
        "      BatchNorm2d-70          [-1, 480, 28, 28]             960\n",
        "             ReLU-71          [-1, 480, 28, 28]               0\n",
        "           Conv2d-72           [-1, 32, 28, 28]          15,392\n",
        "      BatchNorm2d-73          [-1, 512, 28, 28]           1,024\n",
        "             ReLU-74          [-1, 512, 28, 28]               0\n",
        "           Conv2d-75           [-1, 32, 28, 28]         147,488\n",
        "      BatchNorm2d-76          [-1, 544, 28, 28]           1,088\n",
        "             ReLU-77          [-1, 544, 28, 28]               0\n",
        "           Conv2d-78           [-1, 32, 28, 28]          17,440\n",
        "      BatchNorm2d-79          [-1, 576, 28, 28]           1,152\n",
        "             ReLU-80          [-1, 576, 28, 28]               0\n",
        "           Conv2d-81           [-1, 32, 28, 28]         165,920\n",
        "      BatchNorm2d-82          [-1, 608, 28, 28]           1,216\n",
        "             ReLU-83          [-1, 608, 28, 28]               0\n",
        "           Conv2d-84           [-1, 32, 28, 28]          19,488\n",
        "      BatchNorm2d-85          [-1, 640, 28, 28]           1,280\n",
        "             ReLU-86          [-1, 640, 28, 28]               0\n",
        "           Conv2d-87           [-1, 32, 28, 28]         184,352\n",
        "      BatchNorm2d-88          [-1, 672, 28, 28]           1,344\n",
        "             ReLU-89          [-1, 672, 28, 28]               0\n",
        "           Conv2d-90           [-1, 32, 28, 28]          21,536\n",
        "      BatchNorm2d-91          [-1, 704, 28, 28]           1,408\n",
        "             ReLU-92          [-1, 704, 28, 28]               0\n",
        "           Conv2d-93           [-1, 32, 28, 28]         202,784\n",
        "      BatchNorm2d-94          [-1, 736, 28, 28]           1,472\n",
        "             ReLU-95          [-1, 736, 28, 28]               0\n",
        "           Conv2d-96           [-1, 32, 28, 28]          23,584\n",
        "      BatchNorm2d-97          [-1, 768, 28, 28]           1,536\n",
        "             ReLU-98          [-1, 768, 28, 28]               0\n",
        "           Conv2d-99           [-1, 32, 28, 28]         221,216\n",
        "     BatchNorm2d-100          [-1, 800, 28, 28]           1,600\n",
        "            ReLU-101          [-1, 800, 28, 28]               0\n",
        "          Conv2d-102           [-1, 32, 28, 28]          25,632\n",
        "     BatchNorm2d-103          [-1, 832, 28, 28]           1,664\n",
        "            ReLU-104          [-1, 832, 28, 28]               0\n",
        "          Conv2d-105           [-1, 32, 28, 28]         239,648\n",
        "     BatchNorm2d-106          [-1, 864, 28, 28]           1,728\n",
        "            ReLU-107          [-1, 864, 28, 28]               0\n",
        "          Conv2d-108           [-1, 32, 28, 28]          27,680\n",
        "     BatchNorm2d-109          [-1, 896, 28, 28]           1,792\n",
        "            ReLU-110          [-1, 896, 28, 28]               0\n",
        "          Conv2d-111           [-1, 32, 28, 28]         258,080\n",
        "     BatchNorm2d-112          [-1, 928, 28, 28]           1,856\n",
        "            ReLU-113          [-1, 928, 28, 28]               0\n",
        "          Conv2d-114           [-1, 32, 28, 28]          29,728\n",
        "     BatchNorm2d-115          [-1, 960, 28, 28]           1,920\n",
        "            ReLU-116          [-1, 960, 28, 28]               0\n",
        "          Conv2d-117           [-1, 32, 28, 28]         276,512\n",
        "      DenseBlock-118          [-1, 992, 28, 28]               0\n",
        "     BatchNorm2d-119          [-1, 992, 28, 28]           1,984\n",
        "            ReLU-120          [-1, 992, 28, 28]               0\n",
        "          Conv2d-121          [-1, 496, 28, 28]         492,528\n",
        "       AvgPool2d-122          [-1, 496, 14, 14]               0\n",
        "     BatchNorm2d-123          [-1, 496, 14, 14]             992\n",
        "            ReLU-124          [-1, 496, 14, 14]               0\n",
        "          Conv2d-125           [-1, 32, 14, 14]          15,904\n",
        "     BatchNorm2d-126          [-1, 528, 14, 14]           1,056\n",
        "            ReLU-127          [-1, 528, 14, 14]               0\n",
        "          Conv2d-128           [-1, 32, 14, 14]         152,096\n",
        "     BatchNorm2d-129          [-1, 560, 14, 14]           1,120\n",
        "            ReLU-130          [-1, 560, 14, 14]               0\n",
        "          Conv2d-131           [-1, 32, 14, 14]          17,952\n",
        "     BatchNorm2d-132          [-1, 592, 14, 14]           1,184\n",
        "            ReLU-133          [-1, 592, 14, 14]               0\n",
        "          Conv2d-134           [-1, 32, 14, 14]         170,528\n",
        "     BatchNorm2d-135          [-1, 624, 14, 14]           1,248\n",
        "            ReLU-136          [-1, 624, 14, 14]               0\n",
        "          Conv2d-137           [-1, 32, 14, 14]          20,000\n",
        "     BatchNorm2d-138          [-1, 656, 14, 14]           1,312\n",
        "            ReLU-139          [-1, 656, 14, 14]               0\n",
        "          Conv2d-140           [-1, 32, 14, 14]         188,960\n",
        "     BatchNorm2d-141          [-1, 688, 14, 14]           1,376\n",
        "            ReLU-142          [-1, 688, 14, 14]               0\n",
        "          Conv2d-143           [-1, 32, 14, 14]          22,048\n",
        "     BatchNorm2d-144          [-1, 720, 14, 14]           1,440\n",
        "            ReLU-145          [-1, 720, 14, 14]               0\n",
        "          Conv2d-146           [-1, 32, 14, 14]         207,392\n",
        "     BatchNorm2d-147          [-1, 752, 14, 14]           1,504\n",
        "            ReLU-148          [-1, 752, 14, 14]               0\n",
        "          Conv2d-149           [-1, 32, 14, 14]          24,096\n",
        "     BatchNorm2d-150          [-1, 784, 14, 14]           1,568\n",
        "            ReLU-151          [-1, 784, 14, 14]               0\n",
        "          Conv2d-152           [-1, 32, 14, 14]         225,824\n",
        "     BatchNorm2d-153          [-1, 816, 14, 14]           1,632\n",
        "            ReLU-154          [-1, 816, 14, 14]               0\n",
        "          Conv2d-155           [-1, 32, 14, 14]          26,144\n",
        "     BatchNorm2d-156          [-1, 848, 14, 14]           1,696\n",
        "            ReLU-157          [-1, 848, 14, 14]               0\n",
        "          Conv2d-158           [-1, 32, 14, 14]         244,256\n",
        "     BatchNorm2d-159          [-1, 880, 14, 14]           1,760\n",
        "            ReLU-160          [-1, 880, 14, 14]               0\n",
        "          Conv2d-161           [-1, 32, 14, 14]          28,192\n",
        "     BatchNorm2d-162          [-1, 912, 14, 14]           1,824\n",
        "            ReLU-163          [-1, 912, 14, 14]               0\n",
        "          Conv2d-164           [-1, 32, 14, 14]         262,688\n",
        "     BatchNorm2d-165          [-1, 944, 14, 14]           1,888\n",
        "            ReLU-166          [-1, 944, 14, 14]               0\n",
        "          Conv2d-167           [-1, 32, 14, 14]          30,240\n",
        "     BatchNorm2d-168          [-1, 976, 14, 14]           1,952\n",
        "            ReLU-169          [-1, 976, 14, 14]               0\n",
        "          Conv2d-170           [-1, 32, 14, 14]         281,120\n",
        "     BatchNorm2d-171         [-1, 1008, 14, 14]           2,016\n",
        "            ReLU-172         [-1, 1008, 14, 14]               0\n",
        "          Conv2d-173           [-1, 32, 14, 14]          32,288\n",
        "     BatchNorm2d-174         [-1, 1040, 14, 14]           2,080\n",
        "            ReLU-175         [-1, 1040, 14, 14]               0\n",
        "          Conv2d-176           [-1, 32, 14, 14]         299,552\n",
        "     BatchNorm2d-177         [-1, 1072, 14, 14]           2,144\n",
        "            ReLU-178         [-1, 1072, 14, 14]               0\n",
        "          Conv2d-179           [-1, 32, 14, 14]          34,336\n",
        "     BatchNorm2d-180         [-1, 1104, 14, 14]           2,208\n",
        "            ReLU-181         [-1, 1104, 14, 14]               0\n",
        "          Conv2d-182           [-1, 32, 14, 14]         317,984\n",
        "     BatchNorm2d-183         [-1, 1136, 14, 14]           2,272\n",
        "            ReLU-184         [-1, 1136, 14, 14]               0\n",
        "          Conv2d-185           [-1, 32, 14, 14]          36,384\n",
        "     BatchNorm2d-186         [-1, 1168, 14, 14]           2,336\n",
        "            ReLU-187         [-1, 1168, 14, 14]               0\n",
        "          Conv2d-188           [-1, 32, 14, 14]         336,416\n",
        "     BatchNorm2d-189         [-1, 1200, 14, 14]           2,400\n",
        "            ReLU-190         [-1, 1200, 14, 14]               0\n",
        "          Conv2d-191           [-1, 32, 14, 14]          38,432\n",
        "     BatchNorm2d-192         [-1, 1232, 14, 14]           2,464\n",
        "            ReLU-193         [-1, 1232, 14, 14]               0\n",
        "          Conv2d-194           [-1, 32, 14, 14]         354,848\n",
        "     BatchNorm2d-195         [-1, 1264, 14, 14]           2,528\n",
        "            ReLU-196         [-1, 1264, 14, 14]               0\n",
        "          Conv2d-197           [-1, 32, 14, 14]          40,480\n",
        "     BatchNorm2d-198         [-1, 1296, 14, 14]           2,592\n",
        "            ReLU-199         [-1, 1296, 14, 14]               0\n",
        "          Conv2d-200           [-1, 32, 14, 14]         373,280\n",
        "     BatchNorm2d-201         [-1, 1328, 14, 14]           2,656\n",
        "            ReLU-202         [-1, 1328, 14, 14]               0\n",
        "          Conv2d-203           [-1, 32, 14, 14]          42,528\n",
        "     BatchNorm2d-204         [-1, 1360, 14, 14]           2,720\n",
        "            ReLU-205         [-1, 1360, 14, 14]               0\n",
        "          Conv2d-206           [-1, 32, 14, 14]         391,712\n",
        "     BatchNorm2d-207         [-1, 1392, 14, 14]           2,784\n",
        "            ReLU-208         [-1, 1392, 14, 14]               0\n",
        "          Conv2d-209           [-1, 32, 14, 14]          44,576\n",
        "     BatchNorm2d-210         [-1, 1424, 14, 14]           2,848\n",
        "            ReLU-211         [-1, 1424, 14, 14]               0\n",
        "          Conv2d-212           [-1, 32, 14, 14]         410,144\n",
        "     BatchNorm2d-213         [-1, 1456, 14, 14]           2,912\n",
        "            ReLU-214         [-1, 1456, 14, 14]               0\n",
        "          Conv2d-215           [-1, 32, 14, 14]          46,624\n",
        "     BatchNorm2d-216         [-1, 1488, 14, 14]           2,976\n",
        "            ReLU-217         [-1, 1488, 14, 14]               0\n",
        "          Conv2d-218           [-1, 32, 14, 14]         428,576\n",
        "     BatchNorm2d-219         [-1, 1520, 14, 14]           3,040\n",
        "            ReLU-220         [-1, 1520, 14, 14]               0\n",
        "          Conv2d-221           [-1, 32, 14, 14]          48,672\n",
        "     BatchNorm2d-222         [-1, 1552, 14, 14]           3,104\n",
        "            ReLU-223         [-1, 1552, 14, 14]               0\n",
        "          Conv2d-224           [-1, 32, 14, 14]         447,008\n",
        "     BatchNorm2d-225         [-1, 1584, 14, 14]           3,168\n",
        "            ReLU-226         [-1, 1584, 14, 14]               0\n",
        "          Conv2d-227           [-1, 32, 14, 14]          50,720\n",
        "     BatchNorm2d-228         [-1, 1616, 14, 14]           3,232\n",
        "            ReLU-229         [-1, 1616, 14, 14]               0\n",
        "          Conv2d-230           [-1, 32, 14, 14]         465,440\n",
        "     BatchNorm2d-231         [-1, 1648, 14, 14]           3,296\n",
        "            ReLU-232         [-1, 1648, 14, 14]               0\n",
        "          Conv2d-233           [-1, 32, 14, 14]          52,768\n",
        "     BatchNorm2d-234         [-1, 1680, 14, 14]           3,360\n",
        "            ReLU-235         [-1, 1680, 14, 14]               0\n",
        "          Conv2d-236           [-1, 32, 14, 14]         483,872\n",
        "     BatchNorm2d-237         [-1, 1712, 14, 14]           3,424\n",
        "            ReLU-238         [-1, 1712, 14, 14]               0\n",
        "          Conv2d-239           [-1, 32, 14, 14]          54,816\n",
        "     BatchNorm2d-240         [-1, 1744, 14, 14]           3,488\n",
        "            ReLU-241         [-1, 1744, 14, 14]               0\n",
        "          Conv2d-242           [-1, 32, 14, 14]         502,304\n",
        "     BatchNorm2d-243         [-1, 1776, 14, 14]           3,552\n",
        "            ReLU-244         [-1, 1776, 14, 14]               0\n",
        "          Conv2d-245           [-1, 32, 14, 14]          56,864\n",
        "     BatchNorm2d-246         [-1, 1808, 14, 14]           3,616\n",
        "            ReLU-247         [-1, 1808, 14, 14]               0\n",
        "          Conv2d-248           [-1, 32, 14, 14]         520,736\n",
        "     BatchNorm2d-249         [-1, 1840, 14, 14]           3,680\n",
        "            ReLU-250         [-1, 1840, 14, 14]               0\n",
        "          Conv2d-251           [-1, 32, 14, 14]          58,912\n",
        "     BatchNorm2d-252         [-1, 1872, 14, 14]           3,744\n",
        "            ReLU-253         [-1, 1872, 14, 14]               0\n",
        "          Conv2d-254           [-1, 32, 14, 14]         539,168\n",
        "     BatchNorm2d-255         [-1, 1904, 14, 14]           3,808\n",
        "            ReLU-256         [-1, 1904, 14, 14]               0\n",
        "          Conv2d-257           [-1, 32, 14, 14]          60,960\n",
        "     BatchNorm2d-258         [-1, 1936, 14, 14]           3,872\n",
        "            ReLU-259         [-1, 1936, 14, 14]               0\n",
        "          Conv2d-260           [-1, 32, 14, 14]         557,600\n",
        "     BatchNorm2d-261         [-1, 1968, 14, 14]           3,936\n",
        "            ReLU-262         [-1, 1968, 14, 14]               0\n",
        "          Conv2d-263           [-1, 32, 14, 14]          63,008\n",
        "     BatchNorm2d-264         [-1, 2000, 14, 14]           4,000\n",
        "            ReLU-265         [-1, 2000, 14, 14]               0\n",
        "          Conv2d-266           [-1, 32, 14, 14]         576,032\n",
        "      DenseBlock-267         [-1, 2032, 14, 14]               0\n",
        "     BatchNorm2d-268         [-1, 2032, 14, 14]           4,064\n",
        "            ReLU-269         [-1, 2032, 14, 14]               0\n",
        "          Conv2d-270         [-1, 1016, 14, 14]       2,065,528\n",
        "       AvgPool2d-271           [-1, 1016, 7, 7]               0\n",
        "     BatchNorm2d-272           [-1, 1016, 7, 7]           2,032\n",
        "            ReLU-273           [-1, 1016, 7, 7]               0\n",
        "          Conv2d-274             [-1, 32, 7, 7]          32,544\n",
        "     BatchNorm2d-275           [-1, 1048, 7, 7]           2,096\n",
        "            ReLU-276           [-1, 1048, 7, 7]               0\n",
        "          Conv2d-277             [-1, 32, 7, 7]         301,856\n",
        "     BatchNorm2d-278           [-1, 1080, 7, 7]           2,160\n",
        "            ReLU-279           [-1, 1080, 7, 7]               0\n",
        "          Conv2d-280             [-1, 32, 7, 7]          34,592\n",
        "     BatchNorm2d-281           [-1, 1112, 7, 7]           2,224\n",
        "            ReLU-282           [-1, 1112, 7, 7]               0\n",
        "          Conv2d-283             [-1, 32, 7, 7]         320,288\n",
        "     BatchNorm2d-284           [-1, 1144, 7, 7]           2,288\n",
        "            ReLU-285           [-1, 1144, 7, 7]               0\n",
        "          Conv2d-286             [-1, 32, 7, 7]          36,640\n",
        "     BatchNorm2d-287           [-1, 1176, 7, 7]           2,352\n",
        "            ReLU-288           [-1, 1176, 7, 7]               0\n",
        "          Conv2d-289             [-1, 32, 7, 7]         338,720\n",
        "     BatchNorm2d-290           [-1, 1208, 7, 7]           2,416\n",
        "            ReLU-291           [-1, 1208, 7, 7]               0\n",
        "          Conv2d-292             [-1, 32, 7, 7]          38,688\n",
        "     BatchNorm2d-293           [-1, 1240, 7, 7]           2,480\n",
        "            ReLU-294           [-1, 1240, 7, 7]               0\n",
        "          Conv2d-295             [-1, 32, 7, 7]         357,152\n",
        "     BatchNorm2d-296           [-1, 1272, 7, 7]           2,544\n",
        "            ReLU-297           [-1, 1272, 7, 7]               0\n",
        "          Conv2d-298             [-1, 32, 7, 7]          40,736\n",
        "     BatchNorm2d-299           [-1, 1304, 7, 7]           2,608\n",
        "            ReLU-300           [-1, 1304, 7, 7]               0\n",
        "          Conv2d-301             [-1, 32, 7, 7]         375,584\n",
        "     BatchNorm2d-302           [-1, 1336, 7, 7]           2,672\n",
        "            ReLU-303           [-1, 1336, 7, 7]               0\n",
        "          Conv2d-304             [-1, 32, 7, 7]          42,784\n",
        "     BatchNorm2d-305           [-1, 1368, 7, 7]           2,736\n",
        "            ReLU-306           [-1, 1368, 7, 7]               0\n",
        "          Conv2d-307             [-1, 32, 7, 7]         394,016\n",
        "     BatchNorm2d-308           [-1, 1400, 7, 7]           2,800\n",
        "            ReLU-309           [-1, 1400, 7, 7]               0\n",
        "          Conv2d-310             [-1, 32, 7, 7]          44,832\n",
        "     BatchNorm2d-311           [-1, 1432, 7, 7]           2,864\n",
        "            ReLU-312           [-1, 1432, 7, 7]               0\n",
        "          Conv2d-313             [-1, 32, 7, 7]         412,448\n",
        "     BatchNorm2d-314           [-1, 1464, 7, 7]           2,928\n",
        "            ReLU-315           [-1, 1464, 7, 7]               0\n",
        "          Conv2d-316             [-1, 32, 7, 7]          46,880\n",
        "     BatchNorm2d-317           [-1, 1496, 7, 7]           2,992\n",
        "            ReLU-318           [-1, 1496, 7, 7]               0\n",
        "          Conv2d-319             [-1, 32, 7, 7]         430,880\n",
        "     BatchNorm2d-320           [-1, 1528, 7, 7]           3,056\n",
        "            ReLU-321           [-1, 1528, 7, 7]               0\n",
        "          Conv2d-322             [-1, 32, 7, 7]          48,928\n",
        "     BatchNorm2d-323           [-1, 1560, 7, 7]           3,120\n",
        "            ReLU-324           [-1, 1560, 7, 7]               0\n",
        "          Conv2d-325             [-1, 32, 7, 7]         449,312\n",
        "     BatchNorm2d-326           [-1, 1592, 7, 7]           3,184\n",
        "            ReLU-327           [-1, 1592, 7, 7]               0\n",
        "          Conv2d-328             [-1, 32, 7, 7]          50,976\n",
        "     BatchNorm2d-329           [-1, 1624, 7, 7]           3,248\n",
        "            ReLU-330           [-1, 1624, 7, 7]               0\n",
        "          Conv2d-331             [-1, 32, 7, 7]         467,744\n",
        "     BatchNorm2d-332           [-1, 1656, 7, 7]           3,312\n",
        "            ReLU-333           [-1, 1656, 7, 7]               0\n",
        "          Conv2d-334             [-1, 32, 7, 7]          53,024\n",
        "     BatchNorm2d-335           [-1, 1688, 7, 7]           3,376\n",
        "            ReLU-336           [-1, 1688, 7, 7]               0\n",
        "          Conv2d-337             [-1, 32, 7, 7]         486,176\n",
        "     BatchNorm2d-338           [-1, 1720, 7, 7]           3,440\n",
        "            ReLU-339           [-1, 1720, 7, 7]               0\n",
        "          Conv2d-340             [-1, 32, 7, 7]          55,072\n",
        "     BatchNorm2d-341           [-1, 1752, 7, 7]           3,504\n",
        "            ReLU-342           [-1, 1752, 7, 7]               0\n",
        "          Conv2d-343             [-1, 32, 7, 7]         504,608\n",
        "     BatchNorm2d-344           [-1, 1784, 7, 7]           3,568\n",
        "            ReLU-345           [-1, 1784, 7, 7]               0\n",
        "          Conv2d-346             [-1, 32, 7, 7]          57,120\n",
        "     BatchNorm2d-347           [-1, 1816, 7, 7]           3,632\n",
        "            ReLU-348           [-1, 1816, 7, 7]               0\n",
        "          Conv2d-349             [-1, 32, 7, 7]         523,040\n",
        "     BatchNorm2d-350           [-1, 1848, 7, 7]           3,696\n",
        "            ReLU-351           [-1, 1848, 7, 7]               0\n",
        "          Conv2d-352             [-1, 32, 7, 7]          59,168\n",
        "     BatchNorm2d-353           [-1, 1880, 7, 7]           3,760\n",
        "            ReLU-354           [-1, 1880, 7, 7]               0\n",
        "          Conv2d-355             [-1, 32, 7, 7]         541,472\n",
        "     BatchNorm2d-356           [-1, 1912, 7, 7]           3,824\n",
        "            ReLU-357           [-1, 1912, 7, 7]               0\n",
        "          Conv2d-358             [-1, 32, 7, 7]          61,216\n",
        "     BatchNorm2d-359           [-1, 1944, 7, 7]           3,888\n",
        "            ReLU-360           [-1, 1944, 7, 7]               0\n",
        "          Conv2d-361             [-1, 32, 7, 7]         559,904\n",
        "     BatchNorm2d-362           [-1, 1976, 7, 7]           3,952\n",
        "            ReLU-363           [-1, 1976, 7, 7]               0\n",
        "          Conv2d-364             [-1, 32, 7, 7]          63,264\n",
        "     BatchNorm2d-365           [-1, 2008, 7, 7]           4,016\n",
        "            ReLU-366           [-1, 2008, 7, 7]               0\n",
        "          Conv2d-367             [-1, 32, 7, 7]         578,336\n",
        "      DenseBlock-368           [-1, 2040, 7, 7]               0\n",
        "     BatchNorm2d-369           [-1, 2040, 7, 7]           4,080\n",
        "            ReLU-370           [-1, 2040, 7, 7]               0\n",
        "AdaptiveAvgPool2d-371           [-1, 2040, 1, 1]               0\n",
        "         Flatten-372                 [-1, 2040]               0\n",
        "          Linear-373                   [-1, 10]          20,410\n",
        "================================================================\n",
        "Total params: 23,245,586\n",
        "Trainable params: 23,245,586\n",
        "Non-trainable params: 0\n",
        "----------------------------------------------------------------\n",
        "Input size (MB): 0.19\n",
        "Forward/backward pass size (MB): 633.18\n",
        "Params size (MB): 88.67\n",
        "Estimated Total Size (MB): 722.05\n",
        "----------------------------------------------------------------\n",
        "arch169 = ([[[1,0],[3,1]]*6,[[1,0],[3,1]]*12,[[1,0],[3,1]]*32,[[1,0],[3,1]]*32])\n",
        "densenet169 = DenseNet(lr=0.01, arch=arch169)\n",
        "arch201 = ([[[1,0],[3,1]]*6,[[1,0],[3,1]]*12,[[1,0],[3,1]]*48,[[1,0],[3,1]]*32])\n",
        "densenet201 = DenseNet(lr=0.01, arch=arch201)\n",
        "arch264 = ([[[1,0],[3,1]]*6,[[1,0],[3,1]]*12,[[1,0],[3,1]]*64,[[1,0],[3,1]]*48])\n",
        "densenet264 = DenseNet(lr=0.01, arch=arch264)\n"
      ],
      "metadata": {
        "id": "xfg_9Cj0wimM"
      },
      "id": "xfg_9Cj0wimM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import cProfile\n",
        "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
        "import d2l\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class KaggleHouse(d2l.DataModule):\n",
        "    def __init__(self, batch_size, train=None, val=None):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        if self.train is None:\n",
        "            self.raw_train = pd.read_csv(d2l.download(d2l.DATA_URL+ 'kaggle_house_pred_train.csv', self.root,\n",
        "                sha1_hash='585e9cc93e70b39160e7921475f9bcd7d31219ce'))\n",
        "            self.raw_val = pd.read_csv(d2l.download(\n",
        "                d2l.DATA_URL + 'kaggle_house_pred_test.csv', self.root,\n",
        "                sha1_hash='fa19780a7b011d9b009e8bff8e99922a8ee2eb90'))\n",
        "\n",
        "    def preprocess(self, std_flag=True):\n",
        "        label = 'SalePrice'\n",
        "        features = pd.concat((self.raw_train.drop(columns=['Id',label]),\n",
        "                              self.raw_val.drop(columns=['Id'])))\n",
        "        numeric_features = features.dtypes[features.dtypes!='object'].index\n",
        "        if std_flag:\n",
        "            features[numeric_features] = features[numeric_features].apply(lambda x: (x-x.mean())/x.std())\n",
        "        features[numeric_features] = features[numeric_features].fillna(0)\n",
        "        features = pd.get_dummies(features, dummy_na=True)\n",
        "        self.train = features[:self.raw_train.shape[0]].copy()\n",
        "        self.train[label] = self.raw_train[label]\n",
        "        self.val = features[self.raw_train.shape[0]:].copy()\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        label = 'SalePrice'\n",
        "        data = self.train if train else self.val\n",
        "        if label not in data:\n",
        "            return\n",
        "        get_tensor = lambda x: torch.tensor(x.values.astype(float), dtype=torch.float32)\n",
        "        # tensors = (get_tensor(data.drop(columns=[label])),\n",
        "        #            torch.log(get_tensor(data[label])).reshape(-1,1))\n",
        "        tensors = (get_tensor(data.drop(columns=[label])),  # X\n",
        "               torch.log(get_tensor(data[label])).reshape((-1, 1)))  # Y\n",
        "        return self.get_tensorloader(tensors, train)\n",
        "\n",
        "def k_fold_data(data,k):\n",
        "    rets = []\n",
        "    fold_size = data.train.shape[0] // k\n",
        "    for j in range(k):\n",
        "        idx = range(j*fold_size,(j+1)*fold_size)\n",
        "        rets.append(KaggleHouse(data.batch_size,data.train.drop(index=idx),data.train.iloc[idx]))\n",
        "    return rets\n",
        "\n",
        "def k_fold(trainer, data, k, ModelClass,hparams,plot_flag=True):\n",
        "    val_loss, models = [], []\n",
        "    for i, data_fold in enumerate(k_fold_data(data,k)):\n",
        "        model = ModelClass(**hparams)\n",
        "        model.board.yscale='log'\n",
        "        if not plot_flag or i != 0:\n",
        "            model.board.display=False\n",
        "        trainer.fit(model,data_fold)\n",
        "        val_loss.append(float(model.board.data['val_loss'][-1].y))\n",
        "        models.append(model)\n",
        "    avg_val_loss = sum(val_loss)/len(val_loss)\n",
        "    print(f'average validation log mse = {avg_val_loss}, params:{hparams}')\n",
        "    return models, avg_val_loss\n",
        "\n",
        "\n",
        "\n",
        "class HouseResMLP(d2l.LinearRegression):\n",
        "    def __init__(self, num_outputs, num_hiddens, lr, dropouts, weight_decay):\n",
        "        super().__init__(lr)\n",
        "        self.save_hyperparameters()\n",
        "        layers = [nn.Flatten()]\n",
        "        for i in range(len(num_hiddens)):\n",
        "            layers.append(nn.Sequential(nn.LazyLinear(num_hiddens[i]),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(dropouts[i]),\n",
        "                                        nn.LazyBatchNorm1d(),\n",
        "                                        ))\n",
        "        layers.append(nn.LazyLinear(num_outputs))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.net[0](X)\n",
        "        for blk in self.net[1:-1]:\n",
        "            Y = blk(X)\n",
        "            # Concatenate input and output of each block along the channels\n",
        "            X = torch.cat((X, Y), dim=1)\n",
        "        return self.net[-1](X)\n",
        "\n",
        "# class HouseDenseBlock(nn.Module):\n",
        "#     def __init__(self, num_hiddens):\n",
        "#         super().__init__()\n",
        "#         layer = []\n",
        "#         for i in range(len(num_hiddens)):\n",
        "#             layer.append(nn.Sequential(nn.LazyLinear(num_hiddens[i]),\n",
        "#                                         nn.LazyBatchNorm1d(), nn.ReLU(),\n",
        "#                                         ))\n",
        "#         self.net = nn.Sequential(*layer)\n",
        "\n",
        "#     def forward(self, X):\n",
        "#         for blk in self.net:\n",
        "#             Y = blk(X)\n",
        "#             # Concatenate input and output of each block along the channels\n",
        "#             X = torch.cat((X, Y), dim=1)\n",
        "#         return X\n",
        "\n",
        "# def transition_block():\n",
        "#     return nn.Sequential(\n",
        "#         nn.LazyBatchNorm1d(), nn.ReLU(),\n",
        "#         nn.AvgPool1d(kernel_size=2, stride=2))\n",
        "\n",
        "# class HouseResMLP(d2l.LinearRegression):\n",
        "#     def __init__(self, num_outputs, arch, lr, dropouts, weight_decay):\n",
        "#         super().__init__(lr)\n",
        "#         self.save_hyperparameters()\n",
        "#         layers = [nn.Flatten()]\n",
        "#         for num_hiddens in arch:\n",
        "#             layers.append(HouseDenseBlock(num_hiddens))\n",
        "#             # layers.append(nn.LazyLinear(sum(num_hiddens)//4))\n",
        "#         layers.append(nn.LazyLinear(num_outputs))\n",
        "#         self.net = nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, X):\n",
        "#         return self.net(X)\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         return torch.optim.SGD(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "hparams = {'dropouts': [0]*5,\n",
        " 'lr': 0.01,\n",
        " 'num_hiddens': [64,32,16,8],\n",
        " 'num_outputs': 1,\n",
        " 'weight_decay': 0}\n",
        "model = HouseResMLP(**hparams)\n",
        "summary(model,(1,80))\n",
        "----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "           Flatten-1                   [-1, 80]               0\n",
        "            Linear-2                   [-1, 64]           5,184\n",
        "              ReLU-3                   [-1, 64]               0\n",
        "           Dropout-4                   [-1, 64]               0\n",
        "       BatchNorm1d-5                   [-1, 64]             128\n",
        "            Linear-6                   [-1, 32]           4,640\n",
        "              ReLU-7                   [-1, 32]               0\n",
        "           Dropout-8                   [-1, 32]               0\n",
        "       BatchNorm1d-9                   [-1, 32]              64\n",
        "           Linear-10                   [-1, 16]           2,832\n",
        "             ReLU-11                   [-1, 16]               0\n",
        "          Dropout-12                   [-1, 16]               0\n",
        "      BatchNorm1d-13                   [-1, 16]              32\n",
        "           Linear-14                    [-1, 8]           1,544\n",
        "             ReLU-15                    [-1, 8]               0\n",
        "          Dropout-16                    [-1, 8]               0\n",
        "      BatchNorm1d-17                    [-1, 8]              16\n",
        "           Linear-18                    [-1, 1]             201\n",
        "================================================================\n",
        "Total params: 14,641\n",
        "Trainable params: 14,641\n",
        "Non-trainable params: 0\n",
        "----------------------------------------------------------------\n",
        "Input size (MB): 0.00\n",
        "Forward/backward pass size (MB): 0.00\n",
        "Params size (MB): 0.06\n",
        "Estimated Total Size (MB): 0.06\n",
        "----------------------------------------------------------------\n",
        "data = KaggleHouse(batch_size=64)\n",
        "print(data.raw_train.shape, data.raw_val.shape)\n",
        "data.preprocess()\n",
        "(1460, 81) (1459, 80)\n",
        "hparams = {'dropouts': [0]*5,\n",
        " 'lr': 0.01,\n",
        " 'num_hiddens': [64,32,16,8],\n",
        " 'num_outputs': 1,\n",
        " 'weight_decay': 0}\n",
        "trainer = d2l.Trainer(max_epochs=10)\n",
        "models,avg_val_loss = k_fold(trainer, data, k=5,ModelClass=HouseResMLP,hparams=hparams,plot_flag=True)\n",
        "average validation log mse = 0.09697333678603172, params:{'dropouts': [0, 0, 0, 0, 0], 'lr': 0.01, 'num_hiddens': [64, 32, 16, 8], 'num_outputs': 1, 'weight_decay': 0}\n"
      ],
      "metadata": {
        "id": "F2RSq3yswpl1"
      },
      "id": "F2RSq3yswpl1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a06d0ab9",
      "metadata": {
        "id": "a06d0ab9"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/88)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd5f12b",
      "metadata": {
        "id": "0bd5f12b"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea1dde2",
      "metadata": {
        "id": "7ea1dde2"
      },
      "source": [
        "# Designing Convolution Network Architectures\n",
        ":label:`sec_cnn-design`\n",
        "\n",
        "The previous sections have taken us on a tour of modern network design for computer vision. Common to all the work we covered was that it greatly relied on the intuition of scientists. Many of the architectures are heavily informed by human creativity and to a much lesser extent by systematic exploration of the design space that deep networks offer. Nonetheless, this *network engineering* approach has been tremendously successful.\n",
        "\n",
        "Ever since AlexNet (:numref:`sec_alexnet`)\n",
        "beat conventional computer vision models on ImageNet,\n",
        "it has become popular to construct very deep networks\n",
        "by stacking blocks of convolutions, all designed according to the same pattern.\n",
        "In particular, $3 \\times 3$ convolutions were\n",
        "popularized by VGG networks (:numref:`sec_vgg`).\n",
        "NiN (:numref:`sec_nin`) showed that even $1 \\times 1$ convolutions could\n",
        "be beneficial by adding local nonlinearities.\n",
        "Moreover, NiN solved the problem of aggregating information at the head of a network\n",
        "by aggregating across all locations.\n",
        "GoogLeNet (:numref:`sec_googlenet`) added multiple branches of different convolution width,\n",
        "combining the advantages of VGG and NiN in its Inception block.\n",
        "ResNets (:numref:`sec_resnet`)\n",
        "changed the inductive bias towards the identity mapping (from $f(x) = 0$). This allowed for very deep networks. Almost a decade later, the ResNet design is still popular, a testament to its design. Lastly, ResNeXt (:numref:`subsec_resnext`) added grouped convolutions, offering a better trade-off between parameters and computation. A precursor to Transformers for vision, the Squeeze-and-Excitation Networks (SENets) allow for efficient information transfer between locations\n",
        ":cite:`Hu.Shen.Sun.2018`. This was accomplished by computing a per-channel global attention function.\n",
        "\n",
        "Up to now we have omitted networks obtained via *neural architecture search* (NAS) :cite:`zoph2016neural,liu2018darts`. We chose to do so since their cost is usually enormous, relying on brute-force search, genetic algorithms, reinforcement learning, or some other form of hyperparameter optimization. Given a fixed search space,\n",
        "NAS uses a search strategy to automatically select\n",
        "an architecture based on the returned performance estimation.\n",
        "The outcome of NAS\n",
        "is a single network instance. EfficientNets are a notable outcome of this search :cite:`tan2019efficientnet`.\n",
        "\n",
        "In the following we discuss an idea that is quite different to the quest for the *single best network*. It is computationally relatively inexpensive, it leads to scientific insights on the way, and it is quite effective in terms of the quality of outcomes. Let's review the strategy by :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` to *design network design spaces*. The strategy combines the strength of manual design and NAS. It accomplishes this by operating on *distributions of networks* and optimizing the distributions in a way to obtain good performance for entire families of networks. The outcome of it are *RegNets*, specifically RegNetX and RegNetY, plus a range of guiding principles for the design of performant CNNs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d025e9",
      "metadata": {
        "id": "d2d025e9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e554d6",
      "metadata": {
        "id": "a6e554d6"
      },
      "source": [
        "## The AnyNet Design Space\n",
        ":label:`subsec_the-anynet-design-space`\n",
        "\n",
        "The description below closely follows the reasoning in :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` with some abbreviations to make it fit in the scope of the book.\n",
        "To begin, we need a template for the family of networks to explore. One of the commonalities of the designs in this chapter is that the networks consist of a *stem*, a *body* and a *head*. The stem performs initial image processing, often through convolutions with a larger window size. The body consists of multiple blocks, carrying out the bulk of the transformations needed to go from raw images to object representations. Lastly, the head converts this into the desired outputs, such as via a softmax regressor for multiclass classification.\n",
        "The body, in turn, consists of multiple stages, operating on the image at decreasing resolutions. In fact, both the stem and each subsequent stage quarter the spatial resolution. Lastly, each stage consists of one or more blocks. This pattern is common to all networks, from VGG to ResNeXt. Indeed, for the design of generic AnyNet networks, :citet:`Radosavovic.Kosaraju.Girshick.ea.2020` used the ResNeXt block of :numref:`fig_resnext_block`.\n",
        "\n",
        "\n",
        "![The AnyNet design space. The numbers $(\\mathit{c}, \\mathit{r})$ along each arrow indicate the number of channels $c$ and the resolution $\\mathit{r} \\times \\mathit{r}$ of the images at that point. From left to right: generic network structure composed of stem, body, and head; body composed of four stages; detailed structure of a stage; two alternative structures for blocks, one without downsampling and one that halves the resolution in each dimension. Design choices include depth $\\mathit{d_i}$, the number of output channels $\\mathit{c_i}$, the number of groups $\\mathit{g_i}$, and bottleneck ratio $\\mathit{k_i}$ for any stage $\\mathit{i}$.](http://d2l.ai/_images/anynet.svg)\n",
        ":label:`fig_anynet_full`\n",
        "\n",
        "Let's review the structure outlined in :numref:`fig_anynet_full` in detail. As mentioned, an AnyNet consists of a stem, body, and head. The stem takes as its input RGB images (3 channels), using a $3 \\times 3$ convolution with a stride of $2$, followed by a batch norm, to halve the resolution from $r \\times r$ to $r/2 \\times r/2$. Moreover, it generates $c_0$ channels that serve as input to the body.\n",
        "\n",
        "Since the network is designed to work well with ImageNet images of shape $224 \\times 224 \\times 3$, the body serves to reduce this to $7 \\times 7 \\times c_4$ through 4 stages (recall that $224 / 2^{1+4} = 7$), each with an eventual stride of $2$. Lastly, the head employs an entirely standard design via global average pooling, similar to NiN (:numref:`sec_nin`), followed by a fully connected layer to emit an $n$-dimensional vector for $n$-class classification.\n",
        "\n",
        "Most of the relevant design decisions are inherent to the body of the network. It proceeds in stages, where each stage is composed of the same type of ResNeXt blocks as we discussed in :numref:`subsec_resnext`. The design there is again entirely generic: we begin with a block that halves the resolution by using a stride of $2$ (the rightmost in :numref:`fig_anynet_full`). To match this, the residual branch of the ResNeXt block needs to pass through a $1 \\times 1$ convolution. This block is followed by a variable number of additional ResNeXt blocks that leave both resolution and the number of channels unchanged. Note that a common design practice is to add a slight bottleneck in the design of convolutional blocks.\n",
        "As such, with bottleneck ratio $k_i \\geq 1$ we afford some number of channels, $c_i/k_i$,  within each block for stage $i$ (as the experiments show, this is not really effective and should be skipped). Lastly, since we are dealing with ResNeXt blocks, we also need to pick the number of groups $g_i$ for grouped convolutions at stage $i$.\n",
        "\n",
        "This seemingly generic design space provides us nonetheless with many parameters: we can set the block width (number of channels) $c_0, \\ldots c_4$, the depth (number of blocks) per stage $d_1, \\ldots d_4$, the bottleneck ratios $k_1, \\ldots k_4$, and the group widths (numbers of groups) $g_1, \\ldots g_4$.\n",
        "In total this adds up to 17 parameters, resulting in an unreasonably large number of configurations that would warrant exploring. We need some tools to reduce this huge design space effectively. This is where the conceptual beauty of design spaces comes in. Before we do so, let's implement the generic design first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bcf1d35",
      "metadata": {
        "id": "0bcf1d35"
      },
      "outputs": [],
      "source": [
        "class AnyNet(d2l.Classifier):\n",
        "    def stem(self, num_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24d3006",
      "metadata": {
        "id": "e24d3006"
      },
      "source": [
        "Each stage consists of `depth` ResNeXt blocks,\n",
        "where `num_channels` specifies the block width.\n",
        "Note that the first block halves the height and width of input images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6faf93f",
      "metadata": {
        "id": "c6faf93f"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(AnyNet)\n",
        "def stage(self, depth, num_channels, groups, bot_mul):\n",
        "    blk = []\n",
        "    for i in range(depth):\n",
        "        if i == 0:\n",
        "            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n",
        "                use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))\n",
        "    return nn.Sequential(*blk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb035054",
      "metadata": {
        "id": "fb035054"
      },
      "source": [
        "Putting the network stem, body, and head together,\n",
        "we complete the implementation of AnyNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a44834",
      "metadata": {
        "id": "18a44834"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(AnyNet)\n",
        "def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
        "    super(AnyNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.stem(stem_channels))\n",
        "    for i, s in enumerate(arch):\n",
        "        self.net.add_module(f'stage{i+1}', self.stage(*s))\n",
        "    self.net.add_module('head', nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114b65ed",
      "metadata": {
        "id": "114b65ed"
      },
      "source": [
        "## Distributions and Parameters of Design Spaces\n",
        "\n",
        "As just discussed in :numref:`subsec_the-anynet-design-space`, parameters of a design space are hyperparameters of networks in that design space.\n",
        "Consider the problem of identifying good parameters in the AnyNet design space. We could try finding the *single best* parameter choice for a given amount of computation (e.g., FLOPs and compute time). If we allowed for even only *two* possible choices for each parameter, we would have to explore $2^{17} = 131072$ combinations to find the best solution. This is clearly infeasible because of its exorbitant cost. Even worse, we do not really learn anything from this exercise in terms of how one should design a network. Next time we add, say, an X-stage, or a shift operation, or similar, we would need to start from scratch. Even worse, due to the stochasticity in training (rounding, shuffling, bit errors), no two runs are likely to produce exactly the same results. A better strategy would be to try to determine general guidelines of how the choices of parameters should be related. For instance, the bottleneck ratio, the number of channels, blocks, groups, or their change between layers should ideally be governed by a collection of simple rules. The approach in :citet:`radosavovic2019network` relies on the following four assumptions:\n",
        "\n",
        "1. We assume that general design principles actually exist, so that many networks satisfying these requirements should offer good performance. Consequently, identifying a *distribution* over networks can be a sensible strategy. In other words, we assume that there are many good needles in the haystack.\n",
        "1. We need not train networks to convergence before we can assess whether a network is good. Instead, it is sufficient to use the intermediate results as reliable guidance for final accuracy. Using (approximate) proxies to optimize an objective is referred to as multi-fidelity optimization :cite:`forrester2007multi`. Consequently, design optimization is carried out, based on the accuracy achieved after only a few passes through the dataset, reducing the cost significantly.\n",
        "1. Results obtained at a smaller scale (for smaller networks) generalize to larger ones. Consequently, optimization is carried out for networks that are structurally similar, but with a smaller number of blocks, fewer channels, etc. Only in the end will we need to verify that the so-found networks also offer good performance at scale.\n",
        "1. Aspects of the design can be approximately factorized so that it is possible to infer their effect on the quality of the outcome somewhat independently. In other words, the optimization problem is moderately easy.\n",
        "\n",
        "These assumptions allow us to test many networks cheaply. In particular, we can *sample* uniformly from the space of configurations and evaluate their performance. Subsequently, we can evaluate the quality of the choice of parameters by reviewing the *distribution* of error/accuracy that can be achieved with said networks. Denote by $F(e)$ the cumulative distribution function (CDF) for errors committed by networks of a given design space, drawn using probability disribution $p$. That is,\n",
        "\n",
        "$$F(e, p) \\stackrel{\\textrm{def}}{=} P_{\\textrm{net} \\sim p} \\{e(\\textrm{net}) \\leq e\\}.$$\n",
        "\n",
        "Our goal is now to find a distribution $p$ over *networks* such that most networks have a very low error rate and where the support of $p$ is concise. Of course, this is computationally infeasible to perform accurately. We resort to a sample of networks $\\mathcal{Z} \\stackrel{\\textrm{def}}{=} \\{\\textrm{net}_1, \\ldots \\textrm{net}_n\\}$ (with errors $e_1, \\ldots, e_n$, respectively) from $p$ and use the empirical CDF $\\hat{F}(e, \\mathcal{Z})$ instead:\n",
        "\n",
        "$$\\hat{F}(e, \\mathcal{Z}) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}(e_i \\leq e).$$\n",
        "\n",
        "Whenever the CDF for one set of choices majorizes (or matches) another CDF it follows that its choice of parameters is superior (or indifferent). Accordingly\n",
        ":citet:`Radosavovic.Kosaraju.Girshick.ea.2020` experimented with a shared network bottleneck ratio $k_i = k$ for all stages $i$ of the network. This gets rid of three of the four parameters governing the bottleneck ratio. To assess whether this (negatively) affects the performance one can draw networks from the constrained and from the unconstrained distribution and compare the corresonding CDFs. It turns out that this constraint does not affect the accuracy of the distribution of networks at all, as can be seen in the first panel of :numref:`fig_regnet-fig`.\n",
        "Likewise, we could choose to pick the same group width $g_i = g$ occurring at the various stages of the network. Again, this does not affect performance, as can be seen in the second panel of :numref:`fig_regnet-fig`.\n",
        "Both steps combined reduce the number of free parameters by six.\n",
        "\n",
        "![Comparing error empirical distribution functions of design spaces. $\\textrm{AnyNet}_\\mathit{A}$ is the original design space; $\\textrm{AnyNet}_\\mathit{B}$ ties the bottleneck ratios, $\\textrm{AnyNet}_\\mathit{C}$ also ties group widths, $\\textrm{AnyNet}_\\mathit{D}$ increases the network depth across stages. From left to right: (i) tying bottleneck ratios has no effect on performance; (ii) tying group widths has no effect on performance; (iii) increasing network widths (channels) across stages improves performance; (iv) increasing network depths across stages improves performance. Figure courtesy of :citet:`Radosavovic.Kosaraju.Girshick.ea.2020`.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/regnet-fig.png?raw=1)\n",
        ":label:`fig_regnet-fig`\n",
        "\n",
        "Next we look for ways to reduce the multitude of potential choices for width and depth of the stages. It is a reasonable assumption that, as we go deeper, the number of channels should increase, i.e., $c_i \\geq c_{i-1}$ ($w_{i+1} \\geq w_i$ per their notation in :numref:`fig_regnet-fig`), yielding\n",
        "$\\textrm{AnyNetX}_D$. Likewise, it is equally reasonable to assume that as the stages progress, they should become deeper, i.e., $d_i \\geq d_{i-1}$, yielding $\\textrm{AnyNetX}_E$. This can be experimentally verified in the third and fourth panel of :numref:`fig_regnet-fig`, respectively.\n",
        "\n",
        "## RegNet\n",
        "\n",
        "The resulting $\\textrm{AnyNetX}_E$ design space consists of simple networks\n",
        "following easy-to-interpret design principles:\n",
        "\n",
        "* Share the bottleneck ratio $k_i = k$ for all stages $i$;\n",
        "* Share the group width $g_i = g$ for all stages $i$;\n",
        "* Increase network width across stages: $c_{i} \\leq c_{i+1}$;\n",
        "* Increase network depth across stages: $d_{i} \\leq d_{i+1}$.\n",
        "\n",
        "This leaves us with a final set of choices: how to pick the specific values for the above parameters of the eventual $\\textrm{AnyNetX}_E$ design space. By studying the best-performing networks from the distribution in $\\textrm{AnyNetX}_E$ one can observe the following: the width of the network ideally increases linearly with the block index across the network, i.e., $c_j \\approx c_0 + c_a j$, where $j$ is the block index and slope $c_a > 0$. Given that we get to choose a different block width only per stage, we arrive at a piecewise constant function, engineered to match this dependence. Furthermore, experiments also show that a bottleneck ratio of $k = 1$ performs best, i.e., we are advised not to use bottlenecks at all.\n",
        "\n",
        "We recommend the interested reader reviews further details in the design of specific networks for different amounts of computation by perusing :citet:`Radosavovic.Kosaraju.Girshick.ea.2020`. For instance, an effective 32-layer RegNetX variant is given by $k = 1$ (no bottleneck), $g = 16$ (group width is 16), $c_1 = 32$ and $c_2 = 80$ channels for the first and second stage, respectively, chosen to be $d_1=4$ and $d_2=6$ blocks deep. The astonishing insight from the design is that it still applies, even when investigating networks at a larger scale. Even better, it even holds for Squeeze-and-Excitation (SE) network designs (RegNetY) that have a global channel activation :cite:`Hu.Shen.Sun.2018`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab5f41a",
      "metadata": {
        "id": "dab5f41a"
      },
      "outputs": [],
      "source": [
        "class RegNetX32(AnyNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        stem_channels, groups, bot_mul = 32, 16, 1\n",
        "        depths, channels = (4, 6), (32, 80)\n",
        "        super().__init__(\n",
        "            ((depths[0], channels[0], groups, bot_mul),\n",
        "             (depths[1], channels[1], groups, bot_mul)),\n",
        "            stem_channels, lr, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd897988",
      "metadata": {
        "id": "cd897988"
      },
      "source": [
        "We can see that each RegNetX stage progressively reduces resolution and increases output channels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a56c8e",
      "metadata": {
        "id": "75a56c8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb1426e-b871-4aaf-d978-c5494686c130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 32, 48, 48])\n",
            "Sequential output shape:\t torch.Size([1, 32, 24, 24])\n",
            "Sequential output shape:\t torch.Size([1, 80, 12, 12])\n",
            "Sequential output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "RegNetX32().layer_summary((1, 1, 96, 96))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d568cc20",
      "metadata": {
        "id": "d568cc20"
      },
      "source": [
        "## Training\n",
        "\n",
        "Training the 32-layer RegNetX on the Fashion-MNIST dataset is just like before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91cbec9a",
      "metadata": {
        "id": "91cbec9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "95786e34-0d8b-4468-ab66-76096e9a29d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"183.35625pt\" viewBox=\"0 0 238.965625 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-09-27T09:05:57.597964</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 238.965625 183.35625 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \nL 225.403125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m6d141cefd0\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m6d141cefd0\" x=\"30.103125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(26.921875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m6d141cefd0\" x=\"69.163125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <g transform=\"translate(65.981875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m6d141cefd0\" x=\"108.223125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <g transform=\"translate(105.041875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m6d141cefd0\" x=\"147.283125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <g transform=\"translate(144.101875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m6d141cefd0\" x=\"186.343125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 8 -->\n      <g transform=\"translate(183.161875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m6d141cefd0\" x=\"225.403125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 10 -->\n      <g transform=\"translate(219.040625 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"mfb4502344d\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mfb4502344d\" x=\"30.103125\" y=\"134.844415\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 138.643633) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mfb4502344d\" x=\"30.103125\" y=\"111.16909\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 114.968309) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mfb4502344d\" x=\"30.103125\" y=\"87.493766\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 91.292985) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mfb4502344d\" x=\"30.103125\" y=\"63.818442\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 67.617661) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#mfb4502344d\" x=\"30.103125\" y=\"40.143118\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 43.942337) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mfb4502344d\" x=\"30.103125\" y=\"16.467794\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.2 -->\n      <g transform=\"translate(7.2 20.267013) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 34.954394 50.176436 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 49.633125 13.5 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_17\"/>\n   <g id=\"line2d_18\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 49.633125 13.5 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 49.633125 93.536687 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 49.633125 13.5 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 49.633125 93.536687 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 49.633125 13.5 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 49.633125 93.536687 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 49.633125 93.536687 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \nL 190.861259 138.26676 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \nL 190.861259 138.26676 \nL 200.605438 137.423653 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \nL 190.861259 138.26676 \nL 200.605438 137.423653 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \nL 205.873125 121.501822 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \nL 190.861259 138.26676 \nL 200.605438 137.423653 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \nL 205.873125 121.501822 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \nL 205.873125 52.587179 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \nL 190.861259 138.26676 \nL 200.605438 137.423653 \nL 210.349618 139.5 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \nL 205.873125 121.501822 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \nL 205.873125 52.587179 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \nL 190.861259 138.26676 \nL 200.605438 137.423653 \nL 210.349618 139.5 \nL 220.093797 138.67621 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \nL 205.873125 121.501822 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \nL 205.873125 52.587179 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \nL 190.861259 138.26676 \nL 200.605438 137.423653 \nL 210.349618 139.5 \nL 220.093797 138.67621 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \nL 205.873125 121.501822 \nL 225.403125 128.461597 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \nL 205.873125 52.587179 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 34.954394 50.176436 \nL 44.698573 100.489949 \nL 54.442752 111.624367 \nL 64.186931 117.143224 \nL 73.93111 121.572557 \nL 83.675289 123.740854 \nL 93.419468 126.740861 \nL 103.163647 126.528346 \nL 112.907826 130.978977 \nL 122.652006 130.060705 \nL 132.396185 133.131513 \nL 142.140364 132.688947 \nL 151.884543 134.967672 \nL 161.628722 134.264391 \nL 171.372901 136.649408 \nL 181.11708 135.698953 \nL 190.861259 138.26676 \nL 200.605438 137.423653 \nL 210.349618 139.5 \nL 220.093797 138.67621 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_127\">\n    <path d=\"M 49.633125 13.5 \nL 69.163125 114.586506 \nL 88.693125 96.993992 \nL 108.223125 123.4744 \nL 127.753125 123.74696 \nL 147.283125 120.181762 \nL 166.813125 123.31917 \nL 186.343125 115.478808 \nL 205.873125 121.501822 \nL 225.403125 128.461597 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_128\">\n    <path d=\"M 49.633125 93.536687 \nL 69.163125 56.333275 \nL 88.693125 61.203199 \nL 108.223125 52.89155 \nL 127.753125 53.078854 \nL 147.283125 54.038791 \nL 166.813125 52.575473 \nL 186.343125 55.654295 \nL 205.873125 52.587179 \nL 225.403125 50.280989 \n\" clip-path=\"url(#pc2e2eddcb5)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 145.8 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 145.8 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 145.8 \nL 225.403125 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 225.403125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 138.8125 60.06875 \nL 218.403125 60.06875 \nQ 220.403125 60.06875 220.403125 58.06875 \nL 220.403125 14.2 \nQ 220.403125 12.2 218.403125 12.2 \nL 138.8125 12.2 \nQ 136.8125 12.2 136.8125 14.2 \nL 136.8125 58.06875 \nQ 136.8125 60.06875 138.8125 60.06875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_129\">\n     <path d=\"M 140.8125 20.298438 \nL 150.8125 20.298438 \nL 160.8125 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- train_loss -->\n     <g transform=\"translate(168.8125 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_130\">\n     <path d=\"M 140.8125 35.254688 \nL 150.8125 35.254688 \nL 160.8125 35.254688 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- val_loss -->\n     <g transform=\"translate(168.8125 38.754688) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n    <g id=\"line2d_131\">\n     <path d=\"M 140.8125 50.210938 \nL 150.8125 50.210938 \nL 160.8125 50.210938 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #2ca02c; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_16\">\n     <!-- val_acc -->\n     <g transform=\"translate(168.8125 53.710938) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"259.521484\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"314.501953\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc2e2eddcb5\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = RegNetX32(lr=0.05)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60c0281",
      "metadata": {
        "id": "f60c0281"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "With desirable inductive biases (assumptions or preferences) like locality and translation invariance (:numref:`sec_why-conv`)\n",
        "for vision, CNNs have been the dominant architectures in this area. This remained the case from LeNet up until Transformers (:numref:`sec_transformer`) :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021,touvron2021training` started surpassing CNNs in terms of accuracy. While much of the recent progress in terms of vision Transformers *can* be backported into CNNs :cite:`liu2022convnet`, it is only possible at a higher computational cost. Just as importantly, recent hardware optimizations (NVIDIA Ampere and Hopper) have only widened the gap in favor of Transformers.\n",
        "\n",
        "It is worth noting that Transformers have a significantly lower degree of inductive bias towards locality and translation invariance than CNNs. That learned structures prevailed is due, not least, to the availability of large image collections, such as LAION-400m and LAION-5B :cite:`schuhmann2022laion` with up to 5 billion images. Quite surprisingly, some of the more relevant work in this context even includes MLPs :cite:`tolstikhin2021mlp`.\n",
        "\n",
        "In sum, vision Transformers (:numref:`sec_vision-transformer`) by now lead in terms of\n",
        "state-of-the-art performance in large-scale image classification,\n",
        "showing that *scalability trumps inductive biases* :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`.\n",
        "This includes pretraining large-scale Transformers (:numref:`sec_large-pretraining-transformers`) with multi-head self-attention (:numref:`sec_multihead-attention`). We invite the readers to dive into these chapters for a much more detailed discussion.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Increase the number of stages to four. Can you design a deeper RegNetX that performs better?\n",
        "1. De-ResNeXt-ify RegNets by replacing the ResNeXt block with the ResNet block. How does your new model perform?\n",
        "1. Implement multiple instances of a \"VioNet\" family by *violating* the design principles of RegNetX. How do they perform? Which of ($d_i$, $c_i$, $g_i$, $b_i$) is the most important factor?\n",
        "1. Your goal is to design the \"perfect\" MLP. Can you use the design principles introduced above to find good architectures? Is it possible to extrapolate from small to large networks?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sys\n",
        "sys.path.append('/home/jovyan/work/d2l_solutions/notebooks/exercises/d2l_utils/')\n",
        "import d2l\n",
        "from torchsummary import summary\n",
        "\n",
        "class AnyNet(d2l.Classifier):\n",
        "    def stem(self, num_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU())\n",
        "    def stage(self, depth, num_channels, groups, bot_mul):\n",
        "        blk = []\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n",
        "                    use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
        "        super(AnyNet, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.stem(stem_channels))\n",
        "        for i, s in enumerate(arch):\n",
        "            self.net.add_module(f'stage{i+1}', self.stage(*s))\n",
        "        self.net.add_module('head', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "        self.net.apply(d2l.init_cnn)\n",
        "\n",
        "class RegNetX32(AnyNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        stem_channels, groups, bot_mul = 32, 16, 1\n",
        "        depths, channels = (4, 6, 8, 16), (32, 80, 128, 256)\n",
        "        super().__init__(\n",
        "            # ((depths[0], channels[0], groups, bot_mul),\n",
        "            #  (depths[1], channels[1], groups, bot_mul)),\n",
        "            [(depths[i], channels[i], groups, bot_mul) for i in range(len(depths))],\n",
        "            stem_channels, lr, num_classes)\n",
        "model = RegNetX32(lr=0.05)\n",
        "# summary(model,(1,224,224))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "trainer.fit(model, data)\n"
      ],
      "metadata": {
        "id": "eXqbveSkw2Dk"
      },
      "id": "eXqbveSkw2Dk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "class DeAnyNet(d2l.Classifier):\n",
        "    def stem(self, num_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LazyBatchNorm2d(), nn.ReLU())\n",
        "    def stage(self, depth, num_channels):\n",
        "        blk = []\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                blk.append(d2l.Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.append(d2l.Residual(num_channels))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(self.stem(stem_channels))\n",
        "        for i, s in enumerate(arch):\n",
        "            self.net.add_module(f'stage{i+1}', self.stage(*s))\n",
        "        self.net.add_module('head', nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)))\n",
        "        self.net.apply(d2l.init_cnn)\n",
        "\n",
        "class DeResNeXt(DeAnyNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        stem_channels, groups, bot_mul = 32, 16, 1\n",
        "        depths, channels = (5, 6), (32, 80)\n",
        "        super().__init__(\n",
        "            ((depths[0], channels[0]),\n",
        "             (depths[1], channels[1])),\n",
        "            stem_channels, lr, num_classes)\n",
        "model = DeResNeXt(lr=0.05)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(model, data)\n"
      ],
      "metadata": {
        "id": "ApW2qTRUw4Tt"
      },
      "id": "ApW2qTRUw4Tt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "class VioNet(AnyNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10, depths=(4, 6), channels=(32, 80),\n",
        "                 stem_channels=32, groups=(16, 16), bot_mul=(1, 1)):\n",
        "        super().__init__(\n",
        "            [(depths[i], channels[i], groups[i], bot_mul[i]) for i in range(len(depths))],\n",
        "            stem_channels, lr, num_classes)\n",
        "VioNet_d = VioNet(depths=(6, 4))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(VioNet_d, data)\n",
        "VioNet_c = VioNet(channels=(80, 32))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(VioNet_c, data)\n",
        "VioNet_g = VioNet(groups=(16, 32))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(VioNet_g, data)\n",
        "VioNet_b = VioNet(bot_mul=(1, 2))\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "trainer.fit(VioNet_b, data)\n"
      ],
      "metadata": {
        "id": "E-lqos_nw6dB"
      },
      "id": "E-lqos_nw6dB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Designing the \"perfect\" Multilayer Perceptron (MLP) requires careful consideration of architectural decisions to achieve optimal performance for a specific task. The paper \"On Network Design Spaces for Visual Recognition\" outlines several design principles that can guide the creation of effective neural network architectures for visual recognition. Here’s how to apply these principles to build an effective MLP:\n",
        "\n",
        "### Depth and Width\n",
        "\n",
        "- **Depth:** Experiment with various numbers of layers (depth) in your MLP. Begin with a moderate depth and gradually increase it while monitoring performance. Deeper networks can capture complex features but may necessitate techniques like skip connections (as in ResNets) to combat vanishing gradients.\n",
        "- **Width:** Vary the number of neurons (width) in each layer. Wider layers can learn more complex patterns but may increase the risk of overfitting. Techniques like dropout or batch normalization can help regularize the network.\n",
        "\n",
        "### Skip Connections\n",
        "Consider integrating skip connections between layers, similar to Residual Networks (ResNets). These connections can alleviate vanishing gradient issues and facilitate the training of very deep networks.\n",
        "\n",
        "### Kernel Sizes\n",
        "Experiment with different kernel sizes for convolutional layers or adjust the number of neurons in fully connected layers. Smaller kernels can capture fine details, while larger kernels can identify broader patterns.\n",
        "\n",
        "### Pooling Strategies\n",
        "Utilize various pooling strategies, such as max-pooling or average pooling, to downsample feature maps. The choice of pooling can influence the invariance and spatial resolution of the learned features.\n",
        "\n",
        "### Normalization\n",
        "Incorporate batch normalization layers to stabilize training and enhance convergence. Batch normalization can also serve as a regularizer.\n",
        "\n",
        "### Activation Functions\n",
        "Test different activation functions like ReLU, Leaky ReLU, or variants such as Swish. The selection of activation function can impact the network’s capacity to model complex data distributions.\n",
        "\n",
        "### Dropout\n",
        "Implement dropout with varying rates to prevent overfitting. You can apply dropout selectively to certain layers or neurons based on their importance.\n",
        "\n",
        "### Initialization\n",
        "Use suitable weight initialization techniques like Xavier/Glorot or He initialization. Proper initialization can speed up training and improve convergence.\n",
        "\n",
        "### Normalization Layers\n",
        "Experiment with layer normalization or group normalization alongside batch normalization to evaluate if they provide advantages for your specific task.\n",
        "\n",
        "### Optimizers and Learning Rates\n",
        "Choose appropriate optimizers (e.g., Adam, SGD) and learning rate schedules. Learning rate adjustments like annealing or cyclic learning rates can be beneficial during training.\n",
        "\n",
        "### Regularization Techniques\n",
        "Consider employing L1 and L2 regularization to manage the model's complexity and prevent overfitting. Explore advanced regularization methods like dropout, weight decay, or early stopping.\n",
        "\n",
        "### Task-Specific Architectures\n",
        "Tailor your MLP architecture to the specific task at hand. For instance, employ a final softmax layer for classification tasks or a linear layer for regression tasks.\n",
        "\n",
        "### Ensemble Learning\n",
        "Experiment with ensemble methods to combine multiple MLPs, enhancing performance and robustness.\n",
        "\n",
        "### Hyperparameter Search\n",
        "Conduct systematic hyperparameter tuning using techniques like grid search or random search to identify the optimal combination of hyperparameters.\n",
        "\n",
        "### Transfer Learning\n",
        "Explore transfer learning by initializing your MLP with pretrained weights from a model trained on a related task. Fine-tuning on your specific task can significantly enhance performance.\n",
        "\n",
        "### Data Augmentation\n",
        "Implement data augmentation techniques to effectively increase your training dataset size and improve the model’s generalization capabilities.\n",
        "\n",
        "### Regularly Evaluate Performance\n",
        "Continuously monitor and assess the model’s performance on a validation dataset. Adjust architectural choices based on the performance feedback received.\n",
        "\n",
        "Designing the \"perfect\" MLP is an iterative process involving experimentation, evaluation, and refinement. The architectural choices and design principles should align with the specific requirements and constraints of your task and dataset."
      ],
      "metadata": {
        "id": "k6cDAIUgw_6w"
      },
      "id": "k6cDAIUgw_6w"
    },
    {
      "cell_type": "markdown",
      "id": "cdaec6a5",
      "metadata": {
        "id": "cdaec6a5"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/7463)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": []
  },
  "nbformat": 4,
  "nbformat_minor": 5
}